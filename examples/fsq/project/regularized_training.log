2025-01-24 09:07:25,763 - Using device: 0
2025-01-24 09:07:26,230 - 
Epoch 1/50
2025-01-24 09:08:25,450 - Using device: 0
2025-01-24 09:08:26,237 - 
Epoch 1/50
2025-01-24 09:09:41,102 - Using device: 0
2025-01-24 09:09:41,849 - 
Epoch 1/50
2025-01-24 09:09:43,137 - Batch 0: Loss = 0.0188
2025-01-24 09:09:43,880 - Batch 100: Loss = 0.0142
2025-01-24 09:09:44,574 - Batch 200: Loss = 0.0130
2025-01-24 09:09:45,288 - Batch 300: Loss = 0.0120
2025-01-24 09:09:46,063 - Batch 400: Loss = 0.0110
2025-01-24 09:09:46,941 - Batch 500: Loss = 0.0101
2025-01-24 09:09:47,828 - Batch 600: Loss = 0.0092
2025-01-24 09:09:48,724 - Batch 700: Loss = 0.0084
2025-01-24 09:09:49,745 - Batch 800: Loss = 0.0075
2025-01-24 09:09:50,783 - Batch 900: Loss = 0.0068
2025-01-24 09:09:51,818 - Batch 1000: Loss = 0.0060
2025-01-24 09:09:52,854 - Batch 1100: Loss = 0.0053
2025-01-24 09:09:53,864 - Batch 1200: Loss = 0.0046
2025-01-24 09:09:54,823 - Batch 1300: Loss = 0.0040
2025-01-24 09:09:55,793 - Batch 1400: Loss = 0.0033
2025-01-24 09:09:56,793 - Batch 1500: Loss = 0.0027
2025-01-24 09:09:59,500 - Train Loss: 0.0081
2025-01-24 09:09:59,501 - Test Loss: 68222.8552
2025-01-24 09:09:59,501 - FID Score: 0.1229
2025-01-24 09:09:59,501 - Commitment Loss: 0.0000
2025-01-24 09:09:59,501 - Codebook Loss: 0.0000
2025-01-24 09:09:59,501 - Entropy Reg: 0.0000
2025-01-24 09:09:59,501 - L2 Reg: 0.8060
2025-01-24 09:09:59,508 - Saved checkpoint at epoch 0
2025-01-24 09:09:59,530 - 
Epoch 2/50
2025-01-24 09:09:59,662 - Batch 0: Loss = 0.0024
2025-01-24 09:10:00,255 - Batch 100: Loss = 0.0018
2025-01-24 09:10:00,855 - Batch 200: Loss = 0.0013
2025-01-24 09:10:01,432 - Batch 300: Loss = 0.0009
2025-01-24 09:10:02,044 - Batch 400: Loss = 0.0004
2025-01-24 09:10:02,763 - Batch 500: Loss = 0.0001
2025-01-24 09:10:03,717 - Batch 600: Loss = 0.0000
2025-01-24 09:10:04,728 - Batch 700: Loss = 0.0000
2025-01-24 09:10:05,731 - Batch 800: Loss = 0.0000
2025-01-24 09:10:06,748 - Batch 900: Loss = 0.0000
2025-01-24 09:10:07,751 - Batch 1000: Loss = 0.0000
2025-01-24 09:10:08,768 - Batch 1100: Loss = 0.0000
2025-01-24 09:10:09,790 - Batch 1200: Loss = 0.0000
2025-01-24 09:10:10,810 - Batch 1300: Loss = 0.0000
2025-01-24 09:10:11,820 - Batch 1400: Loss = 0.0000
2025-01-24 09:10:12,804 - Batch 1500: Loss = 0.0000
2025-01-24 09:10:15,390 - Train Loss: 0.0004
2025-01-24 09:10:15,390 - Test Loss: 68222.7068
2025-01-24 09:10:15,390 - FID Score: 0.1229
2025-01-24 09:10:15,390 - Commitment Loss: 0.0000
2025-01-24 09:10:15,390 - Codebook Loss: 0.0000
2025-01-24 09:10:15,390 - Entropy Reg: 0.0000
2025-01-24 09:10:15,390 - L2 Reg: 0.0370
2025-01-24 09:10:15,413 - 
Epoch 3/50
2025-01-24 09:10:15,514 - Batch 0: Loss = 0.0000
2025-01-24 09:10:16,227 - Batch 100: Loss = 0.0000
2025-01-24 09:10:16,970 - Batch 200: Loss = 0.0000
2025-01-24 09:10:17,832 - Batch 300: Loss = 0.0000
2025-01-24 09:10:18,716 - Batch 400: Loss = 0.0000
2025-01-24 09:10:19,608 - Batch 500: Loss = 0.0000
2025-01-24 09:10:20,577 - Batch 600: Loss = 0.0000
2025-01-24 09:10:21,622 - Batch 700: Loss = 0.0000
2025-01-24 09:10:22,662 - Batch 800: Loss = 0.0000
2025-01-24 09:10:23,679 - Batch 900: Loss = 0.0000
2025-01-24 09:10:24,715 - Batch 1000: Loss = 0.0000
2025-01-24 09:10:25,734 - Batch 1100: Loss = 0.0000
2025-01-24 09:10:26,737 - Batch 1200: Loss = 0.0000
2025-01-24 09:10:27,726 - Batch 1300: Loss = 0.0000
2025-01-24 09:10:28,720 - Batch 1400: Loss = 0.0000
2025-01-24 09:10:29,715 - Batch 1500: Loss = 0.0000
2025-01-24 09:10:32,283 - Train Loss: 0.0000
2025-01-24 09:10:32,284 - Test Loss: 68222.6365
2025-01-24 09:10:32,284 - FID Score: 0.1229
2025-01-24 09:10:32,284 - Commitment Loss: 0.0000
2025-01-24 09:10:32,284 - Codebook Loss: 0.0000
2025-01-24 09:10:32,284 - Entropy Reg: 0.0000
2025-01-24 09:10:32,284 - L2 Reg: 0.0003
2025-01-24 09:10:32,307 - 
Epoch 4/50
2025-01-24 09:10:32,438 - Batch 0: Loss = 0.0000
2025-01-24 09:10:33,354 - Batch 100: Loss = 0.0000
2025-01-24 09:10:34,309 - Batch 200: Loss = 0.0000
2025-01-24 09:10:35,286 - Batch 300: Loss = 0.0000
2025-01-24 09:10:36,308 - Batch 400: Loss = 0.0000
2025-01-24 09:10:37,383 - Batch 500: Loss = 0.0000
2025-01-24 09:10:38,382 - Batch 600: Loss = 0.0000
2025-01-24 09:10:39,350 - Batch 700: Loss = 0.0000
2025-01-24 09:10:40,320 - Batch 800: Loss = 0.0000
2025-01-24 09:10:41,319 - Batch 900: Loss = 0.0000
2025-01-24 09:10:42,356 - Batch 1000: Loss = 0.0000
2025-01-24 09:10:43,371 - Batch 1100: Loss = 0.0000
2025-01-24 09:10:44,402 - Batch 1200: Loss = 0.0000
2025-01-24 09:10:45,421 - Batch 1300: Loss = 0.0000
2025-01-24 09:10:46,485 - Batch 1400: Loss = 0.0000
2025-01-24 09:10:47,550 - Batch 1500: Loss = 0.0000
2025-01-24 09:10:50,139 - Train Loss: 0.0000
2025-01-24 09:10:50,140 - Test Loss: 68222.6052
2025-01-24 09:10:50,140 - FID Score: 0.1229
2025-01-24 09:10:50,140 - Commitment Loss: 0.0000
2025-01-24 09:10:50,140 - Codebook Loss: 0.0000
2025-01-24 09:10:50,140 - Entropy Reg: 0.0000
2025-01-24 09:10:50,140 - L2 Reg: 0.0002
2025-01-24 09:10:50,163 - 
Epoch 5/50
2025-01-24 09:10:50,306 - Batch 0: Loss = 0.0000
2025-01-24 09:10:51,214 - Batch 100: Loss = 0.0000
2025-01-24 09:10:52,131 - Batch 200: Loss = 0.0000
2025-01-24 09:10:53,073 - Batch 300: Loss = 0.0000
2025-01-24 09:10:53,985 - Batch 400: Loss = 0.0000
2025-01-24 09:10:54,881 - Batch 500: Loss = 0.0000
2025-01-24 09:10:55,779 - Batch 600: Loss = 0.0000
2025-01-24 09:10:56,772 - Batch 700: Loss = 0.0000
2025-01-24 09:10:57,744 - Batch 800: Loss = 0.0000
2025-01-24 09:10:58,651 - Batch 900: Loss = 0.0000
2025-01-24 09:10:59,551 - Batch 1000: Loss = 0.0000
2025-01-24 09:11:00,420 - Batch 1100: Loss = 0.0000
2025-01-24 09:11:01,378 - Batch 1200: Loss = 0.0000
2025-01-24 09:11:02,286 - Batch 1300: Loss = 0.0000
2025-01-24 09:11:03,186 - Batch 1400: Loss = 0.0000
2025-01-24 09:11:04,193 - Batch 1500: Loss = 0.0000
2025-01-24 09:11:06,814 - Train Loss: 0.0000
2025-01-24 09:11:06,814 - Test Loss: 68222.5896
2025-01-24 09:11:06,814 - FID Score: 0.1229
2025-01-24 09:11:06,814 - Commitment Loss: 0.0000
2025-01-24 09:11:06,814 - Codebook Loss: 0.0000
2025-01-24 09:11:06,814 - Entropy Reg: 0.0000
2025-01-24 09:11:06,814 - L2 Reg: 0.0002
2025-01-24 09:11:06,837 - 
Epoch 6/50
2025-01-24 09:11:06,976 - Batch 0: Loss = 0.0000
2025-01-24 09:11:07,961 - Batch 100: Loss = 0.0000
2025-01-24 09:11:09,052 - Batch 200: Loss = 0.0000
2025-01-24 09:11:10,129 - Batch 300: Loss = 0.0000
2025-01-24 09:11:10,965 - Batch 400: Loss = 0.0000
2025-01-24 09:11:11,995 - Batch 500: Loss = 0.0000
2025-01-24 09:11:13,054 - Batch 600: Loss = 0.0000
2025-01-24 09:11:14,091 - Batch 700: Loss = 0.0000
2025-01-24 09:11:15,103 - Batch 800: Loss = 0.0000
2025-01-24 09:11:16,101 - Batch 900: Loss = 0.0000
2025-01-24 09:11:17,061 - Batch 1000: Loss = 0.0000
2025-01-24 09:11:18,044 - Batch 1100: Loss = 0.0000
2025-01-24 09:11:19,067 - Batch 1200: Loss = 0.0000
2025-01-24 09:11:20,110 - Batch 1300: Loss = 0.0000
2025-01-24 09:11:21,005 - Batch 1400: Loss = 0.0000
2025-01-24 09:11:21,796 - Batch 1500: Loss = 0.0000
2025-01-24 09:11:24,469 - Train Loss: 0.0000
2025-01-24 09:11:24,469 - Test Loss: 68222.5818
2025-01-24 09:11:24,469 - FID Score: 0.1229
2025-01-24 09:11:24,469 - Commitment Loss: 0.0000
2025-01-24 09:11:24,469 - Codebook Loss: 0.0000
2025-01-24 09:11:24,469 - Entropy Reg: 0.0000
2025-01-24 09:11:24,469 - L2 Reg: 0.0001
2025-01-24 09:11:24,493 - 
Epoch 7/50
2025-01-24 09:11:24,638 - Batch 0: Loss = 0.0000
2025-01-24 09:11:25,595 - Batch 100: Loss = 0.0000
2025-01-24 09:11:26,478 - Batch 200: Loss = 0.0000
2025-01-24 09:11:27,358 - Batch 300: Loss = 0.0000
2025-01-24 09:11:28,246 - Batch 400: Loss = 0.0000
2025-01-24 09:11:29,103 - Batch 500: Loss = 0.0000
2025-01-24 09:11:29,986 - Batch 600: Loss = 0.0000
2025-01-24 09:11:30,870 - Batch 700: Loss = 0.0000
2025-01-24 09:11:31,747 - Batch 800: Loss = 0.0000
2025-01-24 09:11:32,411 - Batch 900: Loss = 0.0000
2025-01-24 09:11:33,062 - Batch 1000: Loss = 0.0000
2025-01-24 09:11:33,714 - Batch 1100: Loss = 0.0000
2025-01-24 09:11:34,374 - Batch 1200: Loss = 0.0000
2025-01-24 09:11:35,043 - Batch 1300: Loss = 0.0000
2025-01-24 09:11:35,717 - Batch 1400: Loss = 0.0000
2025-01-24 09:11:36,337 - Batch 1500: Loss = 0.0000
2025-01-24 09:11:38,603 - Train Loss: 0.0000
2025-01-24 09:11:38,604 - Test Loss: 68222.5740
2025-01-24 09:11:38,604 - FID Score: 0.1229
2025-01-24 09:11:38,604 - Commitment Loss: 0.0000
2025-01-24 09:11:38,604 - Codebook Loss: 0.0000
2025-01-24 09:11:38,604 - Entropy Reg: 0.0000
2025-01-24 09:11:38,604 - L2 Reg: 0.0002
2025-01-24 09:11:38,624 - 
Epoch 8/50
2025-01-24 09:11:38,723 - Batch 0: Loss = 0.0000
2025-01-24 09:11:39,332 - Batch 100: Loss = 0.0000
2025-01-24 09:11:39,953 - Batch 200: Loss = 0.0000
2025-01-24 09:11:40,595 - Batch 300: Loss = 0.0000
2025-01-24 09:11:41,239 - Batch 400: Loss = 0.0000
2025-01-24 09:11:41,883 - Batch 500: Loss = 0.0000
2025-01-24 09:11:42,527 - Batch 600: Loss = 0.0000
2025-01-24 09:11:43,174 - Batch 700: Loss = 0.0000
2025-01-24 09:11:43,819 - Batch 800: Loss = 0.0000
2025-01-24 09:11:44,466 - Batch 900: Loss = 0.0000
2025-01-24 09:11:45,122 - Batch 1000: Loss = 0.0000
2025-01-24 09:11:45,755 - Batch 1100: Loss = 0.0000
2025-01-24 09:11:46,370 - Batch 1200: Loss = 0.0000
2025-01-24 09:11:46,990 - Batch 1300: Loss = 0.0000
2025-01-24 09:11:47,629 - Batch 1400: Loss = 0.0000
2025-01-24 09:11:48,271 - Batch 1500: Loss = 0.0000
2025-01-24 09:11:50,548 - Train Loss: 0.0000
2025-01-24 09:11:50,549 - Test Loss: 68222.5740
2025-01-24 09:11:50,549 - FID Score: 0.1229
2025-01-24 09:11:50,549 - Commitment Loss: 0.0000
2025-01-24 09:11:50,549 - Codebook Loss: 0.0000
2025-01-24 09:11:50,549 - Entropy Reg: 0.0000
2025-01-24 09:11:50,549 - L2 Reg: 0.0001
2025-01-24 09:11:50,568 - 
Epoch 9/50
2025-01-24 09:11:50,678 - Batch 0: Loss = 0.0000
2025-01-24 09:11:51,297 - Batch 100: Loss = 0.0000
2025-01-24 09:11:51,932 - Batch 200: Loss = 0.0000
2025-01-24 09:11:52,576 - Batch 300: Loss = 0.0000
2025-01-24 09:11:53,232 - Batch 400: Loss = 0.0000
2025-01-24 09:11:53,890 - Batch 500: Loss = 0.0000
2025-01-24 09:11:54,548 - Batch 600: Loss = 0.0000
2025-01-24 09:11:55,205 - Batch 700: Loss = 0.0000
2025-01-24 09:11:55,863 - Batch 800: Loss = 0.0000
2025-01-24 09:11:56,524 - Batch 900: Loss = 0.0000
2025-01-24 09:11:57,185 - Batch 1000: Loss = 0.0000
2025-01-24 09:11:57,844 - Batch 1100: Loss = 0.0000
2025-01-24 09:11:58,500 - Batch 1200: Loss = 0.0000
2025-01-24 09:11:59,155 - Batch 1300: Loss = 0.0000
2025-01-24 09:11:59,818 - Batch 1400: Loss = 0.0000
2025-01-24 09:12:00,476 - Batch 1500: Loss = 0.0000
2025-01-24 09:12:02,756 - Train Loss: 0.0000
2025-01-24 09:12:02,756 - Test Loss: 68222.5662
2025-01-24 09:12:02,756 - FID Score: 0.1229
2025-01-24 09:12:02,756 - Commitment Loss: 0.0000
2025-01-24 09:12:02,756 - Codebook Loss: 0.0000
2025-01-24 09:12:02,756 - Entropy Reg: 0.0000
2025-01-24 09:12:02,756 - L2 Reg: 0.0001
2025-01-24 09:12:02,776 - 
Epoch 10/50
2025-01-24 09:12:02,873 - Batch 0: Loss = 0.0000
2025-01-24 09:12:03,378 - Batch 100: Loss = 0.0000
2025-01-24 09:12:03,862 - Batch 200: Loss = 0.0000
2025-01-24 09:12:04,330 - Batch 300: Loss = 0.0000
2025-01-24 09:12:04,801 - Batch 400: Loss = 0.0000
2025-01-24 09:12:05,266 - Batch 500: Loss = 0.0000
2025-01-24 09:12:05,734 - Batch 600: Loss = 0.0000
2025-01-24 09:12:06,210 - Batch 700: Loss = 0.0000
2025-01-24 09:12:06,700 - Batch 800: Loss = 0.0000
2025-01-24 09:12:07,185 - Batch 900: Loss = 0.0000
2025-01-24 09:12:07,677 - Batch 1000: Loss = 0.0000
2025-01-24 09:12:08,174 - Batch 1100: Loss = 0.0000
2025-01-24 09:12:08,684 - Batch 1200: Loss = 0.0000
2025-01-24 09:12:09,156 - Batch 1300: Loss = 0.0000
2025-01-24 09:12:09,641 - Batch 1400: Loss = 0.0000
2025-01-24 09:12:10,126 - Batch 1500: Loss = 0.0000
2025-01-24 09:12:12,303 - Train Loss: 0.0000
2025-01-24 09:12:12,303 - Test Loss: 68222.5662
2025-01-24 09:12:12,303 - FID Score: 0.1229
2025-01-24 09:12:12,303 - Commitment Loss: 0.0000
2025-01-24 09:12:12,303 - Codebook Loss: 0.0000
2025-01-24 09:12:12,303 - Entropy Reg: 0.0000
2025-01-24 09:12:12,303 - L2 Reg: 0.0001
2025-01-24 09:12:12,323 - 
Epoch 11/50
2025-01-24 09:12:12,419 - Batch 0: Loss = 0.0000
2025-01-24 09:12:13,054 - Batch 100: Loss = 0.0000
2025-01-24 09:12:13,714 - Batch 200: Loss = 0.0000
2025-01-24 09:12:14,383 - Batch 300: Loss = 0.0000
2025-01-24 09:12:15,049 - Batch 400: Loss = 0.0000
2025-01-24 09:12:15,733 - Batch 500: Loss = 0.0000
2025-01-24 09:12:16,438 - Batch 600: Loss = 0.0000
2025-01-24 09:12:17,139 - Batch 700: Loss = 0.0000
2025-01-24 09:12:17,842 - Batch 800: Loss = 0.0000
2025-01-24 09:12:18,543 - Batch 900: Loss = 0.0000
2025-01-24 09:12:19,246 - Batch 1000: Loss = 0.0000
2025-01-24 09:12:19,946 - Batch 1100: Loss = 0.0000
2025-01-24 09:12:20,582 - Batch 1200: Loss = 0.0000
2025-01-24 09:12:21,223 - Batch 1300: Loss = 0.0000
2025-01-24 09:12:21,875 - Batch 1400: Loss = 0.0000
2025-01-24 09:12:22,547 - Batch 1500: Loss = 0.0000
2025-01-24 09:12:24,844 - Train Loss: 0.0000
2025-01-24 09:12:24,845 - Test Loss: 68222.5662
2025-01-24 09:12:24,845 - FID Score: 0.1229
2025-01-24 09:12:24,845 - Commitment Loss: 0.0000
2025-01-24 09:12:24,845 - Codebook Loss: 0.0000
2025-01-24 09:12:24,845 - Entropy Reg: 0.0000
2025-01-24 09:12:24,845 - L2 Reg: 0.0001
2025-01-24 09:12:24,850 - Saved checkpoint at epoch 10
2025-01-24 09:12:24,870 - 
Epoch 12/50
2025-01-24 09:12:24,964 - Batch 0: Loss = 0.0000
2025-01-24 09:12:25,511 - Batch 100: Loss = 0.0000
2025-01-24 09:12:26,053 - Batch 200: Loss = 0.0000
2025-01-24 09:12:26,551 - Batch 300: Loss = 0.0000
2025-01-24 09:12:27,049 - Batch 400: Loss = 0.0000
2025-01-24 09:12:27,530 - Batch 500: Loss = 0.0000
2025-01-24 09:12:28,038 - Batch 600: Loss = 0.0000
2025-01-24 09:12:28,629 - Batch 700: Loss = 0.0000
2025-01-24 09:12:29,260 - Batch 800: Loss = 0.0000
2025-01-24 09:12:29,889 - Batch 900: Loss = 0.0000
2025-01-24 09:12:30,519 - Batch 1000: Loss = 0.0000
2025-01-24 09:12:31,124 - Batch 1100: Loss = 0.0000
2025-01-24 09:12:31,631 - Batch 1200: Loss = 0.0000
2025-01-24 09:12:32,145 - Batch 1300: Loss = 0.0000
2025-01-24 09:12:32,675 - Batch 1400: Loss = 0.0000
2025-01-24 09:12:33,199 - Batch 1500: Loss = 0.0000
2025-01-24 09:12:35,400 - Train Loss: 0.0000
2025-01-24 09:12:35,400 - Test Loss: 68222.5662
2025-01-24 09:12:35,400 - FID Score: 0.1229
2025-01-24 09:12:35,400 - Commitment Loss: 0.0000
2025-01-24 09:12:35,400 - Codebook Loss: 0.0000
2025-01-24 09:12:35,400 - Entropy Reg: 0.0000
2025-01-24 09:12:35,400 - L2 Reg: 0.0001
2025-01-24 09:12:35,420 - 
Epoch 13/50
2025-01-24 09:12:35,516 - Batch 0: Loss = 0.0000
2025-01-24 09:12:36,130 - Batch 100: Loss = 0.0000
2025-01-24 09:12:36,762 - Batch 200: Loss = 0.0000
2025-01-24 09:12:37,388 - Batch 300: Loss = 0.0000
2025-01-24 09:12:37,969 - Batch 400: Loss = 0.0000
2025-01-24 09:12:38,560 - Batch 500: Loss = 0.0000
2025-01-24 09:12:39,169 - Batch 600: Loss = 0.0000
2025-01-24 09:12:39,789 - Batch 700: Loss = 0.0000
2025-01-24 09:12:40,411 - Batch 800: Loss = 0.0000
2025-01-24 09:12:41,127 - Batch 900: Loss = 0.0000
2025-01-24 09:12:41,983 - Batch 1000: Loss = 0.0000
2025-01-24 09:12:43,028 - Batch 1100: Loss = 0.0000
2025-01-24 09:12:44,082 - Batch 1200: Loss = 0.0000
2025-01-24 09:12:45,128 - Batch 1300: Loss = 0.0000
2025-01-24 09:12:46,081 - Batch 1400: Loss = 0.0000
2025-01-24 09:12:47,009 - Batch 1500: Loss = 0.0000
2025-01-24 09:12:49,569 - Train Loss: 0.0000
2025-01-24 09:12:49,569 - Test Loss: 68222.5662
2025-01-24 09:12:49,569 - FID Score: 0.1229
2025-01-24 09:12:49,569 - Commitment Loss: 0.0000
2025-01-24 09:12:49,569 - Codebook Loss: 0.0000
2025-01-24 09:12:49,570 - Entropy Reg: 0.0000
2025-01-24 09:12:49,570 - L2 Reg: 0.0001
2025-01-24 09:12:49,593 - 
Epoch 14/50
2025-01-24 09:12:49,740 - Batch 0: Loss = 0.0000
2025-01-24 09:12:50,638 - Batch 100: Loss = 0.0000
2025-01-24 09:12:51,517 - Batch 200: Loss = 0.0000
2025-01-24 09:12:52,397 - Batch 300: Loss = 0.0000
2025-01-24 09:12:53,276 - Batch 400: Loss = 0.0000
2025-01-24 09:12:54,157 - Batch 500: Loss = 0.0000
2025-01-24 09:12:55,042 - Batch 600: Loss = 0.0000
2025-01-24 09:12:55,929 - Batch 700: Loss = 0.0000
2025-01-24 09:12:56,680 - Batch 800: Loss = 0.0000
2025-01-24 09:12:57,417 - Batch 900: Loss = 0.0000
2025-01-24 09:12:58,149 - Batch 1000: Loss = 0.0000
2025-01-24 09:12:58,879 - Batch 1100: Loss = 0.0000
2025-01-24 09:12:59,606 - Batch 1200: Loss = 0.0000
2025-01-24 09:13:00,141 - Batch 1300: Loss = 0.0000
2025-01-24 09:13:00,605 - Batch 1400: Loss = 0.0000
2025-01-24 09:13:01,094 - Batch 1500: Loss = 0.0000
2025-01-24 09:13:03,284 - Train Loss: 0.0000
2025-01-24 09:13:03,285 - Test Loss: 68222.5583
2025-01-24 09:13:03,285 - FID Score: 0.1229
2025-01-24 09:13:03,285 - Commitment Loss: 0.0000
2025-01-24 09:13:03,285 - Codebook Loss: 0.0000
2025-01-24 09:13:03,285 - Entropy Reg: 0.0000
2025-01-24 09:13:03,285 - L2 Reg: 0.0001
2025-01-24 09:13:03,304 - 
Epoch 15/50
2025-01-24 09:13:03,404 - Batch 0: Loss = 0.0000
2025-01-24 09:13:03,905 - Batch 100: Loss = 0.0000
2025-01-24 09:13:04,369 - Batch 200: Loss = 0.0000
2025-01-24 09:13:04,942 - Batch 300: Loss = 0.0000
2025-01-24 09:13:05,568 - Batch 400: Loss = 0.0000
2025-01-24 09:13:06,171 - Batch 500: Loss = 0.0000
2025-01-24 09:13:06,787 - Batch 600: Loss = 0.0000
2025-01-24 09:13:07,593 - Batch 700: Loss = 0.0000
2025-01-24 09:13:08,476 - Batch 800: Loss = 0.0000
2025-01-24 09:13:09,366 - Batch 900: Loss = 0.0000
2025-01-24 09:13:10,248 - Batch 1000: Loss = 0.0000
2025-01-24 09:13:11,129 - Batch 1100: Loss = 0.0000
2025-01-24 09:13:12,024 - Batch 1200: Loss = 0.0000
2025-01-24 09:13:13,039 - Batch 1300: Loss = 0.0000
2025-01-24 09:13:14,047 - Batch 1400: Loss = 0.0000
2025-01-24 09:13:15,065 - Batch 1500: Loss = 0.0000
2025-01-24 09:13:17,717 - Train Loss: 0.0000
2025-01-24 09:13:17,717 - Test Loss: 68222.5583
2025-01-24 09:13:17,718 - FID Score: 0.1229
2025-01-24 09:13:17,718 - Commitment Loss: 0.0000
2025-01-24 09:13:17,718 - Codebook Loss: 0.0000
2025-01-24 09:13:17,718 - Entropy Reg: 0.0000
2025-01-24 09:13:17,718 - L2 Reg: 0.0001
2025-01-24 09:13:17,741 - 
Epoch 16/50
2025-01-24 09:13:17,889 - Batch 0: Loss = 0.0000
2025-01-24 09:13:18,717 - Batch 100: Loss = 0.0000
2025-01-24 09:13:19,503 - Batch 200: Loss = 0.0000
2025-01-24 09:13:20,283 - Batch 300: Loss = 0.0000
2025-01-24 09:13:20,766 - Batch 400: Loss = 0.0000
2025-01-24 09:13:21,237 - Batch 500: Loss = 0.0000
2025-01-24 09:13:21,715 - Batch 600: Loss = 0.0000
2025-01-24 09:13:22,180 - Batch 700: Loss = 0.0000
2025-01-24 09:13:22,653 - Batch 800: Loss = 0.0000
2025-01-24 09:13:23,116 - Batch 900: Loss = 0.0000
2025-01-24 09:13:23,568 - Batch 1000: Loss = 0.0000
2025-01-24 09:13:24,020 - Batch 1100: Loss = 0.0000
2025-01-24 09:13:24,475 - Batch 1200: Loss = 0.0000
2025-01-24 09:13:24,924 - Batch 1300: Loss = 0.0000
2025-01-24 09:13:25,373 - Batch 1400: Loss = 0.0000
2025-01-24 09:13:25,821 - Batch 1500: Loss = 0.0000
2025-01-24 09:13:27,998 - Train Loss: 0.0000
2025-01-24 09:13:27,999 - Test Loss: 68222.5583
2025-01-24 09:13:27,999 - FID Score: 0.1229
2025-01-24 09:13:27,999 - Commitment Loss: 0.0000
2025-01-24 09:13:27,999 - Codebook Loss: 0.0000
2025-01-24 09:13:27,999 - Entropy Reg: 0.0000
2025-01-24 09:13:27,999 - L2 Reg: 0.0001
2025-01-24 09:13:28,018 - 
Epoch 17/50
2025-01-24 09:13:28,112 - Batch 0: Loss = 0.0000
2025-01-24 09:13:28,596 - Batch 100: Loss = 0.0000
2025-01-24 09:13:29,023 - Batch 200: Loss = 0.0000
2025-01-24 09:13:29,451 - Batch 300: Loss = 0.0000
2025-01-24 09:13:29,879 - Batch 400: Loss = 0.0000
2025-01-24 09:13:30,307 - Batch 500: Loss = 0.0000
2025-01-24 09:13:30,738 - Batch 600: Loss = 0.0000
2025-01-24 09:13:31,168 - Batch 700: Loss = 0.0000
2025-01-24 09:13:31,596 - Batch 800: Loss = 0.0000
2025-01-24 09:13:32,035 - Batch 900: Loss = 0.0000
2025-01-24 09:13:32,466 - Batch 1000: Loss = 0.0000
2025-01-24 09:13:32,894 - Batch 1100: Loss = 0.0000
2025-01-24 09:13:33,322 - Batch 1200: Loss = 0.0000
2025-01-24 09:13:33,750 - Batch 1300: Loss = 0.0000
2025-01-24 09:13:34,178 - Batch 1400: Loss = 0.0000
2025-01-24 09:13:34,609 - Batch 1500: Loss = 0.0000
2025-01-24 09:13:36,746 - Train Loss: 0.0000
2025-01-24 09:13:36,746 - Test Loss: 68222.5583
2025-01-24 09:13:36,746 - FID Score: 0.1229
2025-01-24 09:13:36,746 - Commitment Loss: 0.0000
2025-01-24 09:13:36,746 - Codebook Loss: 0.0000
2025-01-24 09:13:36,746 - Entropy Reg: 0.0000
2025-01-24 09:13:36,746 - L2 Reg: 0.0001
2025-01-24 09:13:36,765 - 
Epoch 18/50
2025-01-24 09:13:36,859 - Batch 0: Loss = 0.0000
2025-01-24 09:13:37,352 - Batch 100: Loss = 0.0000
2025-01-24 09:13:37,793 - Batch 200: Loss = 0.0000
2025-01-24 09:13:38,271 - Batch 300: Loss = 0.0000
2025-01-24 09:13:38,842 - Batch 400: Loss = 0.0000
2025-01-24 09:13:39,473 - Batch 500: Loss = 0.0000
2025-01-24 09:13:40,106 - Batch 600: Loss = 0.0000
2025-01-24 09:13:40,737 - Batch 700: Loss = 0.0000
2025-01-24 09:13:41,370 - Batch 800: Loss = 0.0000
2025-01-24 09:13:41,986 - Batch 900: Loss = 0.0000
2025-01-24 09:13:42,594 - Batch 1000: Loss = 0.0000
2025-01-24 09:13:43,214 - Batch 1100: Loss = 0.0000
2025-01-24 09:13:43,837 - Batch 1200: Loss = 0.0000
2025-01-24 09:13:44,460 - Batch 1300: Loss = 0.0000
2025-01-24 09:13:45,094 - Batch 1400: Loss = 0.0000
2025-01-24 09:13:45,729 - Batch 1500: Loss = 0.0000
2025-01-24 09:13:48,017 - Train Loss: 0.0000
2025-01-24 09:13:48,017 - Test Loss: 68222.5583
2025-01-24 09:13:48,017 - FID Score: 0.1229
2025-01-24 09:13:48,018 - Commitment Loss: 0.0000
2025-01-24 09:13:48,018 - Codebook Loss: 0.0000
2025-01-24 09:13:48,018 - Entropy Reg: 0.0000
2025-01-24 09:13:48,018 - L2 Reg: 0.0001
2025-01-24 09:13:48,037 - 
Epoch 19/50
2025-01-24 09:13:48,130 - Batch 0: Loss = 0.0000
2025-01-24 09:13:48,723 - Batch 100: Loss = 0.0000
2025-01-24 09:13:49,174 - Batch 200: Loss = 0.0000
2025-01-24 09:13:49,605 - Batch 300: Loss = 0.0000
2025-01-24 09:13:50,036 - Batch 400: Loss = 0.0000
2025-01-24 09:13:50,468 - Batch 500: Loss = 0.0000
2025-01-24 09:13:50,902 - Batch 600: Loss = 0.0000
2025-01-24 09:13:51,343 - Batch 700: Loss = 0.0000
2025-01-24 09:13:51,777 - Batch 800: Loss = 0.0000
2025-01-24 09:13:52,222 - Batch 900: Loss = 0.0000
2025-01-24 09:13:52,658 - Batch 1000: Loss = 0.0000
2025-01-24 09:13:53,090 - Batch 1100: Loss = 0.0000
2025-01-24 09:13:53,523 - Batch 1200: Loss = 0.0000
2025-01-24 09:13:53,956 - Batch 1300: Loss = 0.0000
2025-01-24 09:13:54,389 - Batch 1400: Loss = 0.0000
2025-01-24 09:13:54,822 - Batch 1500: Loss = 0.0000
2025-01-24 09:13:56,981 - Train Loss: 0.0000
2025-01-24 09:13:56,981 - Test Loss: 68222.5583
2025-01-24 09:13:56,981 - FID Score: 0.1229
2025-01-24 09:13:56,981 - Commitment Loss: 0.0000
2025-01-24 09:13:56,981 - Codebook Loss: 0.0000
2025-01-24 09:13:56,981 - Entropy Reg: 0.0000
2025-01-24 09:13:56,981 - L2 Reg: 0.0001
2025-01-24 09:13:57,000 - 
Epoch 20/50
2025-01-24 09:13:57,093 - Batch 0: Loss = 0.0000
2025-01-24 09:13:57,587 - Batch 100: Loss = 0.0000
2025-01-24 09:13:58,032 - Batch 200: Loss = 0.0000
2025-01-24 09:13:58,478 - Batch 300: Loss = 0.0000
2025-01-24 09:13:58,922 - Batch 400: Loss = 0.0000
2025-01-24 09:13:59,364 - Batch 500: Loss = 0.0000
2025-01-24 09:13:59,816 - Batch 600: Loss = 0.0000
2025-01-24 09:14:00,297 - Batch 700: Loss = 0.0000
2025-01-24 09:14:00,781 - Batch 800: Loss = 0.0000
2025-01-24 09:14:01,270 - Batch 900: Loss = 0.0000
2025-01-24 09:14:01,753 - Batch 1000: Loss = 0.0000
2025-01-24 09:14:02,234 - Batch 1100: Loss = 0.0000
2025-01-24 09:14:02,697 - Batch 1200: Loss = 0.0000
2025-01-24 09:14:03,139 - Batch 1300: Loss = 0.0000
2025-01-24 09:14:03,581 - Batch 1400: Loss = 0.0000
2025-01-24 09:14:04,120 - Batch 1500: Loss = 0.0000
2025-01-24 09:14:06,387 - Train Loss: 0.0000
2025-01-24 09:14:06,387 - Test Loss: 68222.5583
2025-01-24 09:14:06,387 - FID Score: 0.1229
2025-01-24 09:14:06,387 - Commitment Loss: 0.0000
2025-01-24 09:14:06,387 - Codebook Loss: 0.0000
2025-01-24 09:14:06,387 - Entropy Reg: 0.0000
2025-01-24 09:14:06,387 - L2 Reg: 0.0001
2025-01-24 09:14:06,406 - 
Epoch 21/50
2025-01-24 09:14:06,498 - Batch 0: Loss = 0.0000
2025-01-24 09:14:06,991 - Batch 100: Loss = 0.0000
2025-01-24 09:14:07,440 - Batch 200: Loss = 0.0000
2025-01-24 09:14:07,897 - Batch 300: Loss = 0.0000
2025-01-24 09:14:08,345 - Batch 400: Loss = 0.0000
2025-01-24 09:14:08,796 - Batch 500: Loss = 0.0000
2025-01-24 09:14:09,242 - Batch 600: Loss = 0.0000
2025-01-24 09:14:09,688 - Batch 700: Loss = 0.0000
2025-01-24 09:14:10,132 - Batch 800: Loss = 0.0000
2025-01-24 09:14:10,575 - Batch 900: Loss = 0.0000
2025-01-24 09:14:11,019 - Batch 1000: Loss = 0.0000
2025-01-24 09:14:11,469 - Batch 1100: Loss = 0.0000
2025-01-24 09:14:11,914 - Batch 1200: Loss = 0.0000
2025-01-24 09:14:12,357 - Batch 1300: Loss = 0.0000
2025-01-24 09:14:12,807 - Batch 1400: Loss = 0.0000
2025-01-24 09:14:13,251 - Batch 1500: Loss = 0.0000
2025-01-24 09:14:15,399 - Train Loss: 0.0000
2025-01-24 09:14:15,399 - Test Loss: 68222.5583
2025-01-24 09:14:15,399 - FID Score: 0.1229
2025-01-24 09:14:15,400 - Commitment Loss: 0.0000
2025-01-24 09:14:15,400 - Codebook Loss: 0.0000
2025-01-24 09:14:15,400 - Entropy Reg: 0.0000
2025-01-24 09:14:15,400 - L2 Reg: 0.0001
2025-01-24 09:14:15,405 - Saved checkpoint at epoch 20
2025-01-24 09:14:15,424 - 
Epoch 22/50
2025-01-24 09:14:15,517 - Batch 0: Loss = 0.0000
2025-01-24 09:14:16,008 - Batch 100: Loss = 0.0000
2025-01-24 09:14:16,461 - Batch 200: Loss = 0.0000
2025-01-24 09:14:16,914 - Batch 300: Loss = 0.0000
2025-01-24 09:14:17,402 - Batch 400: Loss = 0.0000
2025-01-24 09:14:17,885 - Batch 500: Loss = 0.0000
2025-01-24 09:14:18,333 - Batch 600: Loss = 0.0000
2025-01-24 09:14:18,780 - Batch 700: Loss = 0.0000
2025-01-24 09:14:19,226 - Batch 800: Loss = 0.0000
2025-01-24 09:14:19,672 - Batch 900: Loss = 0.0000
2025-01-24 09:14:20,118 - Batch 1000: Loss = 0.0000
2025-01-24 09:14:20,563 - Batch 1100: Loss = 0.0000
2025-01-24 09:14:21,010 - Batch 1200: Loss = 0.0000
2025-01-24 09:14:21,461 - Batch 1300: Loss = 0.0000
2025-01-24 09:14:21,905 - Batch 1400: Loss = 0.0000
2025-01-24 09:14:22,349 - Batch 1500: Loss = 0.0000
2025-01-24 09:14:24,516 - Train Loss: 0.0000
2025-01-24 09:14:24,516 - Test Loss: 68222.5583
2025-01-24 09:14:24,516 - FID Score: 0.1229
2025-01-24 09:14:24,516 - Commitment Loss: 0.0000
2025-01-24 09:14:24,516 - Codebook Loss: 0.0000
2025-01-24 09:14:24,516 - Entropy Reg: 0.0000
2025-01-24 09:14:24,516 - L2 Reg: 0.0001
2025-01-24 09:14:24,536 - 
Epoch 23/50
2025-01-24 09:14:24,627 - Batch 0: Loss = 0.0000
2025-01-24 09:14:25,119 - Batch 100: Loss = 0.0000
2025-01-24 09:14:25,560 - Batch 200: Loss = 0.0000
2025-01-24 09:14:26,018 - Batch 300: Loss = 0.0000
2025-01-24 09:14:26,497 - Batch 400: Loss = 0.0000
2025-01-24 09:14:26,976 - Batch 500: Loss = 0.0000
2025-01-24 09:14:27,453 - Batch 600: Loss = 0.0000
2025-01-24 09:14:28,300 - Batch 700: Loss = 0.0000
2025-01-24 09:14:29,222 - Batch 800: Loss = 0.0000
2025-01-24 09:14:30,127 - Batch 900: Loss = 0.0000
2025-01-24 09:14:31,033 - Batch 1000: Loss = 0.0000
2025-01-24 09:14:32,037 - Batch 1100: Loss = 0.0000
2025-01-24 09:14:33,055 - Batch 1200: Loss = 0.0000
2025-01-24 09:14:34,070 - Batch 1300: Loss = 0.0000
2025-01-24 09:14:35,107 - Batch 1400: Loss = 0.0000
2025-01-24 09:14:36,145 - Batch 1500: Loss = 0.0000
2025-01-24 09:14:38,775 - Train Loss: 0.0000
2025-01-24 09:14:38,776 - Test Loss: 68222.5583
2025-01-24 09:14:38,776 - FID Score: 0.1229
2025-01-24 09:14:38,776 - Commitment Loss: 0.0000
2025-01-24 09:14:38,776 - Codebook Loss: 0.0000
2025-01-24 09:14:38,776 - Entropy Reg: 0.0000
2025-01-24 09:14:38,776 - L2 Reg: 0.0001
2025-01-24 09:14:38,799 - 
Epoch 24/50
2025-01-24 09:14:38,943 - Batch 0: Loss = 0.0000
2025-01-24 09:14:39,836 - Batch 100: Loss = 0.0000
2025-01-24 09:14:40,722 - Batch 200: Loss = 0.0000
2025-01-24 09:14:41,559 - Batch 300: Loss = 0.0000
2025-01-24 09:14:42,242 - Batch 400: Loss = 0.0000
2025-01-24 09:14:42,918 - Batch 500: Loss = 0.0000
2025-01-24 09:14:43,592 - Batch 600: Loss = 0.0000
2025-01-24 09:14:44,263 - Batch 700: Loss = 0.0000
2025-01-24 09:14:44,948 - Batch 800: Loss = 0.0000
2025-01-24 09:14:45,638 - Batch 900: Loss = 0.0000
2025-01-24 09:14:46,327 - Batch 1000: Loss = 0.0000
2025-01-24 09:14:47,017 - Batch 1100: Loss = 0.0000
2025-01-24 09:14:47,918 - Batch 1200: Loss = 0.0000
2025-01-24 09:14:48,835 - Batch 1300: Loss = 0.0000
2025-01-24 09:14:49,751 - Batch 1400: Loss = 0.0000
2025-01-24 09:14:50,668 - Batch 1500: Loss = 0.0000
2025-01-24 09:14:53,240 - Train Loss: 0.0000
2025-01-24 09:14:53,240 - Test Loss: 68222.5583
2025-01-24 09:14:53,240 - FID Score: 0.1229
2025-01-24 09:14:53,240 - Commitment Loss: 0.0000
2025-01-24 09:14:53,240 - Codebook Loss: 0.0000
2025-01-24 09:14:53,240 - Entropy Reg: 0.0000
2025-01-24 09:14:53,241 - L2 Reg: 0.0001
2025-01-24 09:14:53,263 - 
Epoch 25/50
2025-01-24 09:14:53,406 - Batch 0: Loss = 0.0000
2025-01-24 09:14:53,974 - Batch 100: Loss = 0.0000
2025-01-24 09:14:54,421 - Batch 200: Loss = 0.0000
2025-01-24 09:14:54,867 - Batch 300: Loss = 0.0000
2025-01-24 09:14:55,314 - Batch 400: Loss = 0.0000
2025-01-24 09:14:55,762 - Batch 500: Loss = 0.0000
2025-01-24 09:14:56,212 - Batch 600: Loss = 0.0000
2025-01-24 09:14:56,662 - Batch 700: Loss = 0.0000
2025-01-24 09:14:57,108 - Batch 800: Loss = 0.0000
2025-01-24 09:14:57,558 - Batch 900: Loss = 0.0000
2025-01-24 09:14:58,008 - Batch 1000: Loss = 0.0000
2025-01-24 09:14:58,458 - Batch 1100: Loss = 0.0000
2025-01-24 09:14:58,910 - Batch 1200: Loss = 0.0000
2025-01-24 09:14:59,358 - Batch 1300: Loss = 0.0000
2025-01-24 09:14:59,802 - Batch 1400: Loss = 0.0000
2025-01-24 09:15:00,229 - Batch 1500: Loss = 0.0000
2025-01-24 09:15:02,353 - Train Loss: 0.0000
2025-01-24 09:15:02,353 - Test Loss: 68222.5583
2025-01-24 09:15:02,353 - FID Score: 0.1229
2025-01-24 09:15:02,353 - Commitment Loss: 0.0000
2025-01-24 09:15:02,353 - Codebook Loss: 0.0000
2025-01-24 09:15:02,354 - Entropy Reg: 0.0000
2025-01-24 09:15:02,354 - L2 Reg: 0.0001
2025-01-24 09:15:02,373 - 
Epoch 26/50
2025-01-24 09:15:02,470 - Batch 0: Loss = 0.0000
2025-01-24 09:15:03,076 - Batch 100: Loss = 0.0000
2025-01-24 09:15:03,765 - Batch 200: Loss = 0.0000
2025-01-24 09:15:04,506 - Batch 300: Loss = 0.0000
2025-01-24 09:15:05,260 - Batch 400: Loss = 0.0000
2025-01-24 09:15:06,021 - Batch 500: Loss = 0.0000
2025-01-24 09:15:06,778 - Batch 600: Loss = 0.0000
2025-01-24 09:15:07,533 - Batch 700: Loss = 0.0000
2025-01-24 09:15:08,286 - Batch 800: Loss = 0.0000
2025-01-24 09:15:09,044 - Batch 900: Loss = 0.0000
2025-01-24 09:15:09,807 - Batch 1000: Loss = 0.0000
2025-01-24 09:15:10,518 - Batch 1100: Loss = 0.0000
2025-01-24 09:15:11,182 - Batch 1200: Loss = 0.0000
2025-01-24 09:15:11,897 - Batch 1300: Loss = 0.0000
2025-01-24 09:15:12,627 - Batch 1400: Loss = 0.0000
2025-01-24 09:15:13,364 - Batch 1500: Loss = 0.0000
2025-01-24 09:15:15,741 - Train Loss: 0.0000
2025-01-24 09:15:15,742 - Test Loss: 68222.5583
2025-01-24 09:15:15,742 - FID Score: 0.1229
2025-01-24 09:15:15,742 - Commitment Loss: 0.0000
2025-01-24 09:15:15,742 - Codebook Loss: 0.0000
2025-01-24 09:15:15,742 - Entropy Reg: 0.0000
2025-01-24 09:15:15,742 - L2 Reg: 0.0001
2025-01-24 09:15:15,765 - 
Epoch 27/50
2025-01-24 09:15:15,907 - Batch 0: Loss = 0.0000
2025-01-24 09:15:16,648 - Batch 100: Loss = 0.0000
2025-01-24 09:15:17,420 - Batch 200: Loss = 0.0000
2025-01-24 09:15:18,297 - Batch 300: Loss = 0.0000
2025-01-24 09:15:19,178 - Batch 400: Loss = 0.0000
2025-01-24 09:15:19,908 - Batch 500: Loss = 0.0000
2025-01-24 09:15:20,642 - Batch 600: Loss = 0.0000
2025-01-24 09:15:21,426 - Batch 700: Loss = 0.0000
2025-01-24 09:15:22,227 - Batch 800: Loss = 0.0000
2025-01-24 09:15:23,141 - Batch 900: Loss = 0.0000
2025-01-24 09:15:24,054 - Batch 1000: Loss = 0.0000
2025-01-24 09:15:24,967 - Batch 1100: Loss = 0.0000
2025-01-24 09:15:25,882 - Batch 1200: Loss = 0.0000
2025-01-24 09:15:26,790 - Batch 1300: Loss = 0.0000
2025-01-24 09:15:27,683 - Batch 1400: Loss = 0.0000
2025-01-24 09:15:28,582 - Batch 1500: Loss = 0.0000
2025-01-24 09:15:31,151 - Train Loss: 0.0000
2025-01-24 09:15:31,151 - Test Loss: 68222.5583
2025-01-24 09:15:31,152 - FID Score: 0.1229
2025-01-24 09:15:31,152 - Commitment Loss: 0.0000
2025-01-24 09:15:31,152 - Codebook Loss: 0.0000
2025-01-24 09:15:31,152 - Entropy Reg: 0.0000
2025-01-24 09:15:31,152 - L2 Reg: 0.0001
2025-01-24 09:15:31,175 - 
Epoch 28/50
2025-01-24 09:15:31,318 - Batch 0: Loss = 0.0000
2025-01-24 09:15:32,202 - Batch 100: Loss = 0.0000
2025-01-24 09:15:33,079 - Batch 200: Loss = 0.0000
2025-01-24 09:15:33,977 - Batch 300: Loss = 0.0000
2025-01-24 09:15:34,875 - Batch 400: Loss = 0.0000
2025-01-24 09:15:35,655 - Batch 500: Loss = 0.0000
2025-01-24 09:15:36,338 - Batch 600: Loss = 0.0000
2025-01-24 09:15:37,020 - Batch 700: Loss = 0.0000
2025-01-24 09:15:37,701 - Batch 800: Loss = 0.0000
2025-01-24 09:15:38,375 - Batch 900: Loss = 0.0000
2025-01-24 09:15:39,048 - Batch 1000: Loss = 0.0000
2025-01-24 09:15:39,702 - Batch 1100: Loss = 0.0000
2025-01-24 09:15:40,371 - Batch 1200: Loss = 0.0000
2025-01-24 09:15:41,040 - Batch 1300: Loss = 0.0000
2025-01-24 09:15:41,712 - Batch 1400: Loss = 0.0000
2025-01-24 09:15:42,379 - Batch 1500: Loss = 0.0000
2025-01-24 09:15:44,637 - Train Loss: 0.0000
2025-01-24 09:15:44,637 - Test Loss: 68222.5583
2025-01-24 09:15:44,637 - FID Score: 0.1229
2025-01-24 09:15:44,637 - Commitment Loss: 0.0000
2025-01-24 09:15:44,637 - Codebook Loss: 0.0000
2025-01-24 09:15:44,637 - Entropy Reg: 0.0000
2025-01-24 09:15:44,637 - L2 Reg: 0.0001
2025-01-24 09:15:44,657 - 
Epoch 29/50
2025-01-24 09:15:44,747 - Batch 0: Loss = 0.0000
2025-01-24 09:15:45,338 - Batch 100: Loss = 0.0000
2025-01-24 09:15:45,916 - Batch 200: Loss = 0.0000
2025-01-24 09:15:46,568 - Batch 300: Loss = 0.0000
2025-01-24 09:15:47,304 - Batch 400: Loss = 0.0000
2025-01-24 09:15:48,241 - Batch 500: Loss = 0.0000
2025-01-24 09:15:49,276 - Batch 600: Loss = 0.0000
2025-01-24 09:15:50,243 - Batch 700: Loss = 0.0000
2025-01-24 09:15:51,071 - Batch 800: Loss = 0.0000
2025-01-24 09:15:51,896 - Batch 900: Loss = 0.0000
2025-01-24 09:15:52,689 - Batch 1000: Loss = 0.0000
2025-01-24 09:15:53,326 - Batch 1100: Loss = 0.0000
2025-01-24 09:15:53,959 - Batch 1200: Loss = 0.0000
2025-01-24 09:15:54,581 - Batch 1300: Loss = 0.0000
2025-01-24 09:15:55,206 - Batch 1400: Loss = 0.0000
2025-01-24 09:15:55,840 - Batch 1500: Loss = 0.0000
2025-01-24 09:15:58,124 - Train Loss: 0.0000
2025-01-24 09:15:58,124 - Test Loss: 68222.5583
2025-01-24 09:15:58,124 - FID Score: 0.1229
2025-01-24 09:15:58,124 - Commitment Loss: 0.0000
2025-01-24 09:15:58,124 - Codebook Loss: 0.0000
2025-01-24 09:15:58,124 - Entropy Reg: 0.0000
2025-01-24 09:15:58,124 - L2 Reg: 0.0001
2025-01-24 09:15:58,144 - 
Epoch 30/50
2025-01-24 09:15:58,237 - Batch 0: Loss = 0.0000
2025-01-24 09:15:58,777 - Batch 100: Loss = 0.0000
2025-01-24 09:15:59,440 - Batch 200: Loss = 0.0000
2025-01-24 09:16:00,327 - Batch 300: Loss = 0.0000
2025-01-24 09:16:01,227 - Batch 400: Loss = 0.0000
2025-01-24 09:16:02,091 - Batch 500: Loss = 0.0000
2025-01-24 09:16:02,846 - Batch 600: Loss = 0.0000
2025-01-24 09:16:03,613 - Batch 700: Loss = 0.0000
2025-01-24 09:16:04,503 - Batch 800: Loss = 0.0000
2025-01-24 09:16:05,275 - Batch 900: Loss = 0.0000
2025-01-24 09:16:06,032 - Batch 1000: Loss = 0.0000
2025-01-24 09:16:06,789 - Batch 1100: Loss = 0.0000
2025-01-24 09:16:07,541 - Batch 1200: Loss = 0.0000
2025-01-24 09:16:08,327 - Batch 1300: Loss = 0.0000
2025-01-24 09:16:09,220 - Batch 1400: Loss = 0.0000
2025-01-24 09:16:10,116 - Batch 1500: Loss = 0.0000
2025-01-24 09:16:12,654 - Train Loss: 0.0000
2025-01-24 09:16:12,655 - Test Loss: 68222.5583
2025-01-24 09:16:12,655 - FID Score: 0.1229
2025-01-24 09:16:12,655 - Commitment Loss: 0.0000
2025-01-24 09:16:12,655 - Codebook Loss: 0.0000
2025-01-24 09:16:12,655 - Entropy Reg: 0.0000
2025-01-24 09:16:12,655 - L2 Reg: 0.0001
2025-01-24 09:16:12,678 - 
Epoch 31/50
2025-01-24 09:16:12,822 - Batch 0: Loss = 0.0000
2025-01-24 09:16:13,675 - Batch 100: Loss = 0.0000
2025-01-24 09:16:14,673 - Batch 200: Loss = 0.0000
2025-01-24 09:16:15,637 - Batch 300: Loss = 0.0000
2025-01-24 09:16:16,503 - Batch 400: Loss = 0.0000
2025-01-24 09:16:17,426 - Batch 500: Loss = 0.0000
2025-01-24 09:16:18,448 - Batch 600: Loss = 0.0000
2025-01-24 09:16:19,483 - Batch 700: Loss = 0.0000
2025-01-24 09:16:20,537 - Batch 800: Loss = 0.0000
2025-01-24 09:16:21,555 - Batch 900: Loss = 0.0000
2025-01-24 09:16:22,576 - Batch 1000: Loss = 0.0000
2025-01-24 09:16:23,600 - Batch 1100: Loss = 0.0000
2025-01-24 09:16:24,646 - Batch 1200: Loss = 0.0000
2025-01-24 09:16:25,701 - Batch 1300: Loss = 0.0000
2025-01-24 09:16:26,626 - Batch 1400: Loss = 0.0000
2025-01-24 09:16:27,540 - Batch 1500: Loss = 0.0000
2025-01-24 09:16:30,031 - Train Loss: 0.0000
2025-01-24 09:16:30,032 - Test Loss: 68222.5583
2025-01-24 09:16:30,032 - FID Score: 0.1229
2025-01-24 09:16:30,032 - Commitment Loss: 0.0000
2025-01-24 09:16:30,032 - Codebook Loss: 0.0000
2025-01-24 09:16:30,032 - Entropy Reg: 0.0000
2025-01-24 09:16:30,032 - L2 Reg: 0.0001
2025-01-24 09:16:30,039 - Saved checkpoint at epoch 30
2025-01-24 09:16:30,063 - 
Epoch 32/50
2025-01-24 09:16:30,205 - Batch 0: Loss = 0.0000
2025-01-24 09:16:30,962 - Batch 100: Loss = 0.0000
2025-01-24 09:16:31,872 - Batch 200: Loss = 0.0000
2025-01-24 09:16:32,895 - Batch 300: Loss = 0.0000
2025-01-24 09:16:33,929 - Batch 400: Loss = 0.0000
2025-01-24 09:16:34,988 - Batch 500: Loss = 0.0000
2025-01-24 09:16:36,045 - Batch 600: Loss = 0.0000
2025-01-24 09:16:37,103 - Batch 700: Loss = 0.0000
2025-01-24 09:16:38,160 - Batch 800: Loss = 0.0000
2025-01-24 09:16:39,219 - Batch 900: Loss = 0.0000
2025-01-24 09:16:40,280 - Batch 1000: Loss = 0.0000
2025-01-24 09:16:41,339 - Batch 1100: Loss = 0.0000
2025-01-24 09:16:42,393 - Batch 1200: Loss = 0.0000
2025-01-24 09:16:43,451 - Batch 1300: Loss = 0.0000
2025-01-24 09:16:44,518 - Batch 1400: Loss = 0.0000
2025-01-24 09:16:45,581 - Batch 1500: Loss = 0.0000
2025-01-24 09:16:48,231 - Train Loss: 0.0000
2025-01-24 09:16:48,231 - Test Loss: 68222.5583
2025-01-24 09:16:48,231 - FID Score: 0.1229
2025-01-24 09:16:48,231 - Commitment Loss: 0.0000
2025-01-24 09:16:48,232 - Codebook Loss: 0.0000
2025-01-24 09:16:48,232 - Entropy Reg: 0.0000
2025-01-24 09:16:48,232 - L2 Reg: 0.0001
2025-01-24 09:16:48,255 - 
Epoch 33/50
2025-01-24 09:16:48,401 - Batch 0: Loss = 0.0000
2025-01-24 09:16:49,311 - Batch 100: Loss = 0.0000
2025-01-24 09:16:50,213 - Batch 200: Loss = 0.0000
2025-01-24 09:16:51,111 - Batch 300: Loss = 0.0000
2025-01-24 09:16:52,084 - Batch 400: Loss = 0.0000
2025-01-24 09:16:53,113 - Batch 500: Loss = 0.0000
2025-01-24 09:16:54,131 - Batch 600: Loss = 0.0000
2025-01-24 09:16:55,047 - Batch 700: Loss = 0.0000
2025-01-24 09:16:55,944 - Batch 800: Loss = 0.0000
2025-01-24 09:16:56,839 - Batch 900: Loss = 0.0000
2025-01-24 09:16:57,727 - Batch 1000: Loss = 0.0000
2025-01-24 09:16:58,608 - Batch 1100: Loss = 0.0000
2025-01-24 09:16:59,478 - Batch 1200: Loss = 0.0000
2025-01-24 09:17:00,158 - Batch 1300: Loss = 0.0000
2025-01-24 09:17:00,835 - Batch 1400: Loss = 0.0000
2025-01-24 09:17:01,511 - Batch 1500: Loss = 0.0000
2025-01-24 09:17:03,874 - Train Loss: 0.0000
2025-01-24 09:17:03,874 - Test Loss: 68222.5583
2025-01-24 09:17:03,874 - FID Score: 0.1229
2025-01-24 09:17:03,874 - Commitment Loss: 0.0000
2025-01-24 09:17:03,874 - Codebook Loss: 0.0000
2025-01-24 09:17:03,874 - Entropy Reg: 0.0000
2025-01-24 09:17:03,874 - L2 Reg: 0.0001
2025-01-24 09:17:03,897 - 
Epoch 34/50
2025-01-24 09:17:04,036 - Batch 0: Loss = 0.0000
2025-01-24 09:17:04,812 - Batch 100: Loss = 0.0000
2025-01-24 09:17:05,579 - Batch 200: Loss = 0.0000
2025-01-24 09:17:06,435 - Batch 300: Loss = 0.0000
2025-01-24 09:17:07,196 - Batch 400: Loss = 0.0000
2025-01-24 09:17:08,066 - Batch 500: Loss = 0.0000
2025-01-24 09:17:09,071 - Batch 600: Loss = 0.0000
2025-01-24 09:17:10,077 - Batch 700: Loss = 0.0000
2025-01-24 09:17:11,084 - Batch 800: Loss = 0.0000
2025-01-24 09:17:12,073 - Batch 900: Loss = 0.0000
2025-01-24 09:17:13,109 - Batch 1000: Loss = 0.0000
2025-01-24 09:17:14,164 - Batch 1100: Loss = 0.0000
2025-01-24 09:17:15,218 - Batch 1200: Loss = 0.0000
2025-01-24 09:17:16,257 - Batch 1300: Loss = 0.0000
2025-01-24 09:17:17,314 - Batch 1400: Loss = 0.0000
2025-01-24 09:17:18,369 - Batch 1500: Loss = 0.0000
2025-01-24 09:17:21,019 - Train Loss: 0.0000
2025-01-24 09:17:21,019 - Test Loss: 68222.5583
2025-01-24 09:17:21,020 - FID Score: 0.1229
2025-01-24 09:17:21,020 - Commitment Loss: 0.0000
2025-01-24 09:17:21,020 - Codebook Loss: 0.0000
2025-01-24 09:17:21,020 - Entropy Reg: 0.0000
2025-01-24 09:17:21,020 - L2 Reg: 0.0001
2025-01-24 09:17:21,043 - 
Epoch 35/50
2025-01-24 09:17:21,187 - Batch 0: Loss = 0.0000
2025-01-24 09:17:21,997 - Batch 100: Loss = 0.0000
2025-01-24 09:17:22,791 - Batch 200: Loss = 0.0000
2025-01-24 09:17:23,547 - Batch 300: Loss = 0.0000
2025-01-24 09:17:24,039 - Batch 400: Loss = 0.0000
2025-01-24 09:17:24,530 - Batch 500: Loss = 0.0000
2025-01-24 09:17:25,021 - Batch 600: Loss = 0.0000
2025-01-24 09:17:25,504 - Batch 700: Loss = 0.0000
2025-01-24 09:17:25,987 - Batch 800: Loss = 0.0000
2025-01-24 09:17:26,476 - Batch 900: Loss = 0.0000
2025-01-24 09:17:26,967 - Batch 1000: Loss = 0.0000
2025-01-24 09:17:27,435 - Batch 1100: Loss = 0.0000
2025-01-24 09:17:27,887 - Batch 1200: Loss = 0.0000
2025-01-24 09:17:28,342 - Batch 1300: Loss = 0.0000
2025-01-24 09:17:28,796 - Batch 1400: Loss = 0.0000
2025-01-24 09:17:29,326 - Batch 1500: Loss = 0.0000
2025-01-24 09:17:31,615 - Train Loss: 0.0000
2025-01-24 09:17:31,615 - Test Loss: 68222.5583
2025-01-24 09:17:31,616 - FID Score: 0.1229
2025-01-24 09:17:31,616 - Commitment Loss: 0.0000
2025-01-24 09:17:31,616 - Codebook Loss: 0.0000
2025-01-24 09:17:31,616 - Entropy Reg: 0.0000
2025-01-24 09:17:31,616 - L2 Reg: 0.0001
2025-01-24 09:17:31,639 - 
Epoch 36/50
2025-01-24 09:17:31,777 - Batch 0: Loss = 0.0000
2025-01-24 09:17:32,524 - Batch 100: Loss = 0.0000
2025-01-24 09:17:33,365 - Batch 200: Loss = 0.0000
2025-01-24 09:17:34,243 - Batch 300: Loss = 0.0000
2025-01-24 09:17:35,118 - Batch 400: Loss = 0.0000
2025-01-24 09:17:35,994 - Batch 500: Loss = 0.0000
2025-01-24 09:17:36,887 - Batch 600: Loss = 0.0000
2025-01-24 09:17:37,781 - Batch 700: Loss = 0.0000
2025-01-24 09:17:38,672 - Batch 800: Loss = 0.0000
2025-01-24 09:17:39,556 - Batch 900: Loss = 0.0000
2025-01-24 09:17:40,438 - Batch 1000: Loss = 0.0000
2025-01-24 09:17:41,123 - Batch 1100: Loss = 0.0000
2025-01-24 09:17:41,740 - Batch 1200: Loss = 0.0000
2025-01-24 09:17:42,296 - Batch 1300: Loss = 0.0000
2025-01-24 09:17:42,962 - Batch 1400: Loss = 0.0000
2025-01-24 09:17:43,628 - Batch 1500: Loss = 0.0000
2025-01-24 09:17:46,000 - Train Loss: 0.0000
2025-01-24 09:17:46,000 - Test Loss: 68222.5583
2025-01-24 09:17:46,000 - FID Score: 0.1229
2025-01-24 09:17:46,000 - Commitment Loss: 0.0000
2025-01-24 09:17:46,000 - Codebook Loss: 0.0000
2025-01-24 09:17:46,000 - Entropy Reg: 0.0000
2025-01-24 09:17:46,000 - L2 Reg: 0.0001
2025-01-24 09:17:46,023 - 
Epoch 37/50
2025-01-24 09:17:46,163 - Batch 0: Loss = 0.0000
2025-01-24 09:17:46,909 - Batch 100: Loss = 0.0000
2025-01-24 09:17:47,663 - Batch 200: Loss = 0.0000
2025-01-24 09:17:48,415 - Batch 300: Loss = 0.0000
2025-01-24 09:17:49,265 - Batch 400: Loss = 0.0000
2025-01-24 09:17:50,153 - Batch 500: Loss = 0.0000
2025-01-24 09:17:51,052 - Batch 600: Loss = 0.0000
2025-01-24 09:17:51,959 - Batch 700: Loss = 0.0000
2025-01-24 09:17:52,865 - Batch 800: Loss = 0.0000
2025-01-24 09:17:53,767 - Batch 900: Loss = 0.0000
2025-01-24 09:17:54,668 - Batch 1000: Loss = 0.0000
2025-01-24 09:17:55,569 - Batch 1100: Loss = 0.0000
2025-01-24 09:17:56,465 - Batch 1200: Loss = 0.0000
2025-01-24 09:17:57,349 - Batch 1300: Loss = 0.0000
2025-01-24 09:17:58,081 - Batch 1400: Loss = 0.0000
2025-01-24 09:17:58,908 - Batch 1500: Loss = 0.0000
2025-01-24 09:18:01,456 - Train Loss: 0.0000
2025-01-24 09:18:01,457 - Test Loss: 68222.5583
2025-01-24 09:18:01,457 - FID Score: 0.1229
2025-01-24 09:18:01,457 - Commitment Loss: 0.0000
2025-01-24 09:18:01,457 - Codebook Loss: 0.0000
2025-01-24 09:18:01,457 - Entropy Reg: 0.0000
2025-01-24 09:18:01,457 - L2 Reg: 0.0001
2025-01-24 09:18:01,480 - 
Epoch 38/50
2025-01-24 09:18:01,622 - Batch 0: Loss = 0.0000
2025-01-24 09:18:02,411 - Batch 100: Loss = 0.0000
2025-01-24 09:18:03,173 - Batch 200: Loss = 0.0000
2025-01-24 09:18:03,929 - Batch 300: Loss = 0.0000
2025-01-24 09:18:04,741 - Batch 400: Loss = 0.0000
2025-01-24 09:18:05,629 - Batch 500: Loss = 0.0000
2025-01-24 09:18:06,476 - Batch 600: Loss = 0.0000
2025-01-24 09:18:07,247 - Batch 700: Loss = 0.0000
2025-01-24 09:18:08,193 - Batch 800: Loss = 0.0000
2025-01-24 09:18:09,169 - Batch 900: Loss = 0.0000
2025-01-24 09:18:10,147 - Batch 1000: Loss = 0.0000
2025-01-24 09:18:11,137 - Batch 1100: Loss = 0.0000
2025-01-24 09:18:12,126 - Batch 1200: Loss = 0.0000
2025-01-24 09:18:13,115 - Batch 1300: Loss = 0.0000
2025-01-24 09:18:14,055 - Batch 1400: Loss = 0.0000
2025-01-24 09:18:15,063 - Batch 1500: Loss = 0.0000
2025-01-24 09:18:17,706 - Train Loss: 0.0000
2025-01-24 09:18:17,706 - Test Loss: 68222.5583
2025-01-24 09:18:17,706 - FID Score: 0.1229
2025-01-24 09:18:17,706 - Commitment Loss: 0.0000
2025-01-24 09:18:17,706 - Codebook Loss: 0.0000
2025-01-24 09:18:17,706 - Entropy Reg: 0.0000
2025-01-24 09:18:17,706 - L2 Reg: 0.0001
2025-01-24 09:18:17,729 - 
Epoch 39/50
2025-01-24 09:18:17,872 - Batch 0: Loss = 0.0000
2025-01-24 09:18:18,686 - Batch 100: Loss = 0.0000
2025-01-24 09:18:19,589 - Batch 200: Loss = 0.0000
2025-01-24 09:18:20,501 - Batch 300: Loss = 0.0000
2025-01-24 09:18:21,412 - Batch 400: Loss = 0.0000
2025-01-24 09:18:22,327 - Batch 500: Loss = 0.0000
2025-01-24 09:18:23,242 - Batch 600: Loss = 0.0000
2025-01-24 09:18:24,139 - Batch 700: Loss = 0.0000
2025-01-24 09:18:25,038 - Batch 800: Loss = 0.0000
2025-01-24 09:18:25,936 - Batch 900: Loss = 0.0000
2025-01-24 09:18:26,840 - Batch 1000: Loss = 0.0000
2025-01-24 09:18:27,753 - Batch 1100: Loss = 0.0000
2025-01-24 09:18:28,670 - Batch 1200: Loss = 0.0000
2025-01-24 09:18:29,584 - Batch 1300: Loss = 0.0000
2025-01-24 09:18:30,494 - Batch 1400: Loss = 0.0000
2025-01-24 09:18:31,398 - Batch 1500: Loss = 0.0000
2025-01-24 09:18:34,012 - Train Loss: 0.0000
2025-01-24 09:18:34,012 - Test Loss: 68222.5583
2025-01-24 09:18:34,012 - FID Score: 0.1229
2025-01-24 09:18:34,012 - Commitment Loss: 0.0000
2025-01-24 09:18:34,012 - Codebook Loss: 0.0000
2025-01-24 09:18:34,012 - Entropy Reg: 0.0000
2025-01-24 09:18:34,012 - L2 Reg: 0.0001
2025-01-24 09:18:34,035 - 
Epoch 40/50
2025-01-24 09:18:34,176 - Batch 0: Loss = 0.0000
2025-01-24 09:18:34,937 - Batch 100: Loss = 0.0000
2025-01-24 09:18:35,682 - Batch 200: Loss = 0.0000
2025-01-24 09:18:36,561 - Batch 300: Loss = 0.0000
2025-01-24 09:18:37,465 - Batch 400: Loss = 0.0000
2025-01-24 09:18:38,372 - Batch 500: Loss = 0.0000
2025-01-24 09:18:39,247 - Batch 600: Loss = 0.0000
2025-01-24 09:18:40,132 - Batch 700: Loss = 0.0000
2025-01-24 09:18:41,018 - Batch 800: Loss = 0.0000
2025-01-24 09:18:41,830 - Batch 900: Loss = 0.0000
2025-01-24 09:18:42,620 - Batch 1000: Loss = 0.0000
2025-01-24 09:18:43,540 - Batch 1100: Loss = 0.0000
2025-01-24 09:18:44,464 - Batch 1200: Loss = 0.0000
2025-01-24 09:18:45,385 - Batch 1300: Loss = 0.0000
2025-01-24 09:18:46,298 - Batch 1400: Loss = 0.0000
2025-01-24 09:18:47,195 - Batch 1500: Loss = 0.0000
2025-01-24 09:18:49,754 - Train Loss: 0.0000
2025-01-24 09:18:49,754 - Test Loss: 68222.5583
2025-01-24 09:18:49,754 - FID Score: 0.1229
2025-01-24 09:18:49,754 - Commitment Loss: 0.0000
2025-01-24 09:18:49,754 - Codebook Loss: 0.0000
2025-01-24 09:18:49,754 - Entropy Reg: 0.0000
2025-01-24 09:18:49,754 - L2 Reg: 0.0001
2025-01-24 09:18:49,777 - 
Epoch 41/50
2025-01-24 09:18:49,918 - Batch 0: Loss = 0.0000
2025-01-24 09:18:50,697 - Batch 100: Loss = 0.0000
2025-01-24 09:18:51,456 - Batch 200: Loss = 0.0000
2025-01-24 09:18:52,215 - Batch 300: Loss = 0.0000
2025-01-24 09:18:52,975 - Batch 400: Loss = 0.0000
2025-01-24 09:18:53,734 - Batch 500: Loss = 0.0000
2025-01-24 09:18:54,492 - Batch 600: Loss = 0.0000
2025-01-24 09:18:55,252 - Batch 700: Loss = 0.0000
2025-01-24 09:18:55,803 - Batch 800: Loss = 0.0000
2025-01-24 09:18:56,299 - Batch 900: Loss = 0.0000
2025-01-24 09:18:56,820 - Batch 1000: Loss = 0.0000
2025-01-24 09:18:57,402 - Batch 1100: Loss = 0.0000
2025-01-24 09:18:57,982 - Batch 1200: Loss = 0.0000
2025-01-24 09:18:58,564 - Batch 1300: Loss = 0.0000
2025-01-24 09:18:59,145 - Batch 1400: Loss = 0.0000
2025-01-24 09:18:59,729 - Batch 1500: Loss = 0.0000
2025-01-24 09:19:01,943 - Train Loss: 0.0000
2025-01-24 09:19:01,944 - Test Loss: 68222.5583
2025-01-24 09:19:01,944 - FID Score: 0.1229
2025-01-24 09:19:01,944 - Commitment Loss: 0.0000
2025-01-24 09:19:01,944 - Codebook Loss: 0.0000
2025-01-24 09:19:01,944 - Entropy Reg: 0.0000
2025-01-24 09:19:01,944 - L2 Reg: 0.0001
2025-01-24 09:19:01,949 - Saved checkpoint at epoch 40
2025-01-24 09:19:01,968 - 
Epoch 42/50
2025-01-24 09:19:02,062 - Batch 0: Loss = 0.0000
2025-01-24 09:19:02,777 - Batch 100: Loss = 0.0000
2025-01-24 09:19:03,482 - Batch 200: Loss = 0.0000
2025-01-24 09:19:04,188 - Batch 300: Loss = 0.0000
2025-01-24 09:19:04,873 - Batch 400: Loss = 0.0000
2025-01-24 09:19:05,481 - Batch 500: Loss = 0.0000
2025-01-24 09:19:06,125 - Batch 600: Loss = 0.0000
2025-01-24 09:19:06,769 - Batch 700: Loss = 0.0000
2025-01-24 09:19:07,412 - Batch 800: Loss = 0.0000
2025-01-24 09:19:08,055 - Batch 900: Loss = 0.0000
2025-01-24 09:19:08,709 - Batch 1000: Loss = 0.0000
2025-01-24 09:19:09,391 - Batch 1100: Loss = 0.0000
2025-01-24 09:19:10,077 - Batch 1200: Loss = 0.0000
2025-01-24 09:19:10,755 - Batch 1300: Loss = 0.0000
2025-01-24 09:19:11,438 - Batch 1400: Loss = 0.0000
2025-01-24 09:19:12,150 - Batch 1500: Loss = 0.0000
2025-01-24 09:19:14,465 - Train Loss: 0.0000
2025-01-24 09:19:14,465 - Test Loss: 68222.5583
2025-01-24 09:19:14,465 - FID Score: 0.1229
2025-01-24 09:19:14,465 - Commitment Loss: 0.0000
2025-01-24 09:19:14,465 - Codebook Loss: 0.0000
2025-01-24 09:19:14,465 - Entropy Reg: 0.0000
2025-01-24 09:19:14,465 - L2 Reg: 0.0001
2025-01-24 09:19:14,485 - 
Epoch 43/50
2025-01-24 09:19:14,584 - Batch 0: Loss = 0.0000
2025-01-24 09:19:15,274 - Batch 100: Loss = 0.0000
2025-01-24 09:19:15,943 - Batch 200: Loss = 0.0000
2025-01-24 09:19:16,612 - Batch 300: Loss = 0.0000
2025-01-24 09:19:17,306 - Batch 400: Loss = 0.0000
2025-01-24 09:19:18,030 - Batch 500: Loss = 0.0000
2025-01-24 09:19:18,758 - Batch 600: Loss = 0.0000
2025-01-24 09:19:19,419 - Batch 700: Loss = 0.0000
2025-01-24 09:19:20,012 - Batch 800: Loss = 0.0000
2025-01-24 09:19:20,532 - Batch 900: Loss = 0.0000
2025-01-24 09:19:21,052 - Batch 1000: Loss = 0.0000
2025-01-24 09:19:21,574 - Batch 1100: Loss = 0.0000
2025-01-24 09:19:22,092 - Batch 1200: Loss = 0.0000
2025-01-24 09:19:22,612 - Batch 1300: Loss = 0.0000
2025-01-24 09:19:23,159 - Batch 1400: Loss = 0.0000
2025-01-24 09:19:23,686 - Batch 1500: Loss = 0.0000
2025-01-24 09:19:25,922 - Train Loss: 0.0000
2025-01-24 09:19:25,922 - Test Loss: 68222.5583
2025-01-24 09:19:25,922 - FID Score: 0.1229
2025-01-24 09:19:25,922 - Commitment Loss: 0.0000
2025-01-24 09:19:25,922 - Codebook Loss: 0.0000
2025-01-24 09:19:25,922 - Entropy Reg: 0.0000
2025-01-24 09:19:25,922 - L2 Reg: 0.0001
2025-01-24 09:19:25,945 - 
Epoch 44/50
2025-01-24 09:19:26,083 - Batch 0: Loss = 0.0000
2025-01-24 09:19:26,645 - Batch 100: Loss = 0.0000
2025-01-24 09:19:27,173 - Batch 200: Loss = 0.0000
2025-01-24 09:19:27,656 - Batch 300: Loss = 0.0000
2025-01-24 09:19:28,074 - Batch 400: Loss = 0.0000
2025-01-24 09:19:28,495 - Batch 500: Loss = 0.0000
2025-01-24 09:19:28,915 - Batch 600: Loss = 0.0000
2025-01-24 09:19:29,337 - Batch 700: Loss = 0.0000
2025-01-24 09:19:29,774 - Batch 800: Loss = 0.0000
2025-01-24 09:19:30,369 - Batch 900: Loss = 0.0000
2025-01-24 09:19:30,988 - Batch 1000: Loss = 0.0000
2025-01-24 09:19:31,597 - Batch 1100: Loss = 0.0000
2025-01-24 09:19:32,206 - Batch 1200: Loss = 0.0000
2025-01-24 09:19:32,817 - Batch 1300: Loss = 0.0000
2025-01-24 09:19:33,427 - Batch 1400: Loss = 0.0000
2025-01-24 09:19:34,037 - Batch 1500: Loss = 0.0000
2025-01-24 09:19:36,302 - Train Loss: 0.0000
2025-01-24 09:19:36,302 - Test Loss: 68222.5583
2025-01-24 09:19:36,302 - FID Score: 0.1229
2025-01-24 09:19:36,302 - Commitment Loss: 0.0000
2025-01-24 09:19:36,302 - Codebook Loss: 0.0000
2025-01-24 09:19:36,302 - Entropy Reg: 0.0000
2025-01-24 09:19:36,302 - L2 Reg: 0.0001
2025-01-24 09:19:36,322 - 
Epoch 45/50
2025-01-24 09:19:36,417 - Batch 0: Loss = 0.0000
2025-01-24 09:19:36,926 - Batch 100: Loss = 0.0000
2025-01-24 09:19:37,497 - Batch 200: Loss = 0.0000
2025-01-24 09:19:38,066 - Batch 300: Loss = 0.0000
2025-01-24 09:19:38,668 - Batch 400: Loss = 0.0000
2025-01-24 09:19:39,293 - Batch 500: Loss = 0.0000
2025-01-24 09:19:39,918 - Batch 600: Loss = 0.0000
2025-01-24 09:19:40,566 - Batch 700: Loss = 0.0000
2025-01-24 09:19:41,235 - Batch 800: Loss = 0.0000
2025-01-24 09:19:41,903 - Batch 900: Loss = 0.0000
2025-01-24 09:19:42,572 - Batch 1000: Loss = 0.0000
2025-01-24 09:19:43,240 - Batch 1100: Loss = 0.0000
2025-01-24 09:19:43,910 - Batch 1200: Loss = 0.0000
2025-01-24 09:19:44,579 - Batch 1300: Loss = 0.0000
2025-01-24 09:19:45,246 - Batch 1400: Loss = 0.0000
2025-01-24 09:19:45,915 - Batch 1500: Loss = 0.0000
2025-01-24 09:19:48,223 - Train Loss: 0.0000
2025-01-24 09:19:48,223 - Test Loss: 68222.5583
2025-01-24 09:19:48,223 - FID Score: 0.1229
2025-01-24 09:19:48,223 - Commitment Loss: 0.0000
2025-01-24 09:19:48,223 - Codebook Loss: 0.0000
2025-01-24 09:19:48,223 - Entropy Reg: 0.0000
2025-01-24 09:19:48,223 - L2 Reg: 0.0001
2025-01-24 09:19:48,242 - 
Epoch 46/50
2025-01-24 09:19:48,336 - Batch 0: Loss = 0.0000
2025-01-24 09:19:48,940 - Batch 100: Loss = 0.0000
2025-01-24 09:19:49,516 - Batch 200: Loss = 0.0000
2025-01-24 09:19:50,125 - Batch 300: Loss = 0.0000
2025-01-24 09:19:50,717 - Batch 400: Loss = 0.0000
2025-01-24 09:19:51,336 - Batch 500: Loss = 0.0000
2025-01-24 09:19:51,983 - Batch 600: Loss = 0.0000
2025-01-24 09:19:52,636 - Batch 700: Loss = 0.0000
2025-01-24 09:19:53,288 - Batch 800: Loss = 0.0000
2025-01-24 09:19:53,941 - Batch 900: Loss = 0.0000
2025-01-24 09:19:54,593 - Batch 1000: Loss = 0.0000
2025-01-24 09:19:55,246 - Batch 1100: Loss = 0.0000
2025-01-24 09:19:55,895 - Batch 1200: Loss = 0.0000
2025-01-24 09:19:56,545 - Batch 1300: Loss = 0.0000
2025-01-24 09:19:57,193 - Batch 1400: Loss = 0.0000
2025-01-24 09:19:57,840 - Batch 1500: Loss = 0.0000
2025-01-24 09:20:00,114 - Train Loss: 0.0000
2025-01-24 09:20:00,114 - Test Loss: 68222.5583
2025-01-24 09:20:00,114 - FID Score: 0.1229
2025-01-24 09:20:00,114 - Commitment Loss: 0.0000
2025-01-24 09:20:00,114 - Codebook Loss: 0.0000
2025-01-24 09:20:00,114 - Entropy Reg: 0.0000
2025-01-24 09:20:00,114 - L2 Reg: 0.0001
2025-01-24 09:20:00,134 - 
Epoch 47/50
2025-01-24 09:20:00,226 - Batch 0: Loss = 0.0000
2025-01-24 09:20:00,723 - Batch 100: Loss = 0.0000
2025-01-24 09:20:01,178 - Batch 200: Loss = 0.0000
2025-01-24 09:20:01,638 - Batch 300: Loss = 0.0000
2025-01-24 09:20:02,099 - Batch 400: Loss = 0.0000
2025-01-24 09:20:02,582 - Batch 500: Loss = 0.0000
2025-01-24 09:20:03,032 - Batch 600: Loss = 0.0000
2025-01-24 09:20:03,484 - Batch 700: Loss = 0.0000
2025-01-24 09:20:03,936 - Batch 800: Loss = 0.0000
2025-01-24 09:20:04,387 - Batch 900: Loss = 0.0000
2025-01-24 09:20:04,837 - Batch 1000: Loss = 0.0000
2025-01-24 09:20:05,277 - Batch 1100: Loss = 0.0000
2025-01-24 09:20:05,721 - Batch 1200: Loss = 0.0000
2025-01-24 09:20:06,162 - Batch 1300: Loss = 0.0000
2025-01-24 09:20:06,599 - Batch 1400: Loss = 0.0000
2025-01-24 09:20:07,022 - Batch 1500: Loss = 0.0000
2025-01-24 09:20:09,154 - Train Loss: 0.0000
2025-01-24 09:20:09,155 - Test Loss: 68222.5583
2025-01-24 09:20:09,155 - FID Score: 0.1229
2025-01-24 09:20:09,155 - Commitment Loss: 0.0000
2025-01-24 09:20:09,155 - Codebook Loss: 0.0000
2025-01-24 09:20:09,155 - Entropy Reg: 0.0000
2025-01-24 09:20:09,155 - L2 Reg: 0.0001
2025-01-24 09:20:09,174 - 
Epoch 48/50
2025-01-24 09:20:09,266 - Batch 0: Loss = 0.0000
2025-01-24 09:20:09,990 - Batch 100: Loss = 0.0000
2025-01-24 09:20:10,852 - Batch 200: Loss = 0.0000
2025-01-24 09:20:11,726 - Batch 300: Loss = 0.0000
2025-01-24 09:20:12,606 - Batch 400: Loss = 0.0000
2025-01-24 09:20:13,488 - Batch 500: Loss = 0.0000
2025-01-24 09:20:14,371 - Batch 600: Loss = 0.0000
2025-01-24 09:20:15,255 - Batch 700: Loss = 0.0000
2025-01-24 09:20:16,034 - Batch 800: Loss = 0.0000
2025-01-24 09:20:16,914 - Batch 900: Loss = 0.0000
2025-01-24 09:20:17,811 - Batch 1000: Loss = 0.0000
2025-01-24 09:20:18,711 - Batch 1100: Loss = 0.0000
2025-01-24 09:20:19,610 - Batch 1200: Loss = 0.0000
2025-01-24 09:20:20,509 - Batch 1300: Loss = 0.0000
2025-01-24 09:20:21,407 - Batch 1400: Loss = 0.0000
2025-01-24 09:20:22,295 - Batch 1500: Loss = 0.0000
2025-01-24 09:20:24,695 - Train Loss: 0.0000
2025-01-24 09:20:24,695 - Test Loss: 68222.5583
2025-01-24 09:20:24,695 - FID Score: 0.1229
2025-01-24 09:20:24,695 - Commitment Loss: 0.0000
2025-01-24 09:20:24,695 - Codebook Loss: 0.0000
2025-01-24 09:20:24,696 - Entropy Reg: 0.0000
2025-01-24 09:20:24,696 - L2 Reg: 0.0001
2025-01-24 09:20:24,718 - 
Epoch 49/50
2025-01-24 09:20:24,861 - Batch 0: Loss = 0.0000
2025-01-24 09:20:25,757 - Batch 100: Loss = 0.0000
2025-01-24 09:20:26,639 - Batch 200: Loss = 0.0000
2025-01-24 09:20:27,521 - Batch 300: Loss = 0.0000
2025-01-24 09:20:28,403 - Batch 400: Loss = 0.0000
2025-01-24 09:20:29,284 - Batch 500: Loss = 0.0000
2025-01-24 09:20:30,167 - Batch 600: Loss = 0.0000
2025-01-24 09:20:31,049 - Batch 700: Loss = 0.0000
2025-01-24 09:20:31,841 - Batch 800: Loss = 0.0000
2025-01-24 09:20:32,507 - Batch 900: Loss = 0.0000
2025-01-24 09:20:33,173 - Batch 1000: Loss = 0.0000
2025-01-24 09:20:33,840 - Batch 1100: Loss = 0.0000
2025-01-24 09:20:34,508 - Batch 1200: Loss = 0.0000
2025-01-24 09:20:35,175 - Batch 1300: Loss = 0.0000
2025-01-24 09:20:35,841 - Batch 1400: Loss = 0.0000
2025-01-24 09:20:36,510 - Batch 1500: Loss = 0.0000
2025-01-24 09:20:38,898 - Train Loss: 0.0000
2025-01-24 09:20:38,898 - Test Loss: 68222.5583
2025-01-24 09:20:38,898 - FID Score: 0.1229
2025-01-24 09:20:38,898 - Commitment Loss: 0.0000
2025-01-24 09:20:38,898 - Codebook Loss: 0.0000
2025-01-24 09:20:38,898 - Entropy Reg: 0.0000
2025-01-24 09:20:38,898 - L2 Reg: 0.0001
2025-01-24 09:20:38,921 - 
Epoch 50/50
2025-01-24 09:20:39,068 - Batch 0: Loss = 0.0000
2025-01-24 09:20:39,933 - Batch 100: Loss = 0.0000
2025-01-24 09:20:40,801 - Batch 200: Loss = 0.0000
2025-01-24 09:20:41,669 - Batch 300: Loss = 0.0000
2025-01-24 09:20:42,536 - Batch 400: Loss = 0.0000
2025-01-24 09:20:43,404 - Batch 500: Loss = 0.0000
2025-01-24 09:20:44,211 - Batch 600: Loss = 0.0000
2025-01-24 09:20:44,690 - Batch 700: Loss = 0.0000
2025-01-24 09:20:45,123 - Batch 800: Loss = 0.0000
2025-01-24 09:20:45,559 - Batch 900: Loss = 0.0000
2025-01-24 09:20:46,125 - Batch 1000: Loss = 0.0000
2025-01-24 09:20:46,754 - Batch 1100: Loss = 0.0000
2025-01-24 09:20:47,400 - Batch 1200: Loss = 0.0000
2025-01-24 09:20:48,051 - Batch 1300: Loss = 0.0000
2025-01-24 09:20:48,706 - Batch 1400: Loss = 0.0000
2025-01-24 09:20:49,361 - Batch 1500: Loss = 0.0000
2025-01-24 09:20:51,648 - Train Loss: 0.0000
2025-01-24 09:20:51,648 - Test Loss: 68222.5583
2025-01-24 09:20:51,648 - FID Score: 0.1229
2025-01-24 09:20:51,648 - Commitment Loss: 0.0000
2025-01-24 09:20:51,648 - Codebook Loss: 0.0000
2025-01-24 09:20:51,648 - Entropy Reg: 0.0000
2025-01-24 09:20:51,648 - L2 Reg: 0.0001
2025-01-24 09:20:51,668 - Training history saved to regularized_training_history.json
2025-01-24 09:20:53,650 - 
Final Evaluation:
2025-01-24 09:20:53,650 - Test Loss: 68222.5583
2025-01-24 09:20:53,650 - FID Score: 0.1229
2025-01-24 09:22:46,181 - Using device: 0
2025-01-24 09:22:46,948 - 
Epoch 1/50
2025-01-24 09:22:48,476 - Batch 0: Loss = 0.0191
2025-01-24 09:22:49,416 - Batch 100: Loss = 0.0142
2025-01-24 09:22:50,388 - Batch 200: Loss = 0.0131
2025-01-24 09:22:51,344 - Batch 300: Loss = 0.0120
2025-01-24 09:22:52,286 - Batch 400: Loss = 0.0110
2025-01-24 09:22:53,247 - Batch 500: Loss = 0.0100
2025-01-24 09:22:54,203 - Batch 600: Loss = 0.0091
2025-01-24 09:22:55,165 - Batch 700: Loss = 0.0082
2025-01-24 09:22:56,139 - Batch 800: Loss = 0.0074
2025-01-24 09:22:57,154 - Batch 900: Loss = 0.0066
2025-01-24 09:22:58,215 - Batch 1000: Loss = 0.0059
2025-01-24 09:22:59,200 - Batch 1100: Loss = 0.0052
2025-01-24 09:22:59,962 - Batch 1200: Loss = 0.0046
2025-01-24 09:23:00,831 - Batch 1300: Loss = 0.0040
2025-01-24 09:23:01,845 - Batch 1400: Loss = 0.0034
2025-01-24 09:23:02,858 - Batch 1500: Loss = 0.0029
2025-01-24 09:23:05,689 - Train Loss: 0.0081
2025-01-24 09:23:05,689 - Test Loss: 68051.5039
2025-01-24 09:23:05,689 - FID Score: 0.0538
2025-01-24 09:23:05,689 - Commitment Loss: 0.0000
2025-01-24 09:23:05,689 - Codebook Loss: 0.0000
2025-01-24 09:23:05,689 - Entropy Reg: 0.0000
2025-01-24 09:23:05,689 - L2 Reg: 0.8040
2025-01-24 09:23:05,697 - Saved checkpoint at epoch 0
2025-01-24 09:23:05,719 - 
Epoch 2/50
2025-01-24 09:23:05,923 - Batch 0: Loss = 0.0026
2025-01-24 09:23:06,814 - Batch 100: Loss = 0.0022
2025-01-24 09:23:07,848 - Batch 200: Loss = 0.0018
2025-01-24 09:23:08,876 - Batch 300: Loss = 0.0014
2025-01-24 09:23:09,879 - Batch 400: Loss = 0.0011
2025-01-24 09:23:10,882 - Batch 500: Loss = 0.0008
2025-01-24 09:23:11,886 - Batch 600: Loss = 0.0006
2025-01-24 09:23:12,863 - Batch 700: Loss = 0.0003
2025-01-24 09:23:13,861 - Batch 800: Loss = 0.0002
2025-01-24 09:23:14,861 - Batch 900: Loss = 0.0000
2025-01-24 09:23:15,860 - Batch 1000: Loss = 0.0000
2025-01-24 09:23:16,861 - Batch 1100: Loss = 0.0000
2025-01-24 09:23:17,859 - Batch 1200: Loss = 0.0000
2025-01-24 09:23:18,859 - Batch 1300: Loss = 0.0000
2025-01-24 09:23:19,859 - Batch 1400: Loss = 0.0000
2025-01-24 09:23:20,835 - Batch 1500: Loss = 0.0000
2025-01-24 09:23:23,517 - Train Loss: 0.0006
2025-01-24 09:23:23,517 - Test Loss: 68051.4102
2025-01-24 09:23:23,517 - FID Score: 0.0538
2025-01-24 09:23:23,517 - Commitment Loss: 0.0000
2025-01-24 09:23:23,517 - Codebook Loss: 0.0000
2025-01-24 09:23:23,517 - Entropy Reg: 0.0000
2025-01-24 09:23:23,517 - L2 Reg: 0.0620
2025-01-24 09:23:23,540 - 
Epoch 3/50
2025-01-24 09:23:23,753 - Batch 0: Loss = 0.0000
2025-01-24 09:23:24,605 - Batch 100: Loss = 0.0000
2025-01-24 09:23:25,548 - Batch 200: Loss = 0.0000
2025-01-24 09:23:26,517 - Batch 300: Loss = 0.0000
2025-01-24 09:23:27,256 - Batch 400: Loss = 0.0000
2025-01-24 09:23:27,984 - Batch 500: Loss = 0.0000
2025-01-24 09:23:28,706 - Batch 600: Loss = 0.0000
2025-01-24 09:23:29,635 - Batch 700: Loss = 0.0000
2025-01-24 09:23:30,614 - Batch 800: Loss = 0.0000
2025-01-24 09:23:31,619 - Batch 900: Loss = 0.0000
2025-01-24 09:23:32,611 - Batch 1000: Loss = 0.0000
2025-01-24 09:23:33,549 - Batch 1100: Loss = 0.0000
2025-01-24 09:23:34,489 - Batch 1200: Loss = 0.0000
2025-01-24 09:23:35,313 - Batch 1300: Loss = 0.0000
2025-01-24 09:23:36,065 - Batch 1400: Loss = 0.0000
2025-01-24 09:23:36,814 - Batch 1500: Loss = 0.0000
2025-01-24 09:23:39,347 - Train Loss: 0.0000
2025-01-24 09:23:39,347 - Test Loss: 68051.3008
2025-01-24 09:23:39,347 - FID Score: 0.0538
2025-01-24 09:23:39,347 - Commitment Loss: 0.0000
2025-01-24 09:23:39,347 - Codebook Loss: 0.0000
2025-01-24 09:23:39,347 - Entropy Reg: 0.0000
2025-01-24 09:23:39,347 - L2 Reg: 0.0002
2025-01-24 09:23:39,370 - 
Epoch 4/50
2025-01-24 09:23:39,583 - Batch 0: Loss = 0.0000
2025-01-24 09:23:40,533 - Batch 100: Loss = 0.0000
2025-01-24 09:23:41,527 - Batch 200: Loss = 0.0000
2025-01-24 09:23:42,525 - Batch 300: Loss = 0.0000
2025-01-24 09:23:43,529 - Batch 400: Loss = 0.0000
2025-01-24 09:23:44,514 - Batch 500: Loss = 0.0000
2025-01-24 09:23:45,518 - Batch 600: Loss = 0.0000
2025-01-24 09:23:46,518 - Batch 700: Loss = 0.0000
2025-01-24 09:23:47,522 - Batch 800: Loss = 0.0000
2025-01-24 09:23:48,524 - Batch 900: Loss = 0.0000
2025-01-24 09:23:49,526 - Batch 1000: Loss = 0.0000
2025-01-24 09:23:50,536 - Batch 1100: Loss = 0.0000
2025-01-24 09:23:51,452 - Batch 1200: Loss = 0.0000
2025-01-24 09:23:52,182 - Batch 1300: Loss = 0.0000
2025-01-24 09:23:53,121 - Batch 1400: Loss = 0.0000
2025-01-24 09:23:54,123 - Batch 1500: Loss = 0.0000
2025-01-24 09:23:56,790 - Train Loss: 0.0000
2025-01-24 09:23:56,790 - Test Loss: 68051.2695
2025-01-24 09:23:56,790 - FID Score: 0.0538
2025-01-24 09:23:56,790 - Commitment Loss: 0.0000
2025-01-24 09:23:56,790 - Codebook Loss: 0.0000
2025-01-24 09:23:56,790 - Entropy Reg: 0.0000
2025-01-24 09:23:56,790 - L2 Reg: 0.0002
2025-01-24 09:23:56,813 - 
Epoch 5/50
2025-01-24 09:23:57,032 - Batch 0: Loss = 0.0000
2025-01-24 09:23:57,976 - Batch 100: Loss = 0.0000
2025-01-24 09:23:58,985 - Batch 200: Loss = 0.0000
2025-01-24 09:24:00,010 - Batch 300: Loss = 0.0000
2025-01-24 09:24:01,038 - Batch 400: Loss = 0.0000
2025-01-24 09:24:01,890 - Batch 500: Loss = 0.0000
2025-01-24 09:24:02,699 - Batch 600: Loss = 0.0000
2025-01-24 09:24:03,511 - Batch 700: Loss = 0.0000
2025-01-24 09:24:04,336 - Batch 800: Loss = 0.0000
2025-01-24 09:24:05,141 - Batch 900: Loss = 0.0000
2025-01-24 09:24:05,985 - Batch 1000: Loss = 0.0000
2025-01-24 09:24:06,790 - Batch 1100: Loss = 0.0000
2025-01-24 09:24:07,559 - Batch 1200: Loss = 0.0000
2025-01-24 09:24:08,406 - Batch 1300: Loss = 0.0000
2025-01-24 09:24:09,402 - Batch 1400: Loss = 0.0000
2025-01-24 09:24:10,440 - Batch 1500: Loss = 0.0000
2025-01-24 09:24:13,144 - Train Loss: 0.0000
2025-01-24 09:24:13,145 - Test Loss: 68051.2461
2025-01-24 09:24:13,145 - FID Score: 0.0538
2025-01-24 09:24:13,145 - Commitment Loss: 0.0000
2025-01-24 09:24:13,145 - Codebook Loss: 0.0000
2025-01-24 09:24:13,145 - Entropy Reg: 0.0000
2025-01-24 09:24:13,145 - L2 Reg: 0.0002
2025-01-24 09:24:13,168 - 
Epoch 6/50
2025-01-24 09:24:13,366 - Batch 0: Loss = 0.0000
2025-01-24 09:24:14,079 - Batch 100: Loss = 0.0000
2025-01-24 09:24:14,909 - Batch 200: Loss = 0.0000
2025-01-24 09:24:15,749 - Batch 300: Loss = 0.0000
2025-01-24 09:24:16,718 - Batch 400: Loss = 0.0000
2025-01-24 09:24:17,623 - Batch 500: Loss = 0.0000
2025-01-24 09:24:18,513 - Batch 600: Loss = 0.0000
2025-01-24 09:24:19,414 - Batch 700: Loss = 0.0000
2025-01-24 09:24:20,307 - Batch 800: Loss = 0.0000
2025-01-24 09:24:21,218 - Batch 900: Loss = 0.0000
2025-01-24 09:24:22,170 - Batch 1000: Loss = 0.0000
2025-01-24 09:24:23,158 - Batch 1100: Loss = 0.0000
2025-01-24 09:24:24,150 - Batch 1200: Loss = 0.0000
2025-01-24 09:24:25,141 - Batch 1300: Loss = 0.0000
2025-01-24 09:24:26,138 - Batch 1400: Loss = 0.0000
2025-01-24 09:24:27,133 - Batch 1500: Loss = 0.0000
2025-01-24 09:24:29,828 - Train Loss: 0.0000
2025-01-24 09:24:29,829 - Test Loss: 68051.2383
2025-01-24 09:24:29,829 - FID Score: 0.0538
2025-01-24 09:24:29,829 - Commitment Loss: 0.0000
2025-01-24 09:24:29,829 - Codebook Loss: 0.0000
2025-01-24 09:24:29,829 - Entropy Reg: 0.0000
2025-01-24 09:24:29,829 - L2 Reg: 0.0001
2025-01-24 09:24:29,852 - 
Epoch 7/50
2025-01-24 09:24:30,072 - Batch 0: Loss = 0.0000
2025-01-24 09:24:31,004 - Batch 100: Loss = 0.0000
2025-01-24 09:24:31,957 - Batch 200: Loss = 0.0000
2025-01-24 09:24:32,908 - Batch 300: Loss = 0.0000
2025-01-24 09:24:33,877 - Batch 400: Loss = 0.0000
2025-01-24 09:24:34,878 - Batch 500: Loss = 0.0000
2025-01-24 09:24:35,875 - Batch 600: Loss = 0.0000
2025-01-24 09:24:36,877 - Batch 700: Loss = 0.0000
2025-01-24 09:24:37,878 - Batch 800: Loss = 0.0000
2025-01-24 09:24:38,877 - Batch 900: Loss = 0.0000
2025-01-24 09:24:39,879 - Batch 1000: Loss = 0.0000
2025-01-24 09:24:40,794 - Batch 1100: Loss = 0.0000
2025-01-24 09:24:41,632 - Batch 1200: Loss = 0.0000
2025-01-24 09:24:42,625 - Batch 1300: Loss = 0.0000
2025-01-24 09:24:43,628 - Batch 1400: Loss = 0.0000
2025-01-24 09:24:44,653 - Batch 1500: Loss = 0.0000
2025-01-24 09:24:47,334 - Train Loss: 0.0000
2025-01-24 09:24:47,334 - Test Loss: 68051.2305
2025-01-24 09:24:47,334 - FID Score: 0.0538
2025-01-24 09:24:47,334 - Commitment Loss: 0.0000
2025-01-24 09:24:47,334 - Codebook Loss: 0.0000
2025-01-24 09:24:47,334 - Entropy Reg: 0.0000
2025-01-24 09:24:47,334 - L2 Reg: 0.0001
2025-01-24 09:24:47,357 - 
Epoch 8/50
2025-01-24 09:24:47,559 - Batch 0: Loss = 0.0000
2025-01-24 09:24:48,455 - Batch 100: Loss = 0.0000
2025-01-24 09:24:49,313 - Batch 200: Loss = 0.0000
2025-01-24 09:24:50,180 - Batch 300: Loss = 0.0000
2025-01-24 09:24:51,048 - Batch 400: Loss = 0.0000
2025-01-24 09:24:51,915 - Batch 500: Loss = 0.0000
2025-01-24 09:24:52,783 - Batch 600: Loss = 0.0000
2025-01-24 09:24:53,630 - Batch 700: Loss = 0.0000
2025-01-24 09:24:54,528 - Batch 800: Loss = 0.0000
2025-01-24 09:24:55,335 - Batch 900: Loss = 0.0000
2025-01-24 09:24:56,081 - Batch 1000: Loss = 0.0000
2025-01-24 09:24:56,826 - Batch 1100: Loss = 0.0000
2025-01-24 09:24:57,572 - Batch 1200: Loss = 0.0000
2025-01-24 09:24:58,320 - Batch 1300: Loss = 0.0000
2025-01-24 09:24:59,061 - Batch 1400: Loss = 0.0000
2025-01-24 09:24:59,809 - Batch 1500: Loss = 0.0000
2025-01-24 09:25:02,266 - Train Loss: 0.0000
2025-01-24 09:25:02,266 - Test Loss: 68051.2227
2025-01-24 09:25:02,266 - FID Score: 0.0538
2025-01-24 09:25:02,266 - Commitment Loss: 0.0000
2025-01-24 09:25:02,266 - Codebook Loss: 0.0000
2025-01-24 09:25:02,267 - Entropy Reg: 0.0000
2025-01-24 09:25:02,267 - L2 Reg: 0.0001
2025-01-24 09:25:02,290 - 
Epoch 9/50
2025-01-24 09:25:02,513 - Batch 0: Loss = 0.0000
2025-01-24 09:25:03,390 - Batch 100: Loss = 0.0000
2025-01-24 09:25:04,259 - Batch 200: Loss = 0.0000
2025-01-24 09:25:05,131 - Batch 300: Loss = 0.0000
2025-01-24 09:25:05,986 - Batch 400: Loss = 0.0000
2025-01-24 09:25:06,930 - Batch 500: Loss = 0.0000
2025-01-24 09:25:07,936 - Batch 600: Loss = 0.0000
2025-01-24 09:25:08,904 - Batch 700: Loss = 0.0000
2025-01-24 09:25:09,811 - Batch 800: Loss = 0.0000
2025-01-24 09:25:10,766 - Batch 900: Loss = 0.0000
2025-01-24 09:25:11,726 - Batch 1000: Loss = 0.0000
2025-01-24 09:25:12,689 - Batch 1100: Loss = 0.0000
2025-01-24 09:25:13,644 - Batch 1200: Loss = 0.0000
2025-01-24 09:25:14,387 - Batch 1300: Loss = 0.0000
2025-01-24 09:25:15,107 - Batch 1400: Loss = 0.0000
2025-01-24 09:25:15,809 - Batch 1500: Loss = 0.0000
2025-01-24 09:25:18,362 - Train Loss: 0.0000
2025-01-24 09:25:18,362 - Test Loss: 68051.2227
2025-01-24 09:25:18,362 - FID Score: 0.0538
2025-01-24 09:25:18,362 - Commitment Loss: 0.0000
2025-01-24 09:25:18,362 - Codebook Loss: 0.0000
2025-01-24 09:25:18,362 - Entropy Reg: 0.0000
2025-01-24 09:25:18,362 - L2 Reg: 0.0001
2025-01-24 09:25:18,385 - 
Epoch 10/50
2025-01-24 09:25:18,529 - Batch 0: Loss = 0.0000
2025-01-24 09:25:19,318 - Batch 100: Loss = 0.0000
2025-01-24 09:25:20,180 - Batch 200: Loss = 0.0000
2025-01-24 09:25:21,124 - Batch 300: Loss = 0.0000
2025-01-24 09:25:22,117 - Batch 400: Loss = 0.0000
2025-01-24 09:25:23,090 - Batch 500: Loss = 0.0000
2025-01-24 09:25:24,094 - Batch 600: Loss = 0.0000
2025-01-24 09:25:25,145 - Batch 700: Loss = 0.0000
2025-01-24 09:25:26,162 - Batch 800: Loss = 0.0000
2025-01-24 09:25:27,182 - Batch 900: Loss = 0.0000
2025-01-24 09:25:28,204 - Batch 1000: Loss = 0.0000
2025-01-24 09:25:29,233 - Batch 1100: Loss = 0.0000
2025-01-24 09:25:30,245 - Batch 1200: Loss = 0.0000
2025-01-24 09:25:31,248 - Batch 1300: Loss = 0.0000
2025-01-24 09:25:32,251 - Batch 1400: Loss = 0.0000
2025-01-24 09:25:33,254 - Batch 1500: Loss = 0.0000
2025-01-24 09:25:35,949 - Train Loss: 0.0000
2025-01-24 09:25:35,949 - Test Loss: 68051.2148
2025-01-24 09:25:35,949 - FID Score: 0.0538
2025-01-24 09:25:35,949 - Commitment Loss: 0.0000
2025-01-24 09:25:35,949 - Codebook Loss: 0.0000
2025-01-24 09:25:35,949 - Entropy Reg: 0.0000
2025-01-24 09:25:35,949 - L2 Reg: 0.0001
2025-01-24 09:25:35,972 - 
Epoch 11/50
2025-01-24 09:25:36,172 - Batch 0: Loss = 0.0000
2025-01-24 09:25:36,877 - Batch 100: Loss = 0.0000
2025-01-24 09:25:37,561 - Batch 200: Loss = 0.0000
2025-01-24 09:25:38,240 - Batch 300: Loss = 0.0000
2025-01-24 09:25:38,920 - Batch 400: Loss = 0.0000
2025-01-24 09:25:39,599 - Batch 500: Loss = 0.0000
2025-01-24 09:25:40,426 - Batch 600: Loss = 0.0000
2025-01-24 09:25:41,397 - Batch 700: Loss = 0.0000
2025-01-24 09:25:42,393 - Batch 800: Loss = 0.0000
2025-01-24 09:25:43,392 - Batch 900: Loss = 0.0000
2025-01-24 09:25:44,395 - Batch 1000: Loss = 0.0000
2025-01-24 09:25:45,391 - Batch 1100: Loss = 0.0000
2025-01-24 09:25:46,396 - Batch 1200: Loss = 0.0000
2025-01-24 09:25:47,384 - Batch 1300: Loss = 0.0000
2025-01-24 09:25:48,380 - Batch 1400: Loss = 0.0000
2025-01-24 09:25:49,350 - Batch 1500: Loss = 0.0000
2025-01-24 09:25:51,992 - Train Loss: 0.0000
2025-01-24 09:25:51,992 - Test Loss: 68051.2148
2025-01-24 09:25:51,992 - FID Score: 0.0538
2025-01-24 09:25:51,992 - Commitment Loss: 0.0000
2025-01-24 09:25:51,992 - Codebook Loss: 0.0000
2025-01-24 09:25:51,992 - Entropy Reg: 0.0000
2025-01-24 09:25:51,992 - L2 Reg: 0.0001
2025-01-24 09:25:52,000 - Saved checkpoint at epoch 10
2025-01-24 09:25:52,023 - 
Epoch 12/50
2025-01-24 09:25:52,243 - Batch 0: Loss = 0.0000
2025-01-24 09:25:53,124 - Batch 100: Loss = 0.0000
2025-01-24 09:25:54,014 - Batch 200: Loss = 0.0000
2025-01-24 09:25:54,882 - Batch 300: Loss = 0.0000
2025-01-24 09:25:55,748 - Batch 400: Loss = 0.0000
2025-01-24 09:25:56,618 - Batch 500: Loss = 0.0000
2025-01-24 09:25:57,459 - Batch 600: Loss = 0.0000
2025-01-24 09:25:58,293 - Batch 700: Loss = 0.0000
2025-01-24 09:25:59,128 - Batch 800: Loss = 0.0000
2025-01-24 09:25:59,994 - Batch 900: Loss = 0.0000
2025-01-24 09:26:00,858 - Batch 1000: Loss = 0.0000
2025-01-24 09:26:01,628 - Batch 1100: Loss = 0.0000
2025-01-24 09:26:02,286 - Batch 1200: Loss = 0.0000
2025-01-24 09:26:02,943 - Batch 1300: Loss = 0.0000
2025-01-24 09:26:03,601 - Batch 1400: Loss = 0.0000
2025-01-24 09:26:04,261 - Batch 1500: Loss = 0.0000
2025-01-24 09:26:06,624 - Train Loss: 0.0000
2025-01-24 09:26:06,625 - Test Loss: 68051.2148
2025-01-24 09:26:06,625 - FID Score: 0.0538
2025-01-24 09:26:06,625 - Commitment Loss: 0.0000
2025-01-24 09:26:06,625 - Codebook Loss: 0.0000
2025-01-24 09:26:06,625 - Entropy Reg: 0.0000
2025-01-24 09:26:06,625 - L2 Reg: 0.0001
2025-01-24 09:26:06,648 - 
Epoch 13/50
2025-01-24 09:26:06,855 - Batch 0: Loss = 0.0000
2025-01-24 09:26:07,648 - Batch 100: Loss = 0.0000
2025-01-24 09:26:08,536 - Batch 200: Loss = 0.0000
2025-01-24 09:26:09,422 - Batch 300: Loss = 0.0000
2025-01-24 09:26:10,275 - Batch 400: Loss = 0.0000
2025-01-24 09:26:11,137 - Batch 500: Loss = 0.0000
2025-01-24 09:26:12,003 - Batch 600: Loss = 0.0000
2025-01-24 09:26:12,874 - Batch 700: Loss = 0.0000
2025-01-24 09:26:13,861 - Batch 800: Loss = 0.0000
2025-01-24 09:26:14,866 - Batch 900: Loss = 0.0000
2025-01-24 09:26:15,883 - Batch 1000: Loss = 0.0000
2025-01-24 09:26:16,902 - Batch 1100: Loss = 0.0000
2025-01-24 09:26:17,924 - Batch 1200: Loss = 0.0000
2025-01-24 09:26:18,947 - Batch 1300: Loss = 0.0000
2025-01-24 09:26:19,967 - Batch 1400: Loss = 0.0000
2025-01-24 09:26:20,992 - Batch 1500: Loss = 0.0000
2025-01-24 09:26:23,718 - Train Loss: 0.0000
2025-01-24 09:26:23,718 - Test Loss: 68051.2148
2025-01-24 09:26:23,718 - FID Score: 0.0538
2025-01-24 09:26:23,718 - Commitment Loss: 0.0000
2025-01-24 09:26:23,718 - Codebook Loss: 0.0000
2025-01-24 09:26:23,719 - Entropy Reg: 0.0000
2025-01-24 09:26:23,719 - L2 Reg: 0.0001
2025-01-24 09:26:23,742 - 
Epoch 14/50
2025-01-24 09:26:23,953 - Batch 0: Loss = 0.0000
2025-01-24 09:26:24,750 - Batch 100: Loss = 0.0000
2025-01-24 09:26:25,740 - Batch 200: Loss = 0.0000
2025-01-24 09:26:26,772 - Batch 300: Loss = 0.0000
2025-01-24 09:26:27,737 - Batch 400: Loss = 0.0000
2025-01-24 09:26:28,707 - Batch 500: Loss = 0.0000
2025-01-24 09:26:29,682 - Batch 600: Loss = 0.0000
2025-01-24 09:26:30,654 - Batch 700: Loss = 0.0000
2025-01-24 09:26:31,643 - Batch 800: Loss = 0.0000
2025-01-24 09:26:32,655 - Batch 900: Loss = 0.0000
2025-01-24 09:26:33,670 - Batch 1000: Loss = 0.0000
2025-01-24 09:26:34,684 - Batch 1100: Loss = 0.0000
2025-01-24 09:26:35,700 - Batch 1200: Loss = 0.0000
2025-01-24 09:26:36,714 - Batch 1300: Loss = 0.0000
2025-01-24 09:26:37,720 - Batch 1400: Loss = 0.0000
2025-01-24 09:26:38,725 - Batch 1500: Loss = 0.0000
2025-01-24 09:26:41,416 - Train Loss: 0.0000
2025-01-24 09:26:41,416 - Test Loss: 68051.2148
2025-01-24 09:26:41,416 - FID Score: 0.0538
2025-01-24 09:26:41,416 - Commitment Loss: 0.0000
2025-01-24 09:26:41,416 - Codebook Loss: 0.0000
2025-01-24 09:26:41,416 - Entropy Reg: 0.0000
2025-01-24 09:26:41,416 - L2 Reg: 0.0001
2025-01-24 09:26:41,439 - 
Epoch 15/50
2025-01-24 09:26:41,649 - Batch 0: Loss = 0.0000
2025-01-24 09:26:42,505 - Batch 100: Loss = 0.0000
2025-01-24 09:26:43,370 - Batch 200: Loss = 0.0000
2025-01-24 09:26:44,244 - Batch 300: Loss = 0.0000
2025-01-24 09:26:45,113 - Batch 400: Loss = 0.0000
2025-01-24 09:26:45,993 - Batch 500: Loss = 0.0000
2025-01-24 09:26:46,873 - Batch 600: Loss = 0.0000
2025-01-24 09:26:47,755 - Batch 700: Loss = 0.0000
2025-01-24 09:26:48,641 - Batch 800: Loss = 0.0000
2025-01-24 09:26:49,518 - Batch 900: Loss = 0.0000
2025-01-24 09:26:50,406 - Batch 1000: Loss = 0.0000
2025-01-24 09:26:51,392 - Batch 1100: Loss = 0.0000
2025-01-24 09:26:52,397 - Batch 1200: Loss = 0.0000
2025-01-24 09:26:53,412 - Batch 1300: Loss = 0.0000
2025-01-24 09:26:54,432 - Batch 1400: Loss = 0.0000
2025-01-24 09:26:55,454 - Batch 1500: Loss = 0.0000
2025-01-24 09:26:58,152 - Train Loss: 0.0000
2025-01-24 09:26:58,153 - Test Loss: 68051.2148
2025-01-24 09:26:58,153 - FID Score: 0.0538
2025-01-24 09:26:58,153 - Commitment Loss: 0.0000
2025-01-24 09:26:58,153 - Codebook Loss: 0.0000
2025-01-24 09:26:58,153 - Entropy Reg: 0.0000
2025-01-24 09:26:58,153 - L2 Reg: 0.0001
2025-01-24 09:26:58,176 - 
Epoch 16/50
2025-01-24 09:26:58,377 - Batch 0: Loss = 0.0000
2025-01-24 09:26:59,282 - Batch 100: Loss = 0.0000
2025-01-24 09:27:00,262 - Batch 200: Loss = 0.0000
2025-01-24 09:27:01,268 - Batch 300: Loss = 0.0000
2025-01-24 09:27:02,276 - Batch 400: Loss = 0.0000
2025-01-24 09:27:03,294 - Batch 500: Loss = 0.0000
2025-01-24 09:27:04,309 - Batch 600: Loss = 0.0000
2025-01-24 09:27:05,319 - Batch 700: Loss = 0.0000
2025-01-24 09:27:06,333 - Batch 800: Loss = 0.0000
2025-01-24 09:27:07,348 - Batch 900: Loss = 0.0000
2025-01-24 09:27:08,363 - Batch 1000: Loss = 0.0000
2025-01-24 09:27:09,395 - Batch 1100: Loss = 0.0000
2025-01-24 09:27:10,458 - Batch 1200: Loss = 0.0000
2025-01-24 09:27:11,519 - Batch 1300: Loss = 0.0000
2025-01-24 09:27:12,588 - Batch 1400: Loss = 0.0000
2025-01-24 09:27:13,650 - Batch 1500: Loss = 0.0000
2025-01-24 09:27:16,387 - Train Loss: 0.0000
2025-01-24 09:27:16,387 - Test Loss: 68051.2070
2025-01-24 09:27:16,387 - FID Score: 0.0538
2025-01-24 09:27:16,387 - Commitment Loss: 0.0000
2025-01-24 09:27:16,387 - Codebook Loss: 0.0000
2025-01-24 09:27:16,387 - Entropy Reg: 0.0000
2025-01-24 09:27:16,387 - L2 Reg: 0.0001
2025-01-24 09:27:16,410 - 
Epoch 17/50
2025-01-24 09:27:16,610 - Batch 0: Loss = 0.0000
2025-01-24 09:27:17,380 - Batch 100: Loss = 0.0000
2025-01-24 09:27:18,333 - Batch 200: Loss = 0.0000
2025-01-24 09:27:19,237 - Batch 300: Loss = 0.0000
2025-01-24 09:27:20,109 - Batch 400: Loss = 0.0000
2025-01-24 09:27:20,978 - Batch 500: Loss = 0.0000
2025-01-24 09:27:21,856 - Batch 600: Loss = 0.0000
2025-01-24 09:27:22,754 - Batch 700: Loss = 0.0000
2025-01-24 09:27:23,744 - Batch 800: Loss = 0.0000
2025-01-24 09:27:24,753 - Batch 900: Loss = 0.0000
2025-01-24 09:27:25,761 - Batch 1000: Loss = 0.0000
2025-01-24 09:27:26,730 - Batch 1100: Loss = 0.0000
2025-01-24 09:27:27,651 - Batch 1200: Loss = 0.0000
2025-01-24 09:27:28,510 - Batch 1300: Loss = 0.0000
2025-01-24 09:27:29,360 - Batch 1400: Loss = 0.0000
2025-01-24 09:27:30,209 - Batch 1500: Loss = 0.0000
2025-01-24 09:27:32,816 - Train Loss: 0.0000
2025-01-24 09:27:32,816 - Test Loss: 68051.2070
2025-01-24 09:27:32,816 - FID Score: 0.0538
2025-01-24 09:27:32,816 - Commitment Loss: 0.0000
2025-01-24 09:27:32,816 - Codebook Loss: 0.0000
2025-01-24 09:27:32,816 - Entropy Reg: 0.0000
2025-01-24 09:27:32,816 - L2 Reg: 0.0001
2025-01-24 09:27:32,839 - 
Epoch 18/50
2025-01-24 09:27:33,039 - Batch 0: Loss = 0.0000
2025-01-24 09:27:33,956 - Batch 100: Loss = 0.0000
2025-01-24 09:27:34,964 - Batch 200: Loss = 0.0000
2025-01-24 09:27:36,004 - Batch 300: Loss = 0.0000
2025-01-24 09:27:37,046 - Batch 400: Loss = 0.0000
2025-01-24 09:27:38,086 - Batch 500: Loss = 0.0000
2025-01-24 09:27:39,138 - Batch 600: Loss = 0.0000
2025-01-24 09:27:40,164 - Batch 700: Loss = 0.0000
2025-01-24 09:27:41,192 - Batch 800: Loss = 0.0000
2025-01-24 09:27:42,207 - Batch 900: Loss = 0.0000
2025-01-24 09:27:43,220 - Batch 1000: Loss = 0.0000
2025-01-24 09:27:44,236 - Batch 1100: Loss = 0.0000
2025-01-24 09:27:45,244 - Batch 1200: Loss = 0.0000
2025-01-24 09:27:46,259 - Batch 1300: Loss = 0.0000
2025-01-24 09:27:47,180 - Batch 1400: Loss = 0.0000
2025-01-24 09:27:48,052 - Batch 1500: Loss = 0.0000
2025-01-24 09:27:50,657 - Train Loss: 0.0000
2025-01-24 09:27:50,657 - Test Loss: 68051.2070
2025-01-24 09:27:50,658 - FID Score: 0.0538
2025-01-24 09:27:50,658 - Commitment Loss: 0.0000
2025-01-24 09:27:50,658 - Codebook Loss: 0.0000
2025-01-24 09:27:50,658 - Entropy Reg: 0.0000
2025-01-24 09:27:50,658 - L2 Reg: 0.0001
2025-01-24 09:27:50,681 - 
Epoch 19/50
2025-01-24 09:27:50,887 - Batch 0: Loss = 0.0000
2025-01-24 09:27:51,777 - Batch 100: Loss = 0.0000
2025-01-24 09:27:52,674 - Batch 200: Loss = 0.0000
2025-01-24 09:27:53,573 - Batch 300: Loss = 0.0000
2025-01-24 09:27:54,485 - Batch 400: Loss = 0.0000
2025-01-24 09:27:55,403 - Batch 500: Loss = 0.0000
2025-01-24 09:27:56,313 - Batch 600: Loss = 0.0000
2025-01-24 09:27:57,224 - Batch 700: Loss = 0.0000
2025-01-24 09:27:58,212 - Batch 800: Loss = 0.0000
2025-01-24 09:27:59,217 - Batch 900: Loss = 0.0000
2025-01-24 09:28:00,217 - Batch 1000: Loss = 0.0000
2025-01-24 09:28:01,216 - Batch 1100: Loss = 0.0000
2025-01-24 09:28:02,190 - Batch 1200: Loss = 0.0000
2025-01-24 09:28:03,161 - Batch 1300: Loss = 0.0000
2025-01-24 09:28:04,159 - Batch 1400: Loss = 0.0000
2025-01-24 09:28:05,156 - Batch 1500: Loss = 0.0000
2025-01-24 09:28:07,832 - Train Loss: 0.0000
2025-01-24 09:28:07,832 - Test Loss: 68051.2070
2025-01-24 09:28:07,832 - FID Score: 0.0538
2025-01-24 09:28:07,832 - Commitment Loss: 0.0000
2025-01-24 09:28:07,832 - Codebook Loss: 0.0000
2025-01-24 09:28:07,832 - Entropy Reg: 0.0000
2025-01-24 09:28:07,832 - L2 Reg: 0.0001
2025-01-24 09:28:07,856 - 
Epoch 20/50
2025-01-24 09:28:08,053 - Batch 0: Loss = 0.0000
2025-01-24 09:28:08,935 - Batch 100: Loss = 0.0000
2025-01-24 09:28:09,861 - Batch 200: Loss = 0.0000
2025-01-24 09:28:10,794 - Batch 300: Loss = 0.0000
2025-01-24 09:28:11,734 - Batch 400: Loss = 0.0000
2025-01-24 09:28:12,683 - Batch 500: Loss = 0.0000
2025-01-24 09:28:13,631 - Batch 600: Loss = 0.0000
2025-01-24 09:28:14,602 - Batch 700: Loss = 0.0000
2025-01-24 09:28:15,577 - Batch 800: Loss = 0.0000
2025-01-24 09:28:16,542 - Batch 900: Loss = 0.0000
2025-01-24 09:28:17,508 - Batch 1000: Loss = 0.0000
2025-01-24 09:28:18,475 - Batch 1100: Loss = 0.0000
2025-01-24 09:28:19,432 - Batch 1200: Loss = 0.0000
2025-01-24 09:28:20,377 - Batch 1300: Loss = 0.0000
2025-01-24 09:28:21,350 - Batch 1400: Loss = 0.0000
2025-01-24 09:28:22,330 - Batch 1500: Loss = 0.0000
2025-01-24 09:28:25,022 - Train Loss: 0.0000
2025-01-24 09:28:25,022 - Test Loss: 68051.2070
2025-01-24 09:28:25,022 - FID Score: 0.0538
2025-01-24 09:28:25,022 - Commitment Loss: 0.0000
2025-01-24 09:28:25,022 - Codebook Loss: 0.0000
2025-01-24 09:28:25,022 - Entropy Reg: 0.0000
2025-01-24 09:28:25,022 - L2 Reg: 0.0001
2025-01-24 09:28:25,045 - 
Epoch 21/50
2025-01-24 09:28:25,250 - Batch 0: Loss = 0.0000
2025-01-24 09:28:26,129 - Batch 100: Loss = 0.0000
2025-01-24 09:28:27,021 - Batch 200: Loss = 0.0000
2025-01-24 09:28:27,978 - Batch 300: Loss = 0.0000
2025-01-24 09:28:28,939 - Batch 400: Loss = 0.0000
2025-01-24 09:28:29,902 - Batch 500: Loss = 0.0000
2025-01-24 09:28:30,852 - Batch 600: Loss = 0.0000
2025-01-24 09:28:31,811 - Batch 700: Loss = 0.0000
2025-01-24 09:28:32,769 - Batch 800: Loss = 0.0000
2025-01-24 09:28:33,728 - Batch 900: Loss = 0.0000
2025-01-24 09:28:34,686 - Batch 1000: Loss = 0.0000
2025-01-24 09:28:35,645 - Batch 1100: Loss = 0.0000
2025-01-24 09:28:36,640 - Batch 1200: Loss = 0.0000
2025-01-24 09:28:37,651 - Batch 1300: Loss = 0.0000
2025-01-24 09:28:38,633 - Batch 1400: Loss = 0.0000
2025-01-24 09:28:39,573 - Batch 1500: Loss = 0.0000
2025-01-24 09:28:42,032 - Train Loss: 0.0000
2025-01-24 09:28:42,033 - Test Loss: 68051.2070
2025-01-24 09:28:42,033 - FID Score: 0.0538
2025-01-24 09:28:42,033 - Commitment Loss: 0.0000
2025-01-24 09:28:42,033 - Codebook Loss: 0.0000
2025-01-24 09:28:42,033 - Entropy Reg: 0.0000
2025-01-24 09:28:42,033 - L2 Reg: 0.0001
2025-01-24 09:28:42,040 - Saved checkpoint at epoch 20
2025-01-24 09:28:42,063 - 
Epoch 22/50
2025-01-24 09:28:42,269 - Batch 0: Loss = 0.0000
2025-01-24 09:28:43,208 - Batch 100: Loss = 0.0000
2025-01-24 09:28:44,207 - Batch 200: Loss = 0.0000
2025-01-24 09:28:45,202 - Batch 300: Loss = 0.0000
2025-01-24 09:28:46,195 - Batch 400: Loss = 0.0000
2025-01-24 09:28:47,192 - Batch 500: Loss = 0.0000
2025-01-24 09:28:48,192 - Batch 600: Loss = 0.0000
2025-01-24 09:28:49,187 - Batch 700: Loss = 0.0000
2025-01-24 09:28:50,184 - Batch 800: Loss = 0.0000
2025-01-24 09:28:51,180 - Batch 900: Loss = 0.0000
2025-01-24 09:28:52,168 - Batch 1000: Loss = 0.0000
2025-01-24 09:28:53,164 - Batch 1100: Loss = 0.0000
2025-01-24 09:28:54,159 - Batch 1200: Loss = 0.0000
2025-01-24 09:28:55,145 - Batch 1300: Loss = 0.0000
2025-01-24 09:28:56,146 - Batch 1400: Loss = 0.0000
2025-01-24 09:28:57,147 - Batch 1500: Loss = 0.0000
2025-01-24 09:28:59,892 - Train Loss: 0.0000
2025-01-24 09:28:59,893 - Test Loss: 68051.2070
2025-01-24 09:28:59,893 - FID Score: 0.0538
2025-01-24 09:28:59,893 - Commitment Loss: 0.0000
2025-01-24 09:28:59,893 - Codebook Loss: 0.0000
2025-01-24 09:28:59,893 - Entropy Reg: 0.0000
2025-01-24 09:28:59,893 - L2 Reg: 0.0001
2025-01-24 09:28:59,916 - 
Epoch 23/50
2025-01-24 09:29:00,133 - Batch 0: Loss = 0.0000
2025-01-24 09:29:00,979 - Batch 100: Loss = 0.0000
2025-01-24 09:29:01,831 - Batch 200: Loss = 0.0000
2025-01-24 09:29:02,682 - Batch 300: Loss = 0.0000
2025-01-24 09:29:03,545 - Batch 400: Loss = 0.0000
2025-01-24 09:29:04,488 - Batch 500: Loss = 0.0000
2025-01-24 09:29:05,470 - Batch 600: Loss = 0.0000
2025-01-24 09:29:06,458 - Batch 700: Loss = 0.0000
2025-01-24 09:29:07,458 - Batch 800: Loss = 0.0000
2025-01-24 09:29:08,476 - Batch 900: Loss = 0.0000
2025-01-24 09:29:09,496 - Batch 1000: Loss = 0.0000
2025-01-24 09:29:10,512 - Batch 1100: Loss = 0.0000
2025-01-24 09:29:11,364 - Batch 1200: Loss = 0.0000
2025-01-24 09:29:12,306 - Batch 1300: Loss = 0.0000
2025-01-24 09:29:13,274 - Batch 1400: Loss = 0.0000
2025-01-24 09:29:14,243 - Batch 1500: Loss = 0.0000
2025-01-24 09:29:16,921 - Train Loss: 0.0000
2025-01-24 09:29:16,921 - Test Loss: 68051.2070
2025-01-24 09:29:16,921 - FID Score: 0.0538
2025-01-24 09:29:16,921 - Commitment Loss: 0.0000
2025-01-24 09:29:16,922 - Codebook Loss: 0.0000
2025-01-24 09:29:16,922 - Entropy Reg: 0.0000
2025-01-24 09:29:16,922 - L2 Reg: 0.0001
2025-01-24 09:29:16,945 - 
Epoch 24/50
2025-01-24 09:29:17,130 - Batch 0: Loss = 0.0000
2025-01-24 09:29:18,111 - Batch 100: Loss = 0.0000
2025-01-24 09:29:19,125 - Batch 200: Loss = 0.0000
2025-01-24 09:29:20,121 - Batch 300: Loss = 0.0000
2025-01-24 09:29:21,128 - Batch 400: Loss = 0.0000
2025-01-24 09:29:22,169 - Batch 500: Loss = 0.0000
2025-01-24 09:29:23,211 - Batch 600: Loss = 0.0000
2025-01-24 09:29:24,237 - Batch 700: Loss = 0.0000
2025-01-24 09:29:25,245 - Batch 800: Loss = 0.0000
2025-01-24 09:29:26,234 - Batch 900: Loss = 0.0000
2025-01-24 09:29:27,222 - Batch 1000: Loss = 0.0000
2025-01-24 09:29:28,209 - Batch 1100: Loss = 0.0000
2025-01-24 09:29:29,187 - Batch 1200: Loss = 0.0000
2025-01-24 09:29:30,219 - Batch 1300: Loss = 0.0000
2025-01-24 09:29:31,230 - Batch 1400: Loss = 0.0000
2025-01-24 09:29:32,253 - Batch 1500: Loss = 0.0000
2025-01-24 09:29:34,980 - Train Loss: 0.0000
2025-01-24 09:29:34,980 - Test Loss: 68051.2070
2025-01-24 09:29:34,980 - FID Score: 0.0538
2025-01-24 09:29:34,980 - Commitment Loss: 0.0000
2025-01-24 09:29:34,980 - Codebook Loss: 0.0000
2025-01-24 09:29:34,980 - Entropy Reg: 0.0000
2025-01-24 09:29:34,980 - L2 Reg: 0.0001
2025-01-24 09:29:35,004 - 
Epoch 25/50
2025-01-24 09:29:35,217 - Batch 0: Loss = 0.0000
2025-01-24 09:29:36,060 - Batch 100: Loss = 0.0000
2025-01-24 09:29:36,897 - Batch 200: Loss = 0.0000
2025-01-24 09:29:37,732 - Batch 300: Loss = 0.0000
2025-01-24 09:29:38,500 - Batch 400: Loss = 0.0000
2025-01-24 09:29:39,173 - Batch 500: Loss = 0.0000
2025-01-24 09:29:39,846 - Batch 600: Loss = 0.0000
2025-01-24 09:29:40,485 - Batch 700: Loss = 0.0000
2025-01-24 09:29:41,121 - Batch 800: Loss = 0.0000
2025-01-24 09:29:41,758 - Batch 900: Loss = 0.0000
2025-01-24 09:29:42,394 - Batch 1000: Loss = 0.0000
2025-01-24 09:29:43,000 - Batch 1100: Loss = 0.0000
2025-01-24 09:29:43,630 - Batch 1200: Loss = 0.0000
2025-01-24 09:29:44,224 - Batch 1300: Loss = 0.0000
2025-01-24 09:29:44,988 - Batch 1400: Loss = 0.0000
2025-01-24 09:29:45,767 - Batch 1500: Loss = 0.0000
2025-01-24 09:29:48,450 - Train Loss: 0.0000
2025-01-24 09:29:48,450 - Test Loss: 68051.2070
2025-01-24 09:29:48,450 - FID Score: 0.0538
2025-01-24 09:29:48,450 - Commitment Loss: 0.0000
2025-01-24 09:29:48,450 - Codebook Loss: 0.0000
2025-01-24 09:29:48,450 - Entropy Reg: 0.0000
2025-01-24 09:29:48,450 - L2 Reg: 0.0001
2025-01-24 09:29:48,473 - 
Epoch 26/50
2025-01-24 09:29:48,682 - Batch 0: Loss = 0.0000
2025-01-24 09:29:49,494 - Batch 100: Loss = 0.0000
2025-01-24 09:29:50,296 - Batch 200: Loss = 0.0000
2025-01-24 09:29:51,035 - Batch 300: Loss = 0.0000
2025-01-24 09:29:51,662 - Batch 400: Loss = 0.0000
2025-01-24 09:29:52,355 - Batch 500: Loss = 0.0000
2025-01-24 09:29:53,237 - Batch 600: Loss = 0.0000
2025-01-24 09:29:54,232 - Batch 700: Loss = 0.0000
2025-01-24 09:29:55,230 - Batch 800: Loss = 0.0000
2025-01-24 09:29:56,229 - Batch 900: Loss = 0.0000
2025-01-24 09:29:57,230 - Batch 1000: Loss = 0.0000
2025-01-24 09:29:58,228 - Batch 1100: Loss = 0.0000
2025-01-24 09:29:59,222 - Batch 1200: Loss = 0.0000
2025-01-24 09:30:00,222 - Batch 1300: Loss = 0.0000
2025-01-24 09:30:01,222 - Batch 1400: Loss = 0.0000
2025-01-24 09:30:02,222 - Batch 1500: Loss = 0.0000
2025-01-24 09:30:04,945 - Train Loss: 0.0000
2025-01-24 09:30:04,945 - Test Loss: 68051.2070
2025-01-24 09:30:04,945 - FID Score: 0.0538
2025-01-24 09:30:04,945 - Commitment Loss: 0.0000
2025-01-24 09:30:04,945 - Codebook Loss: 0.0000
2025-01-24 09:30:04,945 - Entropy Reg: 0.0000
2025-01-24 09:30:04,946 - L2 Reg: 0.0001
2025-01-24 09:30:04,969 - 
Epoch 27/50
2025-01-24 09:30:05,189 - Batch 0: Loss = 0.0000
2025-01-24 09:30:06,043 - Batch 100: Loss = 0.0000
2025-01-24 09:30:06,925 - Batch 200: Loss = 0.0000
2025-01-24 09:30:07,916 - Batch 300: Loss = 0.0000
2025-01-24 09:30:08,894 - Batch 400: Loss = 0.0000
2025-01-24 09:30:09,879 - Batch 500: Loss = 0.0000
2025-01-24 09:30:10,870 - Batch 600: Loss = 0.0000
2025-01-24 09:30:11,861 - Batch 700: Loss = 0.0000
2025-01-24 09:30:12,851 - Batch 800: Loss = 0.0000
2025-01-24 09:30:13,843 - Batch 900: Loss = 0.0000
2025-01-24 09:30:14,835 - Batch 1000: Loss = 0.0000
2025-01-24 09:30:15,833 - Batch 1100: Loss = 0.0000
2025-01-24 09:30:16,879 - Batch 1200: Loss = 0.0000
2025-01-24 09:30:17,639 - Batch 1300: Loss = 0.0000
2025-01-24 09:30:18,376 - Batch 1400: Loss = 0.0000
2025-01-24 09:30:19,257 - Batch 1500: Loss = 0.0000
2025-01-24 09:30:21,961 - Train Loss: 0.0000
2025-01-24 09:30:21,962 - Test Loss: 68051.2070
2025-01-24 09:30:21,962 - FID Score: 0.0538
2025-01-24 09:30:21,962 - Commitment Loss: 0.0000
2025-01-24 09:30:21,962 - Codebook Loss: 0.0000
2025-01-24 09:30:21,962 - Entropy Reg: 0.0000
2025-01-24 09:30:21,962 - L2 Reg: 0.0001
2025-01-24 09:30:21,985 - 
Epoch 28/50
2025-01-24 09:30:22,201 - Batch 0: Loss = 0.0000
2025-01-24 09:30:23,050 - Batch 100: Loss = 0.0000
2025-01-24 09:30:23,906 - Batch 200: Loss = 0.0000
2025-01-24 09:30:24,767 - Batch 300: Loss = 0.0000
2025-01-24 09:30:25,627 - Batch 400: Loss = 0.0000
2025-01-24 09:30:26,486 - Batch 500: Loss = 0.0000
2025-01-24 09:30:27,347 - Batch 600: Loss = 0.0000
2025-01-24 09:30:28,206 - Batch 700: Loss = 0.0000
2025-01-24 09:30:29,065 - Batch 800: Loss = 0.0000
2025-01-24 09:30:29,865 - Batch 900: Loss = 0.0000
2025-01-24 09:30:30,538 - Batch 1000: Loss = 0.0000
2025-01-24 09:30:31,210 - Batch 1100: Loss = 0.0000
2025-01-24 09:30:31,884 - Batch 1200: Loss = 0.0000
2025-01-24 09:30:32,564 - Batch 1300: Loss = 0.0000
2025-01-24 09:30:33,238 - Batch 1400: Loss = 0.0000
2025-01-24 09:30:33,923 - Batch 1500: Loss = 0.0000
2025-01-24 09:30:36,541 - Train Loss: 0.0000
2025-01-24 09:30:36,542 - Test Loss: 68051.2070
2025-01-24 09:30:36,542 - FID Score: 0.0538
2025-01-24 09:30:36,542 - Commitment Loss: 0.0000
2025-01-24 09:30:36,542 - Codebook Loss: 0.0000
2025-01-24 09:30:36,542 - Entropy Reg: 0.0000
2025-01-24 09:30:36,542 - L2 Reg: 0.0001
2025-01-24 09:30:36,565 - 
Epoch 29/50
2025-01-24 09:30:36,782 - Batch 0: Loss = 0.0000
2025-01-24 09:30:37,666 - Batch 100: Loss = 0.0000
2025-01-24 09:30:38,665 - Batch 200: Loss = 0.0000
2025-01-24 09:30:39,670 - Batch 300: Loss = 0.0000
2025-01-24 09:30:40,672 - Batch 400: Loss = 0.0000
2025-01-24 09:30:41,701 - Batch 500: Loss = 0.0000
2025-01-24 09:30:42,725 - Batch 600: Loss = 0.0000
2025-01-24 09:30:43,751 - Batch 700: Loss = 0.0000
2025-01-24 09:30:44,756 - Batch 800: Loss = 0.0000
2025-01-24 09:30:45,739 - Batch 900: Loss = 0.0000
2025-01-24 09:30:46,726 - Batch 1000: Loss = 0.0000
2025-01-24 09:30:47,714 - Batch 1100: Loss = 0.0000
2025-01-24 09:30:48,697 - Batch 1200: Loss = 0.0000
2025-01-24 09:30:49,454 - Batch 1300: Loss = 0.0000
2025-01-24 09:30:50,251 - Batch 1400: Loss = 0.0000
2025-01-24 09:30:51,026 - Batch 1500: Loss = 0.0000
2025-01-24 09:30:53,480 - Train Loss: 0.0000
2025-01-24 09:30:53,480 - Test Loss: 68051.2070
2025-01-24 09:30:53,480 - FID Score: 0.0538
2025-01-24 09:30:53,480 - Commitment Loss: 0.0000
2025-01-24 09:30:53,480 - Codebook Loss: 0.0000
2025-01-24 09:30:53,481 - Entropy Reg: 0.0000
2025-01-24 09:30:53,481 - L2 Reg: 0.0001
2025-01-24 09:30:53,504 - 
Epoch 30/50
2025-01-24 09:30:53,728 - Batch 0: Loss = 0.0000
2025-01-24 09:30:54,672 - Batch 100: Loss = 0.0000
2025-01-24 09:30:55,671 - Batch 200: Loss = 0.0000
2025-01-24 09:30:56,610 - Batch 300: Loss = 0.0000
2025-01-24 09:30:57,480 - Batch 400: Loss = 0.0000
2025-01-24 09:30:58,349 - Batch 500: Loss = 0.0000
2025-01-24 09:30:59,218 - Batch 600: Loss = 0.0000
2025-01-24 09:31:00,087 - Batch 700: Loss = 0.0000
2025-01-24 09:31:00,967 - Batch 800: Loss = 0.0000
2025-01-24 09:31:01,841 - Batch 900: Loss = 0.0000
2025-01-24 09:31:02,711 - Batch 1000: Loss = 0.0000
2025-01-24 09:31:03,581 - Batch 1100: Loss = 0.0000
2025-01-24 09:31:04,451 - Batch 1200: Loss = 0.0000
2025-01-24 09:31:05,308 - Batch 1300: Loss = 0.0000
2025-01-24 09:31:06,165 - Batch 1400: Loss = 0.0000
2025-01-24 09:31:07,028 - Batch 1500: Loss = 0.0000
2025-01-24 09:31:09,657 - Train Loss: 0.0000
2025-01-24 09:31:09,658 - Test Loss: 68051.2070
2025-01-24 09:31:09,658 - FID Score: 0.0538
2025-01-24 09:31:09,658 - Commitment Loss: 0.0000
2025-01-24 09:31:09,658 - Codebook Loss: 0.0000
2025-01-24 09:31:09,658 - Entropy Reg: 0.0000
2025-01-24 09:31:09,658 - L2 Reg: 0.0001
2025-01-24 09:31:09,681 - 
Epoch 31/50
2025-01-24 09:31:09,900 - Batch 0: Loss = 0.0000
2025-01-24 09:31:10,638 - Batch 100: Loss = 0.0000
2025-01-24 09:31:11,542 - Batch 200: Loss = 0.0000
2025-01-24 09:31:12,549 - Batch 300: Loss = 0.0000
2025-01-24 09:31:13,574 - Batch 400: Loss = 0.0000
2025-01-24 09:31:14,610 - Batch 500: Loss = 0.0000
2025-01-24 09:31:15,644 - Batch 600: Loss = 0.0000
2025-01-24 09:31:16,676 - Batch 700: Loss = 0.0000
2025-01-24 09:31:17,637 - Batch 800: Loss = 0.0000
2025-01-24 09:31:18,591 - Batch 900: Loss = 0.0000
2025-01-24 09:31:19,558 - Batch 1000: Loss = 0.0000
2025-01-24 09:31:20,509 - Batch 1100: Loss = 0.0000
2025-01-24 09:31:21,385 - Batch 1200: Loss = 0.0000
2025-01-24 09:31:22,313 - Batch 1300: Loss = 0.0000
2025-01-24 09:31:23,025 - Batch 1400: Loss = 0.0000
2025-01-24 09:31:23,714 - Batch 1500: Loss = 0.0000
2025-01-24 09:31:26,245 - Train Loss: 0.0000
2025-01-24 09:31:26,246 - Test Loss: 68051.2070
2025-01-24 09:31:26,246 - FID Score: 0.0538
2025-01-24 09:31:26,246 - Commitment Loss: 0.0000
2025-01-24 09:31:26,246 - Codebook Loss: 0.0000
2025-01-24 09:31:26,246 - Entropy Reg: 0.0000
2025-01-24 09:31:26,246 - L2 Reg: 0.0001
2025-01-24 09:31:26,253 - Saved checkpoint at epoch 30
2025-01-24 09:31:26,277 - 
Epoch 32/50
2025-01-24 09:31:26,491 - Batch 0: Loss = 0.0000
2025-01-24 09:31:27,074 - Batch 100: Loss = 0.0000
2025-01-24 09:31:27,640 - Batch 200: Loss = 0.0000
2025-01-24 09:31:28,241 - Batch 300: Loss = 0.0000
2025-01-24 09:31:28,853 - Batch 400: Loss = 0.0000
2025-01-24 09:31:29,465 - Batch 500: Loss = 0.0000
2025-01-24 09:31:30,078 - Batch 600: Loss = 0.0000
2025-01-24 09:31:30,695 - Batch 700: Loss = 0.0000
2025-01-24 09:31:31,332 - Batch 800: Loss = 0.0000
2025-01-24 09:31:31,979 - Batch 900: Loss = 0.0000
2025-01-24 09:31:32,624 - Batch 1000: Loss = 0.0000
2025-01-24 09:31:33,269 - Batch 1100: Loss = 0.0000
2025-01-24 09:31:33,915 - Batch 1200: Loss = 0.0000
2025-01-24 09:31:34,560 - Batch 1300: Loss = 0.0000
2025-01-24 09:31:35,206 - Batch 1400: Loss = 0.0000
2025-01-24 09:31:35,851 - Batch 1500: Loss = 0.0000
2025-01-24 09:31:38,186 - Train Loss: 0.0000
2025-01-24 09:31:38,186 - Test Loss: 68051.2070
2025-01-24 09:31:38,186 - FID Score: 0.0538
2025-01-24 09:31:38,186 - Commitment Loss: 0.0000
2025-01-24 09:31:38,186 - Codebook Loss: 0.0000
2025-01-24 09:31:38,186 - Entropy Reg: 0.0000
2025-01-24 09:31:38,186 - L2 Reg: 0.0001
2025-01-24 09:31:38,206 - 
Epoch 33/50
2025-01-24 09:31:38,339 - Batch 0: Loss = 0.0000
2025-01-24 09:31:38,953 - Batch 100: Loss = 0.0000
2025-01-24 09:31:39,673 - Batch 200: Loss = 0.0000
2025-01-24 09:31:40,417 - Batch 300: Loss = 0.0000
2025-01-24 09:31:41,191 - Batch 400: Loss = 0.0000
2025-01-24 09:31:42,047 - Batch 500: Loss = 0.0000
2025-01-24 09:31:42,676 - Batch 600: Loss = 0.0000
2025-01-24 09:31:43,284 - Batch 700: Loss = 0.0000
2025-01-24 09:31:43,906 - Batch 800: Loss = 0.0000
2025-01-24 09:31:44,535 - Batch 900: Loss = 0.0000
2025-01-24 09:31:45,166 - Batch 1000: Loss = 0.0000
2025-01-24 09:31:45,809 - Batch 1100: Loss = 0.0000
2025-01-24 09:31:46,455 - Batch 1200: Loss = 0.0000
2025-01-24 09:31:47,100 - Batch 1300: Loss = 0.0000
2025-01-24 09:31:47,761 - Batch 1400: Loss = 0.0000
2025-01-24 09:31:48,573 - Batch 1500: Loss = 0.0000
2025-01-24 09:31:51,186 - Train Loss: 0.0000
2025-01-24 09:31:51,186 - Test Loss: 68051.2070
2025-01-24 09:31:51,186 - FID Score: 0.0538
2025-01-24 09:31:51,186 - Commitment Loss: 0.0000
2025-01-24 09:31:51,186 - Codebook Loss: 0.0000
2025-01-24 09:31:51,186 - Entropy Reg: 0.0000
2025-01-24 09:31:51,186 - L2 Reg: 0.0001
2025-01-24 09:31:51,209 - 
Epoch 34/50
2025-01-24 09:31:51,415 - Batch 0: Loss = 0.0000
2025-01-24 09:31:52,369 - Batch 100: Loss = 0.0000
2025-01-24 09:31:53,289 - Batch 200: Loss = 0.0000
2025-01-24 09:31:54,168 - Batch 300: Loss = 0.0000
2025-01-24 09:31:55,075 - Batch 400: Loss = 0.0000
2025-01-24 09:31:56,058 - Batch 500: Loss = 0.0000
2025-01-24 09:31:57,058 - Batch 600: Loss = 0.0000
2025-01-24 09:31:58,067 - Batch 700: Loss = 0.0000
2025-01-24 09:31:59,071 - Batch 800: Loss = 0.0000
2025-01-24 09:32:00,082 - Batch 900: Loss = 0.0000
2025-01-24 09:32:01,093 - Batch 1000: Loss = 0.0000
2025-01-24 09:32:02,101 - Batch 1100: Loss = 0.0000
2025-01-24 09:32:03,108 - Batch 1200: Loss = 0.0000
2025-01-24 09:32:04,114 - Batch 1300: Loss = 0.0000
2025-01-24 09:32:05,122 - Batch 1400: Loss = 0.0000
2025-01-24 09:32:06,124 - Batch 1500: Loss = 0.0000
2025-01-24 09:32:08,821 - Train Loss: 0.0000
2025-01-24 09:32:08,822 - Test Loss: 68051.2070
2025-01-24 09:32:08,822 - FID Score: 0.0538
2025-01-24 09:32:08,822 - Commitment Loss: 0.0000
2025-01-24 09:32:08,822 - Codebook Loss: 0.0000
2025-01-24 09:32:08,822 - Entropy Reg: 0.0000
2025-01-24 09:32:08,822 - L2 Reg: 0.0001
2025-01-24 09:32:08,845 - 
Epoch 35/50
2025-01-24 09:32:09,047 - Batch 0: Loss = 0.0000
2025-01-24 09:32:09,908 - Batch 100: Loss = 0.0000
2025-01-24 09:32:10,776 - Batch 200: Loss = 0.0000
2025-01-24 09:32:11,655 - Batch 300: Loss = 0.0000
2025-01-24 09:32:12,533 - Batch 400: Loss = 0.0000
2025-01-24 09:32:13,368 - Batch 500: Loss = 0.0000
2025-01-24 09:32:14,200 - Batch 600: Loss = 0.0000
2025-01-24 09:32:15,033 - Batch 700: Loss = 0.0000
2025-01-24 09:32:15,868 - Batch 800: Loss = 0.0000
2025-01-24 09:32:16,710 - Batch 900: Loss = 0.0000
2025-01-24 09:32:17,561 - Batch 1000: Loss = 0.0000
2025-01-24 09:32:18,414 - Batch 1100: Loss = 0.0000
2025-01-24 09:32:19,276 - Batch 1200: Loss = 0.0000
2025-01-24 09:32:20,129 - Batch 1300: Loss = 0.0000
2025-01-24 09:32:20,830 - Batch 1400: Loss = 0.0000
2025-01-24 09:32:21,500 - Batch 1500: Loss = 0.0000
2025-01-24 09:32:23,878 - Train Loss: 0.0000
2025-01-24 09:32:23,878 - Test Loss: 68051.2070
2025-01-24 09:32:23,878 - FID Score: 0.0538
2025-01-24 09:32:23,878 - Commitment Loss: 0.0000
2025-01-24 09:32:23,878 - Codebook Loss: 0.0000
2025-01-24 09:32:23,878 - Entropy Reg: 0.0000
2025-01-24 09:32:23,878 - L2 Reg: 0.0001
2025-01-24 09:32:23,901 - 
Epoch 36/50
2025-01-24 09:32:24,099 - Batch 0: Loss = 0.0000
2025-01-24 09:32:24,895 - Batch 100: Loss = 0.0000
2025-01-24 09:32:25,617 - Batch 200: Loss = 0.0000
2025-01-24 09:32:26,430 - Batch 300: Loss = 0.0000
2025-01-24 09:32:27,453 - Batch 400: Loss = 0.0000
2025-01-24 09:32:28,517 - Batch 500: Loss = 0.0000
2025-01-24 09:32:29,586 - Batch 600: Loss = 0.0000
2025-01-24 09:32:30,705 - Batch 700: Loss = 0.0000
2025-01-24 09:32:31,810 - Batch 800: Loss = 0.0000
2025-01-24 09:32:32,891 - Batch 900: Loss = 0.0000
2025-01-24 09:32:33,976 - Batch 1000: Loss = 0.0000
2025-01-24 09:32:35,078 - Batch 1100: Loss = 0.0000
2025-01-24 09:32:36,183 - Batch 1200: Loss = 0.0000
2025-01-24 09:32:37,283 - Batch 1300: Loss = 0.0000
2025-01-24 09:32:38,380 - Batch 1400: Loss = 0.0000
2025-01-24 09:32:39,354 - Batch 1500: Loss = 0.0000
2025-01-24 09:32:42,002 - Train Loss: 0.0000
2025-01-24 09:32:42,003 - Test Loss: 68051.2070
2025-01-24 09:32:42,003 - FID Score: 0.0538
2025-01-24 09:32:42,003 - Commitment Loss: 0.0000
2025-01-24 09:32:42,003 - Codebook Loss: 0.0000
2025-01-24 09:32:42,003 - Entropy Reg: 0.0000
2025-01-24 09:32:42,003 - L2 Reg: 0.0001
2025-01-24 09:32:42,026 - 
Epoch 37/50
2025-01-24 09:32:42,200 - Batch 0: Loss = 0.0000
2025-01-24 09:32:43,263 - Batch 100: Loss = 0.0000
2025-01-24 09:32:44,344 - Batch 200: Loss = 0.0000
2025-01-24 09:32:45,377 - Batch 300: Loss = 0.0000
2025-01-24 09:32:46,448 - Batch 400: Loss = 0.0000
2025-01-24 09:32:47,527 - Batch 500: Loss = 0.0000
2025-01-24 09:32:48,594 - Batch 600: Loss = 0.0000
2025-01-24 09:32:49,659 - Batch 700: Loss = 0.0000
2025-01-24 09:32:50,730 - Batch 800: Loss = 0.0000
2025-01-24 09:32:51,824 - Batch 900: Loss = 0.0000
2025-01-24 09:32:52,916 - Batch 1000: Loss = 0.0000
2025-01-24 09:32:54,029 - Batch 1100: Loss = 0.0000
2025-01-24 09:32:55,124 - Batch 1200: Loss = 0.0000
2025-01-24 09:32:56,214 - Batch 1300: Loss = 0.0000
2025-01-24 09:32:57,280 - Batch 1400: Loss = 0.0000
2025-01-24 09:32:58,362 - Batch 1500: Loss = 0.0000
2025-01-24 09:33:00,992 - Train Loss: 0.0000
2025-01-24 09:33:00,993 - Test Loss: 68051.2070
2025-01-24 09:33:00,993 - FID Score: 0.0538
2025-01-24 09:33:00,993 - Commitment Loss: 0.0000
2025-01-24 09:33:00,993 - Codebook Loss: 0.0000
2025-01-24 09:33:00,993 - Entropy Reg: 0.0000
2025-01-24 09:33:00,993 - L2 Reg: 0.0001
2025-01-24 09:33:01,016 - 
Epoch 38/50
2025-01-24 09:33:01,191 - Batch 0: Loss = 0.0000
2025-01-24 09:33:02,215 - Batch 100: Loss = 0.0000
2025-01-24 09:33:03,285 - Batch 200: Loss = 0.0000
2025-01-24 09:33:04,391 - Batch 300: Loss = 0.0000
2025-01-24 09:33:05,483 - Batch 400: Loss = 0.0000
2025-01-24 09:33:06,550 - Batch 500: Loss = 0.0000
2025-01-24 09:33:07,638 - Batch 600: Loss = 0.0000
2025-01-24 09:33:08,761 - Batch 700: Loss = 0.0000
2025-01-24 09:33:09,901 - Batch 800: Loss = 0.0000
2025-01-24 09:33:11,001 - Batch 900: Loss = 0.0000
2025-01-24 09:33:12,060 - Batch 1000: Loss = 0.0000
2025-01-24 09:33:13,113 - Batch 1100: Loss = 0.0000
2025-01-24 09:33:14,165 - Batch 1200: Loss = 0.0000
2025-01-24 09:33:15,219 - Batch 1300: Loss = 0.0000
2025-01-24 09:33:16,304 - Batch 1400: Loss = 0.0000
2025-01-24 09:33:17,366 - Batch 1500: Loss = 0.0000
2025-01-24 09:33:20,045 - Train Loss: 0.0000
2025-01-24 09:33:20,045 - Test Loss: 68051.2070
2025-01-24 09:33:20,045 - FID Score: 0.0538
2025-01-24 09:33:20,045 - Commitment Loss: 0.0000
2025-01-24 09:33:20,045 - Codebook Loss: 0.0000
2025-01-24 09:33:20,045 - Entropy Reg: 0.0000
2025-01-24 09:33:20,045 - L2 Reg: 0.0001
2025-01-24 09:33:20,068 - 
Epoch 39/50
2025-01-24 09:33:20,241 - Batch 0: Loss = 0.0000
2025-01-24 09:33:21,256 - Batch 100: Loss = 0.0000
2025-01-24 09:33:22,318 - Batch 200: Loss = 0.0000
2025-01-24 09:33:23,410 - Batch 300: Loss = 0.0000
2025-01-24 09:33:24,472 - Batch 400: Loss = 0.0000
2025-01-24 09:33:25,574 - Batch 500: Loss = 0.0000
2025-01-24 09:33:26,664 - Batch 600: Loss = 0.0000
2025-01-24 09:33:27,713 - Batch 700: Loss = 0.0000
2025-01-24 09:33:28,813 - Batch 800: Loss = 0.0000
2025-01-24 09:33:29,846 - Batch 900: Loss = 0.0000
2025-01-24 09:33:30,694 - Batch 1000: Loss = 0.0000
2025-01-24 09:33:31,610 - Batch 1100: Loss = 0.0000
2025-01-24 09:33:32,671 - Batch 1200: Loss = 0.0000
2025-01-24 09:33:33,750 - Batch 1300: Loss = 0.0000
2025-01-24 09:33:34,842 - Batch 1400: Loss = 0.0000
2025-01-24 09:33:35,920 - Batch 1500: Loss = 0.0000
2025-01-24 09:33:38,637 - Train Loss: 0.0000
2025-01-24 09:33:38,637 - Test Loss: 68051.2070
2025-01-24 09:33:38,637 - FID Score: 0.0538
2025-01-24 09:33:38,637 - Commitment Loss: 0.0000
2025-01-24 09:33:38,637 - Codebook Loss: 0.0000
2025-01-24 09:33:38,637 - Entropy Reg: 0.0000
2025-01-24 09:33:38,638 - L2 Reg: 0.0001
2025-01-24 09:33:38,660 - 
Epoch 40/50
2025-01-24 09:33:38,836 - Batch 0: Loss = 0.0000
2025-01-24 09:33:39,846 - Batch 100: Loss = 0.0000
2025-01-24 09:33:40,876 - Batch 200: Loss = 0.0000
2025-01-24 09:33:41,984 - Batch 300: Loss = 0.0000
2025-01-24 09:33:43,144 - Batch 400: Loss = 0.0000
2025-01-24 09:33:44,295 - Batch 500: Loss = 0.0000
2025-01-24 09:33:45,440 - Batch 600: Loss = 0.0000
2025-01-24 09:33:46,658 - Batch 700: Loss = 0.0000
2025-01-24 09:33:47,909 - Batch 800: Loss = 0.0000
2025-01-24 09:33:49,052 - Batch 900: Loss = 0.0000
2025-01-24 09:33:50,150 - Batch 1000: Loss = 0.0000
2025-01-24 09:33:51,282 - Batch 1100: Loss = 0.0000
2025-01-24 09:33:52,378 - Batch 1200: Loss = 0.0000
2025-01-24 09:33:53,477 - Batch 1300: Loss = 0.0000
2025-01-24 09:33:54,575 - Batch 1400: Loss = 0.0000
2025-01-24 09:33:55,669 - Batch 1500: Loss = 0.0000
2025-01-24 09:33:58,364 - Train Loss: 0.0000
2025-01-24 09:33:58,365 - Test Loss: 68051.2070
2025-01-24 09:33:58,365 - FID Score: 0.0538
2025-01-24 09:33:58,365 - Commitment Loss: 0.0000
2025-01-24 09:33:58,365 - Codebook Loss: 0.0000
2025-01-24 09:33:58,365 - Entropy Reg: 0.0000
2025-01-24 09:33:58,365 - L2 Reg: 0.0001
2025-01-24 09:33:58,388 - 
Epoch 41/50
2025-01-24 09:33:58,556 - Batch 0: Loss = 0.0000
2025-01-24 09:33:59,478 - Batch 100: Loss = 0.0000
2025-01-24 09:34:00,534 - Batch 200: Loss = 0.0000
2025-01-24 09:34:01,597 - Batch 300: Loss = 0.0000
2025-01-24 09:34:02,664 - Batch 400: Loss = 0.0000
2025-01-24 09:34:03,725 - Batch 500: Loss = 0.0000
2025-01-24 09:34:04,776 - Batch 600: Loss = 0.0000
2025-01-24 09:34:05,794 - Batch 700: Loss = 0.0000
2025-01-24 09:34:06,775 - Batch 800: Loss = 0.0000
2025-01-24 09:34:07,754 - Batch 900: Loss = 0.0000
2025-01-24 09:34:08,748 - Batch 1000: Loss = 0.0000
2025-01-24 09:34:09,750 - Batch 1100: Loss = 0.0000
2025-01-24 09:34:10,751 - Batch 1200: Loss = 0.0000
2025-01-24 09:34:11,756 - Batch 1300: Loss = 0.0000
2025-01-24 09:34:12,758 - Batch 1400: Loss = 0.0000
2025-01-24 09:34:13,748 - Batch 1500: Loss = 0.0000
2025-01-24 09:34:16,417 - Train Loss: 0.0000
2025-01-24 09:34:16,418 - Test Loss: 68051.2070
2025-01-24 09:34:16,418 - FID Score: 0.0538
2025-01-24 09:34:16,418 - Commitment Loss: 0.0000
2025-01-24 09:34:16,418 - Codebook Loss: 0.0000
2025-01-24 09:34:16,418 - Entropy Reg: 0.0000
2025-01-24 09:34:16,418 - L2 Reg: 0.0001
2025-01-24 09:34:16,427 - Saved checkpoint at epoch 40
2025-01-24 09:34:16,450 - 
Epoch 42/50
2025-01-24 09:34:16,622 - Batch 0: Loss = 0.0000
2025-01-24 09:34:17,579 - Batch 100: Loss = 0.0000
2025-01-24 09:34:18,639 - Batch 200: Loss = 0.0000
2025-01-24 09:34:19,700 - Batch 300: Loss = 0.0000
2025-01-24 09:34:20,784 - Batch 400: Loss = 0.0000
2025-01-24 09:34:21,873 - Batch 500: Loss = 0.0000
2025-01-24 09:34:22,933 - Batch 600: Loss = 0.0000
2025-01-24 09:34:23,990 - Batch 700: Loss = 0.0000
2025-01-24 09:34:25,033 - Batch 800: Loss = 0.0000
2025-01-24 09:34:26,055 - Batch 900: Loss = 0.0000
2025-01-24 09:34:27,064 - Batch 1000: Loss = 0.0000
2025-01-24 09:34:28,091 - Batch 1100: Loss = 0.0000
2025-01-24 09:34:29,131 - Batch 1200: Loss = 0.0000
2025-01-24 09:34:30,165 - Batch 1300: Loss = 0.0000
2025-01-24 09:34:31,220 - Batch 1400: Loss = 0.0000
2025-01-24 09:34:32,199 - Batch 1500: Loss = 0.0000
2025-01-24 09:34:34,856 - Train Loss: 0.0000
2025-01-24 09:34:34,856 - Test Loss: 68051.2070
2025-01-24 09:34:34,856 - FID Score: 0.0538
2025-01-24 09:34:34,856 - Commitment Loss: 0.0000
2025-01-24 09:34:34,856 - Codebook Loss: 0.0000
2025-01-24 09:34:34,856 - Entropy Reg: 0.0000
2025-01-24 09:34:34,857 - L2 Reg: 0.0001
2025-01-24 09:34:34,880 - 
Epoch 43/50
2025-01-24 09:34:35,058 - Batch 0: Loss = 0.0000
2025-01-24 09:34:36,073 - Batch 100: Loss = 0.0000
2025-01-24 09:34:37,102 - Batch 200: Loss = 0.0000
2025-01-24 09:34:38,143 - Batch 300: Loss = 0.0000
2025-01-24 09:34:39,208 - Batch 400: Loss = 0.0000
2025-01-24 09:34:40,232 - Batch 500: Loss = 0.0000
2025-01-24 09:34:41,130 - Batch 600: Loss = 0.0000
2025-01-24 09:34:42,094 - Batch 700: Loss = 0.0000
2025-01-24 09:34:43,067 - Batch 800: Loss = 0.0000
2025-01-24 09:34:44,073 - Batch 900: Loss = 0.0000
2025-01-24 09:34:45,122 - Batch 1000: Loss = 0.0000
2025-01-24 09:34:46,172 - Batch 1100: Loss = 0.0000
2025-01-24 09:34:47,221 - Batch 1200: Loss = 0.0000
2025-01-24 09:34:48,265 - Batch 1300: Loss = 0.0000
2025-01-24 09:34:49,316 - Batch 1400: Loss = 0.0000
2025-01-24 09:34:50,394 - Batch 1500: Loss = 0.0000
2025-01-24 09:34:53,102 - Train Loss: 0.0000
2025-01-24 09:34:53,102 - Test Loss: 68051.2070
2025-01-24 09:34:53,102 - FID Score: 0.0538
2025-01-24 09:34:53,102 - Commitment Loss: 0.0000
2025-01-24 09:34:53,102 - Codebook Loss: 0.0000
2025-01-24 09:34:53,103 - Entropy Reg: 0.0000
2025-01-24 09:34:53,103 - L2 Reg: 0.0001
2025-01-24 09:34:53,126 - 
Epoch 44/50
2025-01-24 09:34:53,301 - Batch 0: Loss = 0.0000
2025-01-24 09:34:54,248 - Batch 100: Loss = 0.0000
2025-01-24 09:34:55,219 - Batch 200: Loss = 0.0000
2025-01-24 09:34:56,200 - Batch 300: Loss = 0.0000
2025-01-24 09:34:57,253 - Batch 400: Loss = 0.0000
2025-01-24 09:34:58,285 - Batch 500: Loss = 0.0000
2025-01-24 09:34:59,327 - Batch 600: Loss = 0.0000
2025-01-24 09:35:00,356 - Batch 700: Loss = 0.0000
2025-01-24 09:35:01,410 - Batch 800: Loss = 0.0000
2025-01-24 09:35:02,490 - Batch 900: Loss = 0.0000
2025-01-24 09:35:03,593 - Batch 1000: Loss = 0.0000
2025-01-24 09:35:04,688 - Batch 1100: Loss = 0.0000
2025-01-24 09:35:05,760 - Batch 1200: Loss = 0.0000
2025-01-24 09:35:06,807 - Batch 1300: Loss = 0.0000
2025-01-24 09:35:07,864 - Batch 1400: Loss = 0.0000
2025-01-24 09:35:08,911 - Batch 1500: Loss = 0.0000
2025-01-24 09:35:11,631 - Train Loss: 0.0000
2025-01-24 09:35:11,631 - Test Loss: 68051.2070
2025-01-24 09:35:11,631 - FID Score: 0.0538
2025-01-24 09:35:11,631 - Commitment Loss: 0.0000
2025-01-24 09:35:11,631 - Codebook Loss: 0.0000
2025-01-24 09:35:11,631 - Entropy Reg: 0.0000
2025-01-24 09:35:11,631 - L2 Reg: 0.0001
2025-01-24 09:35:11,654 - 
Epoch 45/50
2025-01-24 09:35:11,823 - Batch 0: Loss = 0.0000
2025-01-24 09:35:12,836 - Batch 100: Loss = 0.0000
2025-01-24 09:35:13,676 - Batch 200: Loss = 0.0000
2025-01-24 09:35:14,594 - Batch 300: Loss = 0.0000
2025-01-24 09:35:15,616 - Batch 400: Loss = 0.0000
2025-01-24 09:35:16,640 - Batch 500: Loss = 0.0000
2025-01-24 09:35:17,682 - Batch 600: Loss = 0.0000
2025-01-24 09:35:18,763 - Batch 700: Loss = 0.0000
2025-01-24 09:35:19,837 - Batch 800: Loss = 0.0000
2025-01-24 09:35:20,927 - Batch 900: Loss = 0.0000
2025-01-24 09:35:21,979 - Batch 1000: Loss = 0.0000
2025-01-24 09:35:23,026 - Batch 1100: Loss = 0.0000
2025-01-24 09:35:24,069 - Batch 1200: Loss = 0.0000
2025-01-24 09:35:25,120 - Batch 1300: Loss = 0.0000
2025-01-24 09:35:26,165 - Batch 1400: Loss = 0.0000
2025-01-24 09:35:27,212 - Batch 1500: Loss = 0.0000
2025-01-24 09:35:29,925 - Train Loss: 0.0000
2025-01-24 09:35:29,925 - Test Loss: 68051.2070
2025-01-24 09:35:29,925 - FID Score: 0.0538
2025-01-24 09:35:29,925 - Commitment Loss: 0.0000
2025-01-24 09:35:29,925 - Codebook Loss: 0.0000
2025-01-24 09:35:29,925 - Entropy Reg: 0.0000
2025-01-24 09:35:29,925 - L2 Reg: 0.0001
2025-01-24 09:35:29,948 - 
Epoch 46/50
2025-01-24 09:35:30,121 - Batch 0: Loss = 0.0000
2025-01-24 09:35:30,887 - Batch 100: Loss = 0.0000
2025-01-24 09:35:31,587 - Batch 200: Loss = 0.0000
2025-01-24 09:35:32,304 - Batch 300: Loss = 0.0000
2025-01-24 09:35:33,160 - Batch 400: Loss = 0.0000
2025-01-24 09:35:34,061 - Batch 500: Loss = 0.0000
2025-01-24 09:35:34,954 - Batch 600: Loss = 0.0000
2025-01-24 09:35:35,848 - Batch 700: Loss = 0.0000
2025-01-24 09:35:36,784 - Batch 800: Loss = 0.0000
2025-01-24 09:35:37,704 - Batch 900: Loss = 0.0000
2025-01-24 09:35:38,616 - Batch 1000: Loss = 0.0000
2025-01-24 09:35:39,516 - Batch 1100: Loss = 0.0000
2025-01-24 09:35:40,411 - Batch 1200: Loss = 0.0000
2025-01-24 09:35:41,308 - Batch 1300: Loss = 0.0000
2025-01-24 09:35:42,209 - Batch 1400: Loss = 0.0000
2025-01-24 09:35:43,141 - Batch 1500: Loss = 0.0000
2025-01-24 09:35:45,802 - Train Loss: 0.0000
2025-01-24 09:35:45,802 - Test Loss: 68051.2070
2025-01-24 09:35:45,802 - FID Score: 0.0538
2025-01-24 09:35:45,802 - Commitment Loss: 0.0000
2025-01-24 09:35:45,802 - Codebook Loss: 0.0000
2025-01-24 09:35:45,802 - Entropy Reg: 0.0000
2025-01-24 09:35:45,802 - L2 Reg: 0.0001
2025-01-24 09:35:45,825 - 
Epoch 47/50
2025-01-24 09:35:45,999 - Batch 0: Loss = 0.0000
2025-01-24 09:35:46,678 - Batch 100: Loss = 0.0000
2025-01-24 09:35:47,375 - Batch 200: Loss = 0.0000
2025-01-24 09:35:48,057 - Batch 300: Loss = 0.0000
2025-01-24 09:35:48,721 - Batch 400: Loss = 0.0000
2025-01-24 09:35:49,405 - Batch 500: Loss = 0.0000
2025-01-24 09:35:50,095 - Batch 600: Loss = 0.0000
2025-01-24 09:35:50,780 - Batch 700: Loss = 0.0000
2025-01-24 09:35:51,474 - Batch 800: Loss = 0.0000
2025-01-24 09:35:52,166 - Batch 900: Loss = 0.0000
2025-01-24 09:35:52,852 - Batch 1000: Loss = 0.0000
2025-01-24 09:35:53,544 - Batch 1100: Loss = 0.0000
2025-01-24 09:35:54,233 - Batch 1200: Loss = 0.0000
2025-01-24 09:35:54,917 - Batch 1300: Loss = 0.0000
2025-01-24 09:35:55,608 - Batch 1400: Loss = 0.0000
2025-01-24 09:35:56,303 - Batch 1500: Loss = 0.0000
2025-01-24 09:35:58,654 - Train Loss: 0.0000
2025-01-24 09:35:58,654 - Test Loss: 68051.2070
2025-01-24 09:35:58,654 - FID Score: 0.0538
2025-01-24 09:35:58,654 - Commitment Loss: 0.0000
2025-01-24 09:35:58,654 - Codebook Loss: 0.0000
2025-01-24 09:35:58,654 - Entropy Reg: 0.0000
2025-01-24 09:35:58,654 - L2 Reg: 0.0001
2025-01-24 09:35:58,674 - 
Epoch 48/50
2025-01-24 09:35:58,791 - Batch 0: Loss = 0.0000
2025-01-24 09:35:59,615 - Batch 100: Loss = 0.0000
2025-01-24 09:36:00,640 - Batch 200: Loss = 0.0000
2025-01-24 09:36:01,684 - Batch 300: Loss = 0.0000
2025-01-24 09:36:02,687 - Batch 400: Loss = 0.0000
2025-01-24 09:36:03,734 - Batch 500: Loss = 0.0000
2025-01-24 09:36:04,790 - Batch 600: Loss = 0.0000
2025-01-24 09:36:05,857 - Batch 700: Loss = 0.0000
2025-01-24 09:36:06,955 - Batch 800: Loss = 0.0000
2025-01-24 09:36:08,049 - Batch 900: Loss = 0.0000
2025-01-24 09:36:09,144 - Batch 1000: Loss = 0.0000
2025-01-24 09:36:10,205 - Batch 1100: Loss = 0.0000
2025-01-24 09:36:11,163 - Batch 1200: Loss = 0.0000
2025-01-24 09:36:12,116 - Batch 1300: Loss = 0.0000
2025-01-24 09:36:13,056 - Batch 1400: Loss = 0.0000
2025-01-24 09:36:13,988 - Batch 1500: Loss = 0.0000
2025-01-24 09:36:16,676 - Train Loss: 0.0000
2025-01-24 09:36:16,676 - Test Loss: 68051.2070
2025-01-24 09:36:16,676 - FID Score: 0.0538
2025-01-24 09:36:16,676 - Commitment Loss: 0.0000
2025-01-24 09:36:16,676 - Codebook Loss: 0.0000
2025-01-24 09:36:16,676 - Entropy Reg: 0.0000
2025-01-24 09:36:16,676 - L2 Reg: 0.0001
2025-01-24 09:36:16,696 - 
Epoch 49/50
2025-01-24 09:36:16,822 - Batch 0: Loss = 0.0000
2025-01-24 09:36:17,649 - Batch 100: Loss = 0.0000
2025-01-24 09:36:18,625 - Batch 200: Loss = 0.0000
2025-01-24 09:36:19,683 - Batch 300: Loss = 0.0000
2025-01-24 09:36:20,756 - Batch 400: Loss = 0.0000
2025-01-24 09:36:21,822 - Batch 500: Loss = 0.0000
2025-01-24 09:36:22,832 - Batch 600: Loss = 0.0000
2025-01-24 09:36:23,814 - Batch 700: Loss = 0.0000
2025-01-24 09:36:24,851 - Batch 800: Loss = 0.0000
2025-01-24 09:36:25,979 - Batch 900: Loss = 0.0000
2025-01-24 09:36:27,100 - Batch 1000: Loss = 0.0000
2025-01-24 09:36:28,209 - Batch 1100: Loss = 0.0000
2025-01-24 09:36:29,314 - Batch 1200: Loss = 0.0000
2025-01-24 09:36:30,425 - Batch 1300: Loss = 0.0000
2025-01-24 09:36:31,526 - Batch 1400: Loss = 0.0000
2025-01-24 09:36:32,607 - Batch 1500: Loss = 0.0000
2025-01-24 09:36:35,339 - Train Loss: 0.0000
2025-01-24 09:36:35,339 - Test Loss: 68051.2070
2025-01-24 09:36:35,339 - FID Score: 0.0538
2025-01-24 09:36:35,339 - Commitment Loss: 0.0000
2025-01-24 09:36:35,339 - Codebook Loss: 0.0000
2025-01-24 09:36:35,339 - Entropy Reg: 0.0000
2025-01-24 09:36:35,339 - L2 Reg: 0.0001
2025-01-24 09:36:35,362 - 
Epoch 50/50
2025-01-24 09:36:35,543 - Batch 0: Loss = 0.0000
2025-01-24 09:36:36,564 - Batch 100: Loss = 0.0000
2025-01-24 09:36:37,615 - Batch 200: Loss = 0.0000
2025-01-24 09:36:38,664 - Batch 300: Loss = 0.0000
2025-01-24 09:36:39,732 - Batch 400: Loss = 0.0000
2025-01-24 09:36:40,801 - Batch 500: Loss = 0.0000
2025-01-24 09:36:41,870 - Batch 600: Loss = 0.0000
2025-01-24 09:36:42,963 - Batch 700: Loss = 0.0000
2025-01-24 09:36:44,054 - Batch 800: Loss = 0.0000
2025-01-24 09:36:45,139 - Batch 900: Loss = 0.0000
2025-01-24 09:36:46,224 - Batch 1000: Loss = 0.0000
2025-01-24 09:36:47,324 - Batch 1100: Loss = 0.0000
2025-01-24 09:36:48,414 - Batch 1200: Loss = 0.0000
2025-01-24 09:36:49,510 - Batch 1300: Loss = 0.0000
2025-01-24 09:36:50,584 - Batch 1400: Loss = 0.0000
2025-01-24 09:36:51,683 - Batch 1500: Loss = 0.0000
2025-01-24 09:36:54,387 - Train Loss: 0.0000
2025-01-24 09:36:54,388 - Test Loss: 68051.2070
2025-01-24 09:36:54,388 - FID Score: 0.0538
2025-01-24 09:36:54,388 - Commitment Loss: 0.0000
2025-01-24 09:36:54,388 - Codebook Loss: 0.0000
2025-01-24 09:36:54,388 - Entropy Reg: 0.0000
2025-01-24 09:36:54,388 - L2 Reg: 0.0001
2025-01-24 09:36:54,412 - Training history saved to regularized_training_history.json
2025-01-24 09:36:56,405 - 
Final Evaluation:
2025-01-24 09:36:56,405 - Test Loss: 68051.2070
2025-01-24 09:36:56,405 - FID Score: 0.0538
2025-01-24 09:59:31,722 - Using device: 0
2025-01-24 09:59:32,646 - 
Epoch 1/50
2025-01-24 09:59:34,075 - Batch 0: Loss = 0.0201
2025-01-24 09:59:34,982 - Batch 100: Loss = 0.0149
2025-01-24 09:59:35,937 - Batch 200: Loss = 0.0137
2025-01-24 09:59:36,932 - Batch 300: Loss = 0.0126
2025-01-24 09:59:37,932 - Batch 400: Loss = 0.0116
2025-01-24 09:59:38,928 - Batch 500: Loss = 0.0107
2025-01-24 09:59:39,915 - Batch 600: Loss = 0.0098
2025-01-24 09:59:40,914 - Batch 700: Loss = 0.0089
2025-01-24 09:59:41,928 - Batch 800: Loss = 0.0081
2025-01-24 09:59:42,954 - Batch 900: Loss = 0.0074
2025-01-24 09:59:43,900 - Batch 1000: Loss = 0.0066
2025-01-24 09:59:44,773 - Batch 1100: Loss = 0.0059
2025-01-24 09:59:45,464 - Batch 1200: Loss = 0.0053
2025-01-24 09:59:46,126 - Batch 1300: Loss = 0.0047
2025-01-24 09:59:46,789 - Batch 1400: Loss = 0.0041
2025-01-24 09:59:47,453 - Batch 1500: Loss = 0.0035
2025-01-24 09:59:49,931 - Train Loss: 0.0088
2025-01-24 09:59:49,931 - Test Loss: 68412.8843
2025-01-24 09:59:49,931 - FID Score: 0.1331
2025-01-24 09:59:49,931 - Commitment Loss: 0.0000
2025-01-24 09:59:49,931 - Codebook Loss: 0.0000
2025-01-24 09:59:49,931 - Entropy Reg: 0.0000
2025-01-24 09:59:49,931 - L2 Reg: 0.8711
2025-01-24 09:59:49,936 - Saved checkpoint at epoch 0
2025-01-24 09:59:49,954 - 
Epoch 2/50
2025-01-24 09:59:50,077 - Batch 0: Loss = 0.0032
2025-01-24 09:59:50,579 - Batch 100: Loss = 0.0028
2025-01-24 09:59:51,150 - Batch 200: Loss = 0.0023
2025-01-24 09:59:51,914 - Batch 300: Loss = 0.0020
2025-01-24 09:59:52,935 - Batch 400: Loss = 0.0016
2025-01-24 09:59:53,941 - Batch 500: Loss = 0.0013
2025-01-24 09:59:54,960 - Batch 600: Loss = 0.0011
2025-01-24 09:59:55,915 - Batch 700: Loss = 0.0009
2025-01-24 09:59:56,870 - Batch 800: Loss = 0.0007
2025-01-24 09:59:57,849 - Batch 900: Loss = 0.0005
2025-01-24 09:59:58,856 - Batch 1000: Loss = 0.0003
2025-01-24 09:59:59,864 - Batch 1100: Loss = 0.0002
2025-01-24 10:00:00,883 - Batch 1200: Loss = 0.0000
2025-01-24 10:00:01,914 - Batch 1300: Loss = 0.0000
2025-01-24 10:00:02,961 - Batch 1400: Loss = 0.0000
2025-01-24 10:00:04,002 - Batch 1500: Loss = 0.0000
2025-01-24 10:00:06,706 - Train Loss: 0.0010
2025-01-24 10:00:06,706 - Test Loss: 68412.8218
2025-01-24 10:00:06,706 - FID Score: 0.1329
2025-01-24 10:00:06,706 - Commitment Loss: 0.0000
2025-01-24 10:00:06,706 - Codebook Loss: 0.0000
2025-01-24 10:00:06,707 - Entropy Reg: 0.0000
2025-01-24 10:00:06,707 - L2 Reg: 0.0979
2025-01-24 10:00:06,730 - 
Epoch 3/50
2025-01-24 10:00:06,922 - Batch 0: Loss = 0.0000
2025-01-24 10:00:07,803 - Batch 100: Loss = 0.0000
2025-01-24 10:00:08,706 - Batch 200: Loss = 0.0000
2025-01-24 10:00:09,623 - Batch 300: Loss = 0.0000
2025-01-24 10:00:10,532 - Batch 400: Loss = 0.0000
2025-01-24 10:00:11,445 - Batch 500: Loss = 0.0000
2025-01-24 10:00:12,361 - Batch 600: Loss = 0.0000
2025-01-24 10:00:13,273 - Batch 700: Loss = 0.0000
2025-01-24 10:00:14,186 - Batch 800: Loss = 0.0000
2025-01-24 10:00:15,099 - Batch 900: Loss = 0.0000
2025-01-24 10:00:16,011 - Batch 1000: Loss = 0.0000
2025-01-24 10:00:16,914 - Batch 1100: Loss = 0.0000
2025-01-24 10:00:17,777 - Batch 1200: Loss = 0.0000
2025-01-24 10:00:18,585 - Batch 1300: Loss = 0.0000
2025-01-24 10:00:19,472 - Batch 1400: Loss = 0.0000
2025-01-24 10:00:20,474 - Batch 1500: Loss = 0.0000
2025-01-24 10:00:23,158 - Train Loss: 0.0000
2025-01-24 10:00:23,158 - Test Loss: 68412.7124
2025-01-24 10:00:23,158 - FID Score: 0.1330
2025-01-24 10:00:23,158 - Commitment Loss: 0.0000
2025-01-24 10:00:23,158 - Codebook Loss: 0.0000
2025-01-24 10:00:23,158 - Entropy Reg: 0.0000
2025-01-24 10:00:23,159 - L2 Reg: 0.0003
2025-01-24 10:00:23,182 - 
Epoch 4/50
2025-01-24 10:00:23,377 - Batch 0: Loss = 0.0000
2025-01-24 10:00:24,268 - Batch 100: Loss = 0.0000
2025-01-24 10:00:25,138 - Batch 200: Loss = 0.0000
2025-01-24 10:00:26,009 - Batch 300: Loss = 0.0000
2025-01-24 10:00:26,763 - Batch 400: Loss = 0.0000
2025-01-24 10:00:27,321 - Batch 500: Loss = 0.0000
2025-01-24 10:00:27,883 - Batch 600: Loss = 0.0000
2025-01-24 10:00:28,443 - Batch 700: Loss = 0.0000
2025-01-24 10:00:29,001 - Batch 800: Loss = 0.0000
2025-01-24 10:00:29,556 - Batch 900: Loss = 0.0000
2025-01-24 10:00:30,133 - Batch 1000: Loss = 0.0000
2025-01-24 10:00:31,047 - Batch 1100: Loss = 0.0000
2025-01-24 10:00:32,028 - Batch 1200: Loss = 0.0000
2025-01-24 10:00:32,949 - Batch 1300: Loss = 0.0000
2025-01-24 10:00:33,605 - Batch 1400: Loss = 0.0000
2025-01-24 10:00:34,250 - Batch 1500: Loss = 0.0000
2025-01-24 10:00:36,561 - Train Loss: 0.0000
2025-01-24 10:00:36,561 - Test Loss: 68412.6655
2025-01-24 10:00:36,561 - FID Score: 0.1331
2025-01-24 10:00:36,561 - Commitment Loss: 0.0000
2025-01-24 10:00:36,561 - Codebook Loss: 0.0000
2025-01-24 10:00:36,561 - Entropy Reg: 0.0000
2025-01-24 10:00:36,561 - L2 Reg: 0.0002
2025-01-24 10:00:36,584 - 
Epoch 5/50
2025-01-24 10:00:36,773 - Batch 0: Loss = 0.0000
2025-01-24 10:00:37,604 - Batch 100: Loss = 0.0000
2025-01-24 10:00:38,565 - Batch 200: Loss = 0.0000
2025-01-24 10:00:39,570 - Batch 300: Loss = 0.0000
2025-01-24 10:00:40,589 - Batch 400: Loss = 0.0000
2025-01-24 10:00:41,580 - Batch 500: Loss = 0.0000
2025-01-24 10:00:42,540 - Batch 600: Loss = 0.0000
2025-01-24 10:00:43,427 - Batch 700: Loss = 0.0000
2025-01-24 10:00:44,277 - Batch 800: Loss = 0.0000
2025-01-24 10:00:45,126 - Batch 900: Loss = 0.0000
2025-01-24 10:00:45,984 - Batch 1000: Loss = 0.0000
2025-01-24 10:00:46,849 - Batch 1100: Loss = 0.0000
2025-01-24 10:00:47,695 - Batch 1200: Loss = 0.0000
2025-01-24 10:00:48,542 - Batch 1300: Loss = 0.0000
2025-01-24 10:00:49,391 - Batch 1400: Loss = 0.0000
2025-01-24 10:00:50,240 - Batch 1500: Loss = 0.0000
2025-01-24 10:00:52,850 - Train Loss: 0.0000
2025-01-24 10:00:52,851 - Test Loss: 68412.6421
2025-01-24 10:00:52,851 - FID Score: 0.1331
2025-01-24 10:00:52,851 - Commitment Loss: 0.0000
2025-01-24 10:00:52,851 - Codebook Loss: 0.0000
2025-01-24 10:00:52,851 - Entropy Reg: 0.0000
2025-01-24 10:00:52,851 - L2 Reg: 0.0002
2025-01-24 10:00:52,874 - 
Epoch 6/50
2025-01-24 10:00:53,077 - Batch 0: Loss = 0.0000
2025-01-24 10:00:53,662 - Batch 100: Loss = 0.0000
2025-01-24 10:00:54,204 - Batch 200: Loss = 0.0000
2025-01-24 10:00:54,746 - Batch 300: Loss = 0.0000
2025-01-24 10:00:55,290 - Batch 400: Loss = 0.0000
2025-01-24 10:00:55,835 - Batch 500: Loss = 0.0000
2025-01-24 10:00:56,381 - Batch 600: Loss = 0.0000
2025-01-24 10:00:56,924 - Batch 700: Loss = 0.0000
2025-01-24 10:00:57,469 - Batch 800: Loss = 0.0000
2025-01-24 10:00:58,017 - Batch 900: Loss = 0.0000
2025-01-24 10:00:58,571 - Batch 1000: Loss = 0.0000
2025-01-24 10:00:59,119 - Batch 1100: Loss = 0.0000
2025-01-24 10:00:59,668 - Batch 1200: Loss = 0.0000
2025-01-24 10:01:00,217 - Batch 1300: Loss = 0.0000
2025-01-24 10:01:00,767 - Batch 1400: Loss = 0.0000
2025-01-24 10:01:01,318 - Batch 1500: Loss = 0.0000
2025-01-24 10:01:03,566 - Train Loss: 0.0000
2025-01-24 10:01:03,566 - Test Loss: 68412.6265
2025-01-24 10:01:03,566 - FID Score: 0.1329
2025-01-24 10:01:03,566 - Commitment Loss: 0.0000
2025-01-24 10:01:03,566 - Codebook Loss: 0.0000
2025-01-24 10:01:03,566 - Entropy Reg: 0.0000
2025-01-24 10:01:03,566 - L2 Reg: 0.0002
2025-01-24 10:01:03,589 - 
Epoch 7/50
2025-01-24 10:01:03,776 - Batch 0: Loss = 0.0000
2025-01-24 10:01:04,593 - Batch 100: Loss = 0.0000
2025-01-24 10:01:05,412 - Batch 200: Loss = 0.0000
2025-01-24 10:01:06,227 - Batch 300: Loss = 0.0000
2025-01-24 10:01:07,044 - Batch 400: Loss = 0.0000
2025-01-24 10:01:07,927 - Batch 500: Loss = 0.0000
2025-01-24 10:01:08,911 - Batch 600: Loss = 0.0000
2025-01-24 10:01:09,904 - Batch 700: Loss = 0.0000
2025-01-24 10:01:10,896 - Batch 800: Loss = 0.0000
2025-01-24 10:01:11,892 - Batch 900: Loss = 0.0000
2025-01-24 10:01:12,885 - Batch 1000: Loss = 0.0000
2025-01-24 10:01:13,844 - Batch 1100: Loss = 0.0000
2025-01-24 10:01:14,679 - Batch 1200: Loss = 0.0000
2025-01-24 10:01:15,513 - Batch 1300: Loss = 0.0000
2025-01-24 10:01:16,358 - Batch 1400: Loss = 0.0000
2025-01-24 10:01:17,192 - Batch 1500: Loss = 0.0000
2025-01-24 10:01:19,777 - Train Loss: 0.0000
2025-01-24 10:01:19,777 - Test Loss: 68412.6187
2025-01-24 10:01:19,778 - FID Score: 0.1331
2025-01-24 10:01:19,778 - Commitment Loss: 0.0000
2025-01-24 10:01:19,778 - Codebook Loss: 0.0000
2025-01-24 10:01:19,778 - Entropy Reg: 0.0000
2025-01-24 10:01:19,778 - L2 Reg: 0.0001
2025-01-24 10:01:19,801 - 
Epoch 8/50
2025-01-24 10:01:19,996 - Batch 0: Loss = 0.0000
2025-01-24 10:01:20,904 - Batch 100: Loss = 0.0000
2025-01-24 10:01:21,867 - Batch 200: Loss = 0.0000
2025-01-24 10:01:22,853 - Batch 300: Loss = 0.0000
2025-01-24 10:01:23,836 - Batch 400: Loss = 0.0000
2025-01-24 10:01:24,809 - Batch 500: Loss = 0.0000
2025-01-24 10:01:25,798 - Batch 600: Loss = 0.0000
2025-01-24 10:01:26,780 - Batch 700: Loss = 0.0000
2025-01-24 10:01:27,770 - Batch 800: Loss = 0.0000
2025-01-24 10:01:28,755 - Batch 900: Loss = 0.0000
2025-01-24 10:01:29,741 - Batch 1000: Loss = 0.0000
2025-01-24 10:01:30,727 - Batch 1100: Loss = 0.0000
2025-01-24 10:01:31,712 - Batch 1200: Loss = 0.0000
2025-01-24 10:01:32,700 - Batch 1300: Loss = 0.0000
2025-01-24 10:01:33,685 - Batch 1400: Loss = 0.0000
2025-01-24 10:01:34,662 - Batch 1500: Loss = 0.0000
2025-01-24 10:01:37,335 - Train Loss: 0.0000
2025-01-24 10:01:37,336 - Test Loss: 68412.6108
2025-01-24 10:01:37,336 - FID Score: 0.1329
2025-01-24 10:01:37,336 - Commitment Loss: 0.0000
2025-01-24 10:01:37,336 - Codebook Loss: 0.0000
2025-01-24 10:01:37,336 - Entropy Reg: 0.0000
2025-01-24 10:01:37,336 - L2 Reg: 0.0002
2025-01-24 10:01:37,359 - 
Epoch 9/50
2025-01-24 10:01:37,551 - Batch 0: Loss = 0.0000
2025-01-24 10:01:38,378 - Batch 100: Loss = 0.0000
2025-01-24 10:01:39,300 - Batch 200: Loss = 0.0000
2025-01-24 10:01:40,263 - Batch 300: Loss = 0.0000
2025-01-24 10:01:41,238 - Batch 400: Loss = 0.0000
2025-01-24 10:01:42,233 - Batch 500: Loss = 0.0000
2025-01-24 10:01:43,224 - Batch 600: Loss = 0.0000
2025-01-24 10:01:44,149 - Batch 700: Loss = 0.0000
2025-01-24 10:01:45,117 - Batch 800: Loss = 0.0000
2025-01-24 10:01:46,110 - Batch 900: Loss = 0.0000
2025-01-24 10:01:47,101 - Batch 1000: Loss = 0.0000
2025-01-24 10:01:48,092 - Batch 1100: Loss = 0.0000
2025-01-24 10:01:49,082 - Batch 1200: Loss = 0.0000
2025-01-24 10:01:50,070 - Batch 1300: Loss = 0.0000
2025-01-24 10:01:51,065 - Batch 1400: Loss = 0.0000
2025-01-24 10:01:52,059 - Batch 1500: Loss = 0.0000
2025-01-24 10:01:54,750 - Train Loss: 0.0000
2025-01-24 10:01:54,751 - Test Loss: 68412.6108
2025-01-24 10:01:54,751 - FID Score: 0.1331
2025-01-24 10:01:54,751 - Commitment Loss: 0.0000
2025-01-24 10:01:54,751 - Codebook Loss: 0.0000
2025-01-24 10:01:54,751 - Entropy Reg: 0.0000
2025-01-24 10:01:54,751 - L2 Reg: 0.0001
2025-01-24 10:01:54,774 - 
Epoch 10/50
2025-01-24 10:01:54,978 - Batch 0: Loss = 0.0000
2025-01-24 10:01:55,823 - Batch 100: Loss = 0.0000
2025-01-24 10:01:56,746 - Batch 200: Loss = 0.0000
2025-01-24 10:01:57,738 - Batch 300: Loss = 0.0000
2025-01-24 10:01:58,732 - Batch 400: Loss = 0.0000
2025-01-24 10:01:59,724 - Batch 500: Loss = 0.0000
2025-01-24 10:02:00,625 - Batch 600: Loss = 0.0000
2025-01-24 10:02:01,500 - Batch 700: Loss = 0.0000
2025-01-24 10:02:02,357 - Batch 800: Loss = 0.0000
2025-01-24 10:02:03,206 - Batch 900: Loss = 0.0000
2025-01-24 10:02:04,045 - Batch 1000: Loss = 0.0000
2025-01-24 10:02:04,885 - Batch 1100: Loss = 0.0000
2025-01-24 10:02:05,724 - Batch 1200: Loss = 0.0000
2025-01-24 10:02:06,564 - Batch 1300: Loss = 0.0000
2025-01-24 10:02:07,407 - Batch 1400: Loss = 0.0000
2025-01-24 10:02:08,252 - Batch 1500: Loss = 0.0000
2025-01-24 10:02:10,805 - Train Loss: 0.0000
2025-01-24 10:02:10,805 - Test Loss: 68412.6030
2025-01-24 10:02:10,805 - FID Score: 0.1329
2025-01-24 10:02:10,805 - Commitment Loss: 0.0000
2025-01-24 10:02:10,805 - Codebook Loss: 0.0000
2025-01-24 10:02:10,805 - Entropy Reg: 0.0000
2025-01-24 10:02:10,805 - L2 Reg: 0.0001
2025-01-24 10:02:10,828 - 
Epoch 11/50
2025-01-24 10:02:11,021 - Batch 0: Loss = 0.0000
2025-01-24 10:02:11,932 - Batch 100: Loss = 0.0000
2025-01-24 10:02:12,933 - Batch 200: Loss = 0.0000
2025-01-24 10:02:13,926 - Batch 300: Loss = 0.0000
2025-01-24 10:02:14,926 - Batch 400: Loss = 0.0000
2025-01-24 10:02:15,851 - Batch 500: Loss = 0.0000
2025-01-24 10:02:16,837 - Batch 600: Loss = 0.0000
2025-01-24 10:02:17,858 - Batch 700: Loss = 0.0000
2025-01-24 10:02:18,881 - Batch 800: Loss = 0.0000
2025-01-24 10:02:19,899 - Batch 900: Loss = 0.0000
2025-01-24 10:02:20,928 - Batch 1000: Loss = 0.0000
2025-01-24 10:02:21,938 - Batch 1100: Loss = 0.0000
2025-01-24 10:02:22,932 - Batch 1200: Loss = 0.0000
2025-01-24 10:02:23,928 - Batch 1300: Loss = 0.0000
2025-01-24 10:02:24,856 - Batch 1400: Loss = 0.0000
2025-01-24 10:02:25,589 - Batch 1500: Loss = 0.0000
2025-01-24 10:02:27,996 - Train Loss: 0.0000
2025-01-24 10:02:27,996 - Test Loss: 68412.6030
2025-01-24 10:02:27,996 - FID Score: 0.1330
2025-01-24 10:02:27,996 - Commitment Loss: 0.0000
2025-01-24 10:02:27,996 - Codebook Loss: 0.0000
2025-01-24 10:02:27,996 - Entropy Reg: 0.0000
2025-01-24 10:02:27,996 - L2 Reg: 0.0001
2025-01-24 10:02:28,004 - Saved checkpoint at epoch 10
2025-01-24 10:02:28,027 - 
Epoch 12/50
2025-01-24 10:02:28,223 - Batch 0: Loss = 0.0000
2025-01-24 10:02:28,939 - Batch 100: Loss = 0.0000
2025-01-24 10:02:29,730 - Batch 200: Loss = 0.0000
2025-01-24 10:02:30,577 - Batch 300: Loss = 0.0000
2025-01-24 10:02:31,444 - Batch 400: Loss = 0.0000
2025-01-24 10:02:32,307 - Batch 500: Loss = 0.0000
2025-01-24 10:02:33,171 - Batch 600: Loss = 0.0000
2025-01-24 10:02:34,042 - Batch 700: Loss = 0.0000
2025-01-24 10:02:34,963 - Batch 800: Loss = 0.0000
2025-01-24 10:02:35,954 - Batch 900: Loss = 0.0000
2025-01-24 10:02:36,974 - Batch 1000: Loss = 0.0000
2025-01-24 10:02:37,996 - Batch 1100: Loss = 0.0000
2025-01-24 10:02:39,006 - Batch 1200: Loss = 0.0000
2025-01-24 10:02:40,021 - Batch 1300: Loss = 0.0000
2025-01-24 10:02:41,002 - Batch 1400: Loss = 0.0000
2025-01-24 10:02:41,881 - Batch 1500: Loss = 0.0000
2025-01-24 10:02:44,477 - Train Loss: 0.0000
2025-01-24 10:02:44,478 - Test Loss: 68412.6030
2025-01-24 10:02:44,478 - FID Score: 0.1329
2025-01-24 10:02:44,478 - Commitment Loss: 0.0000
2025-01-24 10:02:44,478 - Codebook Loss: 0.0000
2025-01-24 10:02:44,478 - Entropy Reg: 0.0000
2025-01-24 10:02:44,478 - L2 Reg: 0.0001
2025-01-24 10:02:44,501 - 
Epoch 13/50
2025-01-24 10:02:44,703 - Batch 0: Loss = 0.0000
2025-01-24 10:02:45,579 - Batch 100: Loss = 0.0000
2025-01-24 10:02:46,449 - Batch 200: Loss = 0.0000
2025-01-24 10:02:47,385 - Batch 300: Loss = 0.0000
2025-01-24 10:02:48,376 - Batch 400: Loss = 0.0000
2025-01-24 10:02:49,369 - Batch 500: Loss = 0.0000
2025-01-24 10:02:50,353 - Batch 600: Loss = 0.0000
2025-01-24 10:02:51,256 - Batch 700: Loss = 0.0000
2025-01-24 10:02:52,141 - Batch 800: Loss = 0.0000
2025-01-24 10:02:53,020 - Batch 900: Loss = 0.0000
2025-01-24 10:02:53,890 - Batch 1000: Loss = 0.0000
2025-01-24 10:02:54,751 - Batch 1100: Loss = 0.0000
2025-01-24 10:02:55,612 - Batch 1200: Loss = 0.0000
2025-01-24 10:02:56,471 - Batch 1300: Loss = 0.0000
2025-01-24 10:02:57,316 - Batch 1400: Loss = 0.0000
2025-01-24 10:02:58,165 - Batch 1500: Loss = 0.0000
2025-01-24 10:03:00,764 - Train Loss: 0.0000
2025-01-24 10:03:00,765 - Test Loss: 68412.5952
2025-01-24 10:03:00,765 - FID Score: 0.1331
2025-01-24 10:03:00,765 - Commitment Loss: 0.0000
2025-01-24 10:03:00,765 - Codebook Loss: 0.0000
2025-01-24 10:03:00,765 - Entropy Reg: 0.0000
2025-01-24 10:03:00,765 - L2 Reg: 0.0001
2025-01-24 10:03:00,788 - 
Epoch 14/50
2025-01-24 10:03:00,977 - Batch 0: Loss = 0.0000
2025-01-24 10:03:01,509 - Batch 100: Loss = 0.0000
2025-01-24 10:03:02,099 - Batch 200: Loss = 0.0000
2025-01-24 10:03:02,729 - Batch 300: Loss = 0.0000
2025-01-24 10:03:03,361 - Batch 400: Loss = 0.0000
2025-01-24 10:03:04,018 - Batch 500: Loss = 0.0000
2025-01-24 10:03:04,678 - Batch 600: Loss = 0.0000
2025-01-24 10:03:05,337 - Batch 700: Loss = 0.0000
2025-01-24 10:03:05,997 - Batch 800: Loss = 0.0000
2025-01-24 10:03:06,654 - Batch 900: Loss = 0.0000
2025-01-24 10:03:07,307 - Batch 1000: Loss = 0.0000
2025-01-24 10:03:07,959 - Batch 1100: Loss = 0.0000
2025-01-24 10:03:08,611 - Batch 1200: Loss = 0.0000
2025-01-24 10:03:09,264 - Batch 1300: Loss = 0.0000
2025-01-24 10:03:09,916 - Batch 1400: Loss = 0.0000
2025-01-24 10:03:10,569 - Batch 1500: Loss = 0.0000
2025-01-24 10:03:12,930 - Train Loss: 0.0000
2025-01-24 10:03:12,931 - Test Loss: 68412.5952
2025-01-24 10:03:12,931 - FID Score: 0.1329
2025-01-24 10:03:12,931 - Commitment Loss: 0.0000
2025-01-24 10:03:12,931 - Codebook Loss: 0.0000
2025-01-24 10:03:12,931 - Entropy Reg: 0.0000
2025-01-24 10:03:12,931 - L2 Reg: 0.0001
2025-01-24 10:03:12,950 - 
Epoch 15/50
2025-01-24 10:03:13,086 - Batch 0: Loss = 0.0000
2025-01-24 10:03:13,659 - Batch 100: Loss = 0.0000
2025-01-24 10:03:14,196 - Batch 200: Loss = 0.0000
2025-01-24 10:03:14,777 - Batch 300: Loss = 0.0000
2025-01-24 10:03:15,381 - Batch 400: Loss = 0.0000
2025-01-24 10:03:16,051 - Batch 500: Loss = 0.0000
2025-01-24 10:03:16,906 - Batch 600: Loss = 0.0000
2025-01-24 10:03:17,890 - Batch 700: Loss = 0.0000
2025-01-24 10:03:18,880 - Batch 800: Loss = 0.0000
2025-01-24 10:03:19,872 - Batch 900: Loss = 0.0000
2025-01-24 10:03:20,862 - Batch 1000: Loss = 0.0000
2025-01-24 10:03:21,844 - Batch 1100: Loss = 0.0000
2025-01-24 10:03:22,807 - Batch 1200: Loss = 0.0000
2025-01-24 10:03:23,772 - Batch 1300: Loss = 0.0000
2025-01-24 10:03:24,694 - Batch 1400: Loss = 0.0000
2025-01-24 10:03:25,661 - Batch 1500: Loss = 0.0000
2025-01-24 10:03:28,363 - Train Loss: 0.0000
2025-01-24 10:03:28,364 - Test Loss: 68412.5952
2025-01-24 10:03:28,364 - FID Score: 0.1330
2025-01-24 10:03:28,364 - Commitment Loss: 0.0000
2025-01-24 10:03:28,364 - Codebook Loss: 0.0000
2025-01-24 10:03:28,364 - Entropy Reg: 0.0000
2025-01-24 10:03:28,364 - L2 Reg: 0.0001
2025-01-24 10:03:28,387 - 
Epoch 16/50
2025-01-24 10:03:28,593 - Batch 0: Loss = 0.0000
2025-01-24 10:03:29,405 - Batch 100: Loss = 0.0000
2025-01-24 10:03:30,207 - Batch 200: Loss = 0.0000
2025-01-24 10:03:31,007 - Batch 300: Loss = 0.0000
2025-01-24 10:03:31,844 - Batch 400: Loss = 0.0000
2025-01-24 10:03:32,803 - Batch 500: Loss = 0.0000
2025-01-24 10:03:33,766 - Batch 600: Loss = 0.0000
2025-01-24 10:03:34,743 - Batch 700: Loss = 0.0000
2025-01-24 10:03:35,734 - Batch 800: Loss = 0.0000
2025-01-24 10:03:36,724 - Batch 900: Loss = 0.0000
2025-01-24 10:03:37,645 - Batch 1000: Loss = 0.0000
2025-01-24 10:03:38,528 - Batch 1100: Loss = 0.0000
2025-01-24 10:03:39,397 - Batch 1200: Loss = 0.0000
2025-01-24 10:03:40,312 - Batch 1300: Loss = 0.0000
2025-01-24 10:03:41,290 - Batch 1400: Loss = 0.0000
2025-01-24 10:03:42,264 - Batch 1500: Loss = 0.0000
2025-01-24 10:03:44,763 - Train Loss: 0.0000
2025-01-24 10:03:44,763 - Test Loss: 68412.5952
2025-01-24 10:03:44,763 - FID Score: 0.1330
2025-01-24 10:03:44,763 - Commitment Loss: 0.0000
2025-01-24 10:03:44,763 - Codebook Loss: 0.0000
2025-01-24 10:03:44,763 - Entropy Reg: 0.0000
2025-01-24 10:03:44,763 - L2 Reg: 0.0001
2025-01-24 10:03:44,786 - 
Epoch 17/50
2025-01-24 10:03:44,982 - Batch 0: Loss = 0.0000
2025-01-24 10:03:45,702 - Batch 100: Loss = 0.0000
2025-01-24 10:03:46,520 - Batch 200: Loss = 0.0000
2025-01-24 10:03:47,372 - Batch 300: Loss = 0.0000
2025-01-24 10:03:48,226 - Batch 400: Loss = 0.0000
2025-01-24 10:03:49,089 - Batch 500: Loss = 0.0000
2025-01-24 10:03:49,952 - Batch 600: Loss = 0.0000
2025-01-24 10:03:50,816 - Batch 700: Loss = 0.0000
2025-01-24 10:03:51,681 - Batch 800: Loss = 0.0000
2025-01-24 10:03:52,537 - Batch 900: Loss = 0.0000
2025-01-24 10:03:53,365 - Batch 1000: Loss = 0.0000
2025-01-24 10:03:54,205 - Batch 1100: Loss = 0.0000
2025-01-24 10:03:55,049 - Batch 1200: Loss = 0.0000
2025-01-24 10:03:55,905 - Batch 1300: Loss = 0.0000
2025-01-24 10:03:56,764 - Batch 1400: Loss = 0.0000
2025-01-24 10:03:57,629 - Batch 1500: Loss = 0.0000
2025-01-24 10:04:00,220 - Train Loss: 0.0000
2025-01-24 10:04:00,220 - Test Loss: 68412.5952
2025-01-24 10:04:00,220 - FID Score: 0.1331
2025-01-24 10:04:00,220 - Commitment Loss: 0.0000
2025-01-24 10:04:00,220 - Codebook Loss: 0.0000
2025-01-24 10:04:00,220 - Entropy Reg: 0.0000
2025-01-24 10:04:00,220 - L2 Reg: 0.0001
2025-01-24 10:04:00,243 - 
Epoch 18/50
2025-01-24 10:04:00,437 - Batch 0: Loss = 0.0000
2025-01-24 10:04:01,257 - Batch 100: Loss = 0.0000
2025-01-24 10:04:02,077 - Batch 200: Loss = 0.0000
2025-01-24 10:04:02,821 - Batch 300: Loss = 0.0000
2025-01-24 10:04:03,490 - Batch 400: Loss = 0.0000
2025-01-24 10:04:04,156 - Batch 500: Loss = 0.0000
2025-01-24 10:04:04,778 - Batch 600: Loss = 0.0000
2025-01-24 10:04:05,270 - Batch 700: Loss = 0.0000
2025-01-24 10:04:05,764 - Batch 800: Loss = 0.0000
2025-01-24 10:04:06,258 - Batch 900: Loss = 0.0000
2025-01-24 10:04:06,753 - Batch 1000: Loss = 0.0000
2025-01-24 10:04:07,245 - Batch 1100: Loss = 0.0000
2025-01-24 10:04:07,738 - Batch 1200: Loss = 0.0000
2025-01-24 10:04:08,233 - Batch 1300: Loss = 0.0000
2025-01-24 10:04:08,727 - Batch 1400: Loss = 0.0000
2025-01-24 10:04:09,150 - Batch 1500: Loss = 0.0000
2025-01-24 10:04:11,344 - Train Loss: 0.0000
2025-01-24 10:04:11,345 - Test Loss: 68412.5874
2025-01-24 10:04:11,345 - FID Score: 0.1330
2025-01-24 10:04:11,345 - Commitment Loss: 0.0000
2025-01-24 10:04:11,345 - Codebook Loss: 0.0000
2025-01-24 10:04:11,345 - Entropy Reg: 0.0000
2025-01-24 10:04:11,345 - L2 Reg: 0.0001
2025-01-24 10:04:11,368 - 
Epoch 19/50
2025-01-24 10:04:11,560 - Batch 0: Loss = 0.0000
2025-01-24 10:04:12,163 - Batch 100: Loss = 0.0000
2025-01-24 10:04:12,767 - Batch 200: Loss = 0.0000
2025-01-24 10:04:13,398 - Batch 300: Loss = 0.0000
2025-01-24 10:04:14,043 - Batch 400: Loss = 0.0000
2025-01-24 10:04:14,777 - Batch 500: Loss = 0.0000
2025-01-24 10:04:15,701 - Batch 600: Loss = 0.0000
2025-01-24 10:04:16,683 - Batch 700: Loss = 0.0000
2025-01-24 10:04:17,672 - Batch 800: Loss = 0.0000
2025-01-24 10:04:18,680 - Batch 900: Loss = 0.0000
2025-01-24 10:04:19,684 - Batch 1000: Loss = 0.0000
2025-01-24 10:04:20,691 - Batch 1100: Loss = 0.0000
2025-01-24 10:04:21,701 - Batch 1200: Loss = 0.0000
2025-01-24 10:04:22,712 - Batch 1300: Loss = 0.0000
2025-01-24 10:04:23,726 - Batch 1400: Loss = 0.0000
2025-01-24 10:04:24,737 - Batch 1500: Loss = 0.0000
2025-01-24 10:04:27,390 - Train Loss: 0.0000
2025-01-24 10:04:27,391 - Test Loss: 68412.5874
2025-01-24 10:04:27,391 - FID Score: 0.1331
2025-01-24 10:04:27,391 - Commitment Loss: 0.0000
2025-01-24 10:04:27,391 - Codebook Loss: 0.0000
2025-01-24 10:04:27,391 - Entropy Reg: 0.0000
2025-01-24 10:04:27,391 - L2 Reg: 0.0001
2025-01-24 10:04:27,414 - 
Epoch 20/50
2025-01-24 10:04:27,620 - Batch 0: Loss = 0.0000
2025-01-24 10:04:28,329 - Batch 100: Loss = 0.0000
2025-01-24 10:04:29,019 - Batch 200: Loss = 0.0000
2025-01-24 10:04:29,711 - Batch 300: Loss = 0.0000
2025-01-24 10:04:30,403 - Batch 400: Loss = 0.0000
2025-01-24 10:04:31,094 - Batch 500: Loss = 0.0000
2025-01-24 10:04:31,786 - Batch 600: Loss = 0.0000
2025-01-24 10:04:32,477 - Batch 700: Loss = 0.0000
2025-01-24 10:04:33,088 - Batch 800: Loss = 0.0000
2025-01-24 10:04:33,492 - Batch 900: Loss = 0.0000
2025-01-24 10:04:33,892 - Batch 1000: Loss = 0.0000
2025-01-24 10:04:34,291 - Batch 1100: Loss = 0.0000
2025-01-24 10:04:34,697 - Batch 1200: Loss = 0.0000
2025-01-24 10:04:35,278 - Batch 1300: Loss = 0.0000
2025-01-24 10:04:35,890 - Batch 1400: Loss = 0.0000
2025-01-24 10:04:36,504 - Batch 1500: Loss = 0.0000
2025-01-24 10:04:38,817 - Train Loss: 0.0000
2025-01-24 10:04:38,817 - Test Loss: 68412.5874
2025-01-24 10:04:38,818 - FID Score: 0.1329
2025-01-24 10:04:38,818 - Commitment Loss: 0.0000
2025-01-24 10:04:38,818 - Codebook Loss: 0.0000
2025-01-24 10:04:38,818 - Entropy Reg: 0.0000
2025-01-24 10:04:38,818 - L2 Reg: 0.0001
2025-01-24 10:04:38,837 - 
Epoch 21/50
2025-01-24 10:04:38,967 - Batch 0: Loss = 0.0000
2025-01-24 10:04:39,424 - Batch 100: Loss = 0.0000
2025-01-24 10:04:39,872 - Batch 200: Loss = 0.0000
2025-01-24 10:04:40,418 - Batch 300: Loss = 0.0000
2025-01-24 10:04:40,962 - Batch 400: Loss = 0.0000
2025-01-24 10:04:41,632 - Batch 500: Loss = 0.0000
2025-01-24 10:04:42,419 - Batch 600: Loss = 0.0000
2025-01-24 10:04:43,369 - Batch 700: Loss = 0.0000
2025-01-24 10:04:44,325 - Batch 800: Loss = 0.0000
2025-01-24 10:04:45,307 - Batch 900: Loss = 0.0000
2025-01-24 10:04:46,292 - Batch 1000: Loss = 0.0000
2025-01-24 10:04:47,282 - Batch 1100: Loss = 0.0000
2025-01-24 10:04:48,270 - Batch 1200: Loss = 0.0000
2025-01-24 10:04:49,268 - Batch 1300: Loss = 0.0000
2025-01-24 10:04:50,273 - Batch 1400: Loss = 0.0000
2025-01-24 10:04:51,280 - Batch 1500: Loss = 0.0000
2025-01-24 10:04:53,971 - Train Loss: 0.0000
2025-01-24 10:04:53,971 - Test Loss: 68412.5874
2025-01-24 10:04:53,972 - FID Score: 0.1331
2025-01-24 10:04:53,972 - Commitment Loss: 0.0000
2025-01-24 10:04:53,972 - Codebook Loss: 0.0000
2025-01-24 10:04:53,972 - Entropy Reg: 0.0000
2025-01-24 10:04:53,972 - L2 Reg: 0.0001
2025-01-24 10:04:53,979 - Saved checkpoint at epoch 20
2025-01-24 10:04:54,002 - 
Epoch 22/50
2025-01-24 10:04:54,206 - Batch 0: Loss = 0.0000
2025-01-24 10:04:54,981 - Batch 100: Loss = 0.0000
2025-01-24 10:04:55,909 - Batch 200: Loss = 0.0000
2025-01-24 10:04:56,891 - Batch 300: Loss = 0.0000
2025-01-24 10:04:57,872 - Batch 400: Loss = 0.0000
2025-01-24 10:04:58,848 - Batch 500: Loss = 0.0000
2025-01-24 10:04:59,582 - Batch 600: Loss = 0.0000
2025-01-24 10:05:00,367 - Batch 700: Loss = 0.0000
2025-01-24 10:05:01,167 - Batch 800: Loss = 0.0000
2025-01-24 10:05:01,965 - Batch 900: Loss = 0.0000
2025-01-24 10:05:02,858 - Batch 1000: Loss = 0.0000
2025-01-24 10:05:03,840 - Batch 1100: Loss = 0.0000
2025-01-24 10:05:04,708 - Batch 1200: Loss = 0.0000
2025-01-24 10:05:05,546 - Batch 1300: Loss = 0.0000
2025-01-24 10:05:06,383 - Batch 1400: Loss = 0.0000
2025-01-24 10:05:07,222 - Batch 1500: Loss = 0.0000
2025-01-24 10:05:09,789 - Train Loss: 0.0000
2025-01-24 10:05:09,790 - Test Loss: 68412.5874
2025-01-24 10:05:09,790 - FID Score: 0.1329
2025-01-24 10:05:09,790 - Commitment Loss: 0.0000
2025-01-24 10:05:09,790 - Codebook Loss: 0.0000
2025-01-24 10:05:09,790 - Entropy Reg: 0.0000
2025-01-24 10:05:09,790 - L2 Reg: 0.0001
2025-01-24 10:05:09,813 - 
Epoch 23/50
2025-01-24 10:05:10,013 - Batch 0: Loss = 0.0000
2025-01-24 10:05:10,661 - Batch 100: Loss = 0.0000
2025-01-24 10:05:11,293 - Batch 200: Loss = 0.0000
2025-01-24 10:05:12,055 - Batch 300: Loss = 0.0000
2025-01-24 10:05:12,912 - Batch 400: Loss = 0.0000
2025-01-24 10:05:13,729 - Batch 500: Loss = 0.0000
2025-01-24 10:05:14,644 - Batch 600: Loss = 0.0000
2025-01-24 10:05:15,538 - Batch 700: Loss = 0.0000
2025-01-24 10:05:16,433 - Batch 800: Loss = 0.0000
2025-01-24 10:05:17,354 - Batch 900: Loss = 0.0000
2025-01-24 10:05:18,272 - Batch 1000: Loss = 0.0000
2025-01-24 10:05:19,219 - Batch 1100: Loss = 0.0000
2025-01-24 10:05:20,188 - Batch 1200: Loss = 0.0000
2025-01-24 10:05:21,168 - Batch 1300: Loss = 0.0000
2025-01-24 10:05:22,129 - Batch 1400: Loss = 0.0000
2025-01-24 10:05:23,093 - Batch 1500: Loss = 0.0000
2025-01-24 10:05:25,764 - Train Loss: 0.0000
2025-01-24 10:05:25,765 - Test Loss: 68412.5874
2025-01-24 10:05:25,765 - FID Score: 0.1331
2025-01-24 10:05:25,765 - Commitment Loss: 0.0000
2025-01-24 10:05:25,765 - Codebook Loss: 0.0000
2025-01-24 10:05:25,765 - Entropy Reg: 0.0000
2025-01-24 10:05:25,765 - L2 Reg: 0.0001
2025-01-24 10:05:25,788 - 
Epoch 24/50
2025-01-24 10:05:25,990 - Batch 0: Loss = 0.0000
2025-01-24 10:05:26,674 - Batch 100: Loss = 0.0000
2025-01-24 10:05:27,339 - Batch 200: Loss = 0.0000
2025-01-24 10:05:28,121 - Batch 300: Loss = 0.0000
2025-01-24 10:05:28,942 - Batch 400: Loss = 0.0000
2025-01-24 10:05:29,753 - Batch 500: Loss = 0.0000
2025-01-24 10:05:30,424 - Batch 600: Loss = 0.0000
2025-01-24 10:05:31,084 - Batch 700: Loss = 0.0000
2025-01-24 10:05:31,598 - Batch 800: Loss = 0.0000
2025-01-24 10:05:32,109 - Batch 900: Loss = 0.0000
2025-01-24 10:05:32,621 - Batch 1000: Loss = 0.0000
2025-01-24 10:05:33,131 - Batch 1100: Loss = 0.0000
2025-01-24 10:05:33,643 - Batch 1200: Loss = 0.0000
2025-01-24 10:05:34,154 - Batch 1300: Loss = 0.0000
2025-01-24 10:05:34,665 - Batch 1400: Loss = 0.0000
2025-01-24 10:05:35,185 - Batch 1500: Loss = 0.0000
2025-01-24 10:05:37,796 - Train Loss: 0.0000
2025-01-24 10:05:37,796 - Test Loss: 68412.5874
2025-01-24 10:05:37,796 - FID Score: 0.1331
2025-01-24 10:05:37,796 - Commitment Loss: 0.0000
2025-01-24 10:05:37,796 - Codebook Loss: 0.0000
2025-01-24 10:05:37,796 - Entropy Reg: 0.0000
2025-01-24 10:05:37,796 - L2 Reg: 0.0001
2025-01-24 10:05:37,819 - 
Epoch 25/50
2025-01-24 10:05:38,022 - Batch 0: Loss = 0.0000
2025-01-24 10:05:38,871 - Batch 100: Loss = 0.0000
2025-01-24 10:05:39,770 - Batch 200: Loss = 0.0000
2025-01-24 10:05:40,789 - Batch 300: Loss = 0.0000
2025-01-24 10:05:41,806 - Batch 400: Loss = 0.0000
2025-01-24 10:05:42,819 - Batch 500: Loss = 0.0000
2025-01-24 10:05:43,834 - Batch 600: Loss = 0.0000
2025-01-24 10:05:44,858 - Batch 700: Loss = 0.0000
2025-01-24 10:05:45,881 - Batch 800: Loss = 0.0000
2025-01-24 10:05:46,901 - Batch 900: Loss = 0.0000
2025-01-24 10:05:47,922 - Batch 1000: Loss = 0.0000
2025-01-24 10:05:48,903 - Batch 1100: Loss = 0.0000
2025-01-24 10:05:49,923 - Batch 1200: Loss = 0.0000
2025-01-24 10:05:50,953 - Batch 1300: Loss = 0.0000
2025-01-24 10:05:51,975 - Batch 1400: Loss = 0.0000
2025-01-24 10:05:52,999 - Batch 1500: Loss = 0.0000
2025-01-24 10:05:55,737 - Train Loss: 0.0000
2025-01-24 10:05:55,738 - Test Loss: 68412.5874
2025-01-24 10:05:55,738 - FID Score: 0.1331
2025-01-24 10:05:55,738 - Commitment Loss: 0.0000
2025-01-24 10:05:55,738 - Codebook Loss: 0.0000
2025-01-24 10:05:55,738 - Entropy Reg: 0.0000
2025-01-24 10:05:55,738 - L2 Reg: 0.0001
2025-01-24 10:05:55,761 - 
Epoch 26/50
2025-01-24 10:05:55,973 - Batch 0: Loss = 0.0000
2025-01-24 10:05:56,799 - Batch 100: Loss = 0.0000
2025-01-24 10:05:57,639 - Batch 200: Loss = 0.0000
2025-01-24 10:05:58,482 - Batch 300: Loss = 0.0000
2025-01-24 10:05:59,325 - Batch 400: Loss = 0.0000
2025-01-24 10:06:00,168 - Batch 500: Loss = 0.0000
2025-01-24 10:06:01,011 - Batch 600: Loss = 0.0000
2025-01-24 10:06:01,971 - Batch 700: Loss = 0.0000
2025-01-24 10:06:02,968 - Batch 800: Loss = 0.0000
2025-01-24 10:06:03,962 - Batch 900: Loss = 0.0000
2025-01-24 10:06:04,960 - Batch 1000: Loss = 0.0000
2025-01-24 10:06:05,960 - Batch 1100: Loss = 0.0000
2025-01-24 10:06:06,957 - Batch 1200: Loss = 0.0000
2025-01-24 10:06:07,956 - Batch 1300: Loss = 0.0000
2025-01-24 10:06:08,993 - Batch 1400: Loss = 0.0000
2025-01-24 10:06:10,018 - Batch 1500: Loss = 0.0000
2025-01-24 10:06:12,704 - Train Loss: 0.0000
2025-01-24 10:06:12,704 - Test Loss: 68412.5874
2025-01-24 10:06:12,705 - FID Score: 0.1329
2025-01-24 10:06:12,705 - Commitment Loss: 0.0000
2025-01-24 10:06:12,705 - Codebook Loss: 0.0000
2025-01-24 10:06:12,705 - Entropy Reg: 0.0000
2025-01-24 10:06:12,705 - L2 Reg: 0.0001
2025-01-24 10:06:12,728 - 
Epoch 27/50
2025-01-24 10:06:12,925 - Batch 0: Loss = 0.0000
2025-01-24 10:06:13,789 - Batch 100: Loss = 0.0000
2025-01-24 10:06:14,699 - Batch 200: Loss = 0.0000
2025-01-24 10:06:15,629 - Batch 300: Loss = 0.0000
2025-01-24 10:06:16,553 - Batch 400: Loss = 0.0000
2025-01-24 10:06:17,477 - Batch 500: Loss = 0.0000
2025-01-24 10:06:18,401 - Batch 600: Loss = 0.0000
2025-01-24 10:06:19,328 - Batch 700: Loss = 0.0000
2025-01-24 10:06:20,255 - Batch 800: Loss = 0.0000
2025-01-24 10:06:21,173 - Batch 900: Loss = 0.0000
2025-01-24 10:06:22,081 - Batch 1000: Loss = 0.0000
2025-01-24 10:06:22,980 - Batch 1100: Loss = 0.0000
2025-01-24 10:06:23,782 - Batch 1200: Loss = 0.0000
2025-01-24 10:06:24,453 - Batch 1300: Loss = 0.0000
2025-01-24 10:06:25,202 - Batch 1400: Loss = 0.0000
2025-01-24 10:06:26,010 - Batch 1500: Loss = 0.0000
2025-01-24 10:06:28,536 - Train Loss: 0.0000
2025-01-24 10:06:28,536 - Test Loss: 68412.5874
2025-01-24 10:06:28,536 - FID Score: 0.1331
2025-01-24 10:06:28,536 - Commitment Loss: 0.0000
2025-01-24 10:06:28,536 - Codebook Loss: 0.0000
2025-01-24 10:06:28,536 - Entropy Reg: 0.0000
2025-01-24 10:06:28,536 - L2 Reg: 0.0001
2025-01-24 10:06:28,559 - 
Epoch 28/50
2025-01-24 10:06:28,760 - Batch 0: Loss = 0.0000
2025-01-24 10:06:29,457 - Batch 100: Loss = 0.0000
2025-01-24 10:06:30,165 - Batch 200: Loss = 0.0000
2025-01-24 10:06:30,885 - Batch 300: Loss = 0.0000
2025-01-24 10:06:31,558 - Batch 400: Loss = 0.0000
2025-01-24 10:06:32,225 - Batch 500: Loss = 0.0000
2025-01-24 10:06:32,854 - Batch 600: Loss = 0.0000
2025-01-24 10:06:33,484 - Batch 700: Loss = 0.0000
2025-01-24 10:06:34,126 - Batch 800: Loss = 0.0000
2025-01-24 10:06:34,767 - Batch 900: Loss = 0.0000
2025-01-24 10:06:35,409 - Batch 1000: Loss = 0.0000
2025-01-24 10:06:36,048 - Batch 1100: Loss = 0.0000
2025-01-24 10:06:36,634 - Batch 1200: Loss = 0.0000
2025-01-24 10:06:37,210 - Batch 1300: Loss = 0.0000
2025-01-24 10:06:37,787 - Batch 1400: Loss = 0.0000
2025-01-24 10:06:38,363 - Batch 1500: Loss = 0.0000
2025-01-24 10:06:40,615 - Train Loss: 0.0000
2025-01-24 10:06:40,615 - Test Loss: 68412.5874
2025-01-24 10:06:40,615 - FID Score: 0.1329
2025-01-24 10:06:40,615 - Commitment Loss: 0.0000
2025-01-24 10:06:40,615 - Codebook Loss: 0.0000
2025-01-24 10:06:40,615 - Entropy Reg: 0.0000
2025-01-24 10:06:40,615 - L2 Reg: 0.0001
2025-01-24 10:06:40,634 - 
Epoch 29/50
2025-01-24 10:06:40,766 - Batch 0: Loss = 0.0000
2025-01-24 10:06:41,404 - Batch 100: Loss = 0.0000
2025-01-24 10:06:42,196 - Batch 200: Loss = 0.0000
2025-01-24 10:06:43,189 - Batch 300: Loss = 0.0000
2025-01-24 10:06:44,194 - Batch 400: Loss = 0.0000
2025-01-24 10:06:45,200 - Batch 500: Loss = 0.0000
2025-01-24 10:06:46,205 - Batch 600: Loss = 0.0000
2025-01-24 10:06:47,055 - Batch 700: Loss = 0.0000
2025-01-24 10:06:47,850 - Batch 800: Loss = 0.0000
2025-01-24 10:06:48,656 - Batch 900: Loss = 0.0000
2025-01-24 10:06:49,450 - Batch 1000: Loss = 0.0000
2025-01-24 10:06:50,246 - Batch 1100: Loss = 0.0000
2025-01-24 10:06:51,039 - Batch 1200: Loss = 0.0000
2025-01-24 10:06:51,828 - Batch 1300: Loss = 0.0000
2025-01-24 10:06:52,610 - Batch 1400: Loss = 0.0000
2025-01-24 10:06:53,525 - Batch 1500: Loss = 0.0000
2025-01-24 10:06:56,156 - Train Loss: 0.0000
2025-01-24 10:06:56,157 - Test Loss: 68412.5874
2025-01-24 10:06:56,157 - FID Score: 0.1330
2025-01-24 10:06:56,157 - Commitment Loss: 0.0000
2025-01-24 10:06:56,157 - Codebook Loss: 0.0000
2025-01-24 10:06:56,157 - Entropy Reg: 0.0000
2025-01-24 10:06:56,157 - L2 Reg: 0.0001
2025-01-24 10:06:56,180 - 
Epoch 30/50
2025-01-24 10:06:56,387 - Batch 0: Loss = 0.0000
2025-01-24 10:06:57,197 - Batch 100: Loss = 0.0000
2025-01-24 10:06:58,122 - Batch 200: Loss = 0.0000
2025-01-24 10:06:59,079 - Batch 300: Loss = 0.0000
2025-01-24 10:07:00,089 - Batch 400: Loss = 0.0000
2025-01-24 10:07:01,082 - Batch 500: Loss = 0.0000
2025-01-24 10:07:02,031 - Batch 600: Loss = 0.0000
2025-01-24 10:07:02,873 - Batch 700: Loss = 0.0000
2025-01-24 10:07:03,716 - Batch 800: Loss = 0.0000
2025-01-24 10:07:04,679 - Batch 900: Loss = 0.0000
2025-01-24 10:07:05,666 - Batch 1000: Loss = 0.0000
2025-01-24 10:07:06,649 - Batch 1100: Loss = 0.0000
2025-01-24 10:07:07,598 - Batch 1200: Loss = 0.0000
2025-01-24 10:07:08,577 - Batch 1300: Loss = 0.0000
2025-01-24 10:07:09,564 - Batch 1400: Loss = 0.0000
2025-01-24 10:07:10,548 - Batch 1500: Loss = 0.0000
2025-01-24 10:07:13,215 - Train Loss: 0.0000
2025-01-24 10:07:13,215 - Test Loss: 68412.5874
2025-01-24 10:07:13,215 - FID Score: 0.1329
2025-01-24 10:07:13,215 - Commitment Loss: 0.0000
2025-01-24 10:07:13,215 - Codebook Loss: 0.0000
2025-01-24 10:07:13,215 - Entropy Reg: 0.0000
2025-01-24 10:07:13,215 - L2 Reg: 0.0001
2025-01-24 10:07:13,238 - 
Epoch 31/50
2025-01-24 10:07:13,438 - Batch 0: Loss = 0.0000
2025-01-24 10:07:14,216 - Batch 100: Loss = 0.0000
2025-01-24 10:07:15,041 - Batch 200: Loss = 0.0000
2025-01-24 10:07:15,925 - Batch 300: Loss = 0.0000
2025-01-24 10:07:16,897 - Batch 400: Loss = 0.0000
2025-01-24 10:07:17,898 - Batch 500: Loss = 0.0000
2025-01-24 10:07:18,898 - Batch 600: Loss = 0.0000
2025-01-24 10:07:19,925 - Batch 700: Loss = 0.0000
2025-01-24 10:07:20,939 - Batch 800: Loss = 0.0000
2025-01-24 10:07:21,903 - Batch 900: Loss = 0.0000
2025-01-24 10:07:22,853 - Batch 1000: Loss = 0.0000
2025-01-24 10:07:23,808 - Batch 1100: Loss = 0.0000
2025-01-24 10:07:24,765 - Batch 1200: Loss = 0.0000
2025-01-24 10:07:25,726 - Batch 1300: Loss = 0.0000
2025-01-24 10:07:26,681 - Batch 1400: Loss = 0.0000
2025-01-24 10:07:27,593 - Batch 1500: Loss = 0.0000
2025-01-24 10:07:30,164 - Train Loss: 0.0000
2025-01-24 10:07:30,164 - Test Loss: 68412.5874
2025-01-24 10:07:30,164 - FID Score: 0.1330
2025-01-24 10:07:30,164 - Commitment Loss: 0.0000
2025-01-24 10:07:30,164 - Codebook Loss: 0.0000
2025-01-24 10:07:30,164 - Entropy Reg: 0.0000
2025-01-24 10:07:30,164 - L2 Reg: 0.0001
2025-01-24 10:07:30,172 - Saved checkpoint at epoch 30
2025-01-24 10:07:30,195 - 
Epoch 32/50
2025-01-24 10:07:30,395 - Batch 0: Loss = 0.0000
2025-01-24 10:07:31,353 - Batch 100: Loss = 0.0000
2025-01-24 10:07:32,364 - Batch 200: Loss = 0.0000
2025-01-24 10:07:33,323 - Batch 300: Loss = 0.0000
2025-01-24 10:07:34,330 - Batch 400: Loss = 0.0000
2025-01-24 10:07:35,352 - Batch 500: Loss = 0.0000
2025-01-24 10:07:36,373 - Batch 600: Loss = 0.0000
2025-01-24 10:07:37,403 - Batch 700: Loss = 0.0000
2025-01-24 10:07:38,433 - Batch 800: Loss = 0.0000
2025-01-24 10:07:39,464 - Batch 900: Loss = 0.0000
2025-01-24 10:07:40,498 - Batch 1000: Loss = 0.0000
2025-01-24 10:07:41,564 - Batch 1100: Loss = 0.0000
2025-01-24 10:07:42,607 - Batch 1200: Loss = 0.0000
2025-01-24 10:07:43,611 - Batch 1300: Loss = 0.0000
2025-01-24 10:07:44,655 - Batch 1400: Loss = 0.0000
2025-01-24 10:07:45,728 - Batch 1500: Loss = 0.0000
2025-01-24 10:07:48,441 - Train Loss: 0.0000
2025-01-24 10:07:48,442 - Test Loss: 68412.5874
2025-01-24 10:07:48,442 - FID Score: 0.1329
2025-01-24 10:07:48,442 - Commitment Loss: 0.0000
2025-01-24 10:07:48,442 - Codebook Loss: 0.0000
2025-01-24 10:07:48,442 - Entropy Reg: 0.0000
2025-01-24 10:07:48,442 - L2 Reg: 0.0001
2025-01-24 10:07:48,465 - 
Epoch 33/50
2025-01-24 10:07:48,666 - Batch 0: Loss = 0.0000
2025-01-24 10:07:49,462 - Batch 100: Loss = 0.0000
2025-01-24 10:07:50,302 - Batch 200: Loss = 0.0000
2025-01-24 10:07:51,268 - Batch 300: Loss = 0.0000
2025-01-24 10:07:52,167 - Batch 400: Loss = 0.0000
2025-01-24 10:07:53,055 - Batch 500: Loss = 0.0000
2025-01-24 10:07:53,896 - Batch 600: Loss = 0.0000
2025-01-24 10:07:54,749 - Batch 700: Loss = 0.0000
2025-01-24 10:07:55,601 - Batch 800: Loss = 0.0000
2025-01-24 10:07:56,453 - Batch 900: Loss = 0.0000
2025-01-24 10:07:57,305 - Batch 1000: Loss = 0.0000
2025-01-24 10:07:58,156 - Batch 1100: Loss = 0.0000
2025-01-24 10:07:59,006 - Batch 1200: Loss = 0.0000
2025-01-24 10:07:59,856 - Batch 1300: Loss = 0.0000
2025-01-24 10:08:00,598 - Batch 1400: Loss = 0.0000
2025-01-24 10:08:01,255 - Batch 1500: Loss = 0.0000
2025-01-24 10:08:03,577 - Train Loss: 0.0000
2025-01-24 10:08:03,577 - Test Loss: 68412.5874
2025-01-24 10:08:03,577 - FID Score: 0.1331
2025-01-24 10:08:03,577 - Commitment Loss: 0.0000
2025-01-24 10:08:03,577 - Codebook Loss: 0.0000
2025-01-24 10:08:03,577 - Entropy Reg: 0.0000
2025-01-24 10:08:03,577 - L2 Reg: 0.0001
2025-01-24 10:08:03,596 - 
Epoch 34/50
2025-01-24 10:08:03,726 - Batch 0: Loss = 0.0000
2025-01-24 10:08:04,578 - Batch 100: Loss = 0.0000
2025-01-24 10:08:05,563 - Batch 200: Loss = 0.0000
2025-01-24 10:08:06,557 - Batch 300: Loss = 0.0000
2025-01-24 10:08:07,532 - Batch 400: Loss = 0.0000
2025-01-24 10:08:08,544 - Batch 500: Loss = 0.0000
2025-01-24 10:08:09,516 - Batch 600: Loss = 0.0000
2025-01-24 10:08:10,464 - Batch 700: Loss = 0.0000
2025-01-24 10:08:11,456 - Batch 800: Loss = 0.0000
2025-01-24 10:08:12,460 - Batch 900: Loss = 0.0000
2025-01-24 10:08:13,479 - Batch 1000: Loss = 0.0000
2025-01-24 10:08:14,456 - Batch 1100: Loss = 0.0000
2025-01-24 10:08:15,474 - Batch 1200: Loss = 0.0000
2025-01-24 10:08:16,496 - Batch 1300: Loss = 0.0000
2025-01-24 10:08:17,525 - Batch 1400: Loss = 0.0000
2025-01-24 10:08:18,547 - Batch 1500: Loss = 0.0000
2025-01-24 10:08:21,246 - Train Loss: 0.0000
2025-01-24 10:08:21,246 - Test Loss: 68412.5874
2025-01-24 10:08:21,246 - FID Score: 0.1329
2025-01-24 10:08:21,246 - Commitment Loss: 0.0000
2025-01-24 10:08:21,246 - Codebook Loss: 0.0000
2025-01-24 10:08:21,247 - Entropy Reg: 0.0000
2025-01-24 10:08:21,247 - L2 Reg: 0.0001
2025-01-24 10:08:21,270 - 
Epoch 35/50
2025-01-24 10:08:21,463 - Batch 0: Loss = 0.0000
2025-01-24 10:08:22,345 - Batch 100: Loss = 0.0000
2025-01-24 10:08:23,283 - Batch 200: Loss = 0.0000
2025-01-24 10:08:24,202 - Batch 300: Loss = 0.0000
2025-01-24 10:08:25,149 - Batch 400: Loss = 0.0000
2025-01-24 10:08:26,106 - Batch 500: Loss = 0.0000
2025-01-24 10:08:27,097 - Batch 600: Loss = 0.0000
2025-01-24 10:08:28,087 - Batch 700: Loss = 0.0000
2025-01-24 10:08:29,076 - Batch 800: Loss = 0.0000
2025-01-24 10:08:30,065 - Batch 900: Loss = 0.0000
2025-01-24 10:08:31,060 - Batch 1000: Loss = 0.0000
2025-01-24 10:08:32,053 - Batch 1100: Loss = 0.0000
2025-01-24 10:08:33,049 - Batch 1200: Loss = 0.0000
2025-01-24 10:08:34,037 - Batch 1300: Loss = 0.0000
2025-01-24 10:08:34,893 - Batch 1400: Loss = 0.0000
2025-01-24 10:08:35,734 - Batch 1500: Loss = 0.0000
2025-01-24 10:08:38,325 - Train Loss: 0.0000
2025-01-24 10:08:38,325 - Test Loss: 68412.5874
2025-01-24 10:08:38,325 - FID Score: 0.1331
2025-01-24 10:08:38,325 - Commitment Loss: 0.0000
2025-01-24 10:08:38,325 - Codebook Loss: 0.0000
2025-01-24 10:08:38,325 - Entropy Reg: 0.0000
2025-01-24 10:08:38,325 - L2 Reg: 0.0001
2025-01-24 10:08:38,348 - 
Epoch 36/50
2025-01-24 10:08:38,553 - Batch 0: Loss = 0.0000
2025-01-24 10:08:39,473 - Batch 100: Loss = 0.0000
2025-01-24 10:08:40,446 - Batch 200: Loss = 0.0000
2025-01-24 10:08:41,440 - Batch 300: Loss = 0.0000
2025-01-24 10:08:42,431 - Batch 400: Loss = 0.0000
2025-01-24 10:08:43,428 - Batch 500: Loss = 0.0000
2025-01-24 10:08:44,387 - Batch 600: Loss = 0.0000
2025-01-24 10:08:45,182 - Batch 700: Loss = 0.0000
2025-01-24 10:08:46,010 - Batch 800: Loss = 0.0000
2025-01-24 10:08:46,886 - Batch 900: Loss = 0.0000
2025-01-24 10:08:47,819 - Batch 1000: Loss = 0.0000
2025-01-24 10:08:48,784 - Batch 1100: Loss = 0.0000
2025-01-24 10:08:49,771 - Batch 1200: Loss = 0.0000
2025-01-24 10:08:50,759 - Batch 1300: Loss = 0.0000
2025-01-24 10:08:51,746 - Batch 1400: Loss = 0.0000
2025-01-24 10:08:52,730 - Batch 1500: Loss = 0.0000
2025-01-24 10:08:55,376 - Train Loss: 0.0000
2025-01-24 10:08:55,377 - Test Loss: 68412.5874
2025-01-24 10:08:55,377 - FID Score: 0.1330
2025-01-24 10:08:55,377 - Commitment Loss: 0.0000
2025-01-24 10:08:55,377 - Codebook Loss: 0.0000
2025-01-24 10:08:55,377 - Entropy Reg: 0.0000
2025-01-24 10:08:55,377 - L2 Reg: 0.0001
2025-01-24 10:08:55,400 - 
Epoch 37/50
2025-01-24 10:08:55,597 - Batch 0: Loss = 0.0000
2025-01-24 10:08:56,428 - Batch 100: Loss = 0.0000
2025-01-24 10:08:57,250 - Batch 200: Loss = 0.0000
2025-01-24 10:08:58,097 - Batch 300: Loss = 0.0000
2025-01-24 10:08:58,943 - Batch 400: Loss = 0.0000
2025-01-24 10:08:59,788 - Batch 500: Loss = 0.0000
2025-01-24 10:09:00,640 - Batch 600: Loss = 0.0000
2025-01-24 10:09:01,484 - Batch 700: Loss = 0.0000
2025-01-24 10:09:02,330 - Batch 800: Loss = 0.0000
2025-01-24 10:09:03,177 - Batch 900: Loss = 0.0000
2025-01-24 10:09:04,138 - Batch 1000: Loss = 0.0000
2025-01-24 10:09:05,032 - Batch 1100: Loss = 0.0000
2025-01-24 10:09:05,849 - Batch 1200: Loss = 0.0000
2025-01-24 10:09:06,696 - Batch 1300: Loss = 0.0000
2025-01-24 10:09:07,549 - Batch 1400: Loss = 0.0000
2025-01-24 10:09:08,403 - Batch 1500: Loss = 0.0000
2025-01-24 10:09:10,968 - Train Loss: 0.0000
2025-01-24 10:09:10,968 - Test Loss: 68412.5874
2025-01-24 10:09:10,968 - FID Score: 0.1331
2025-01-24 10:09:10,968 - Commitment Loss: 0.0000
2025-01-24 10:09:10,968 - Codebook Loss: 0.0000
2025-01-24 10:09:10,968 - Entropy Reg: 0.0000
2025-01-24 10:09:10,968 - L2 Reg: 0.0001
2025-01-24 10:09:10,992 - 
Epoch 38/50
2025-01-24 10:09:11,190 - Batch 0: Loss = 0.0000
2025-01-24 10:09:11,792 - Batch 100: Loss = 0.0000
2025-01-24 10:09:12,331 - Batch 200: Loss = 0.0000
2025-01-24 10:09:12,869 - Batch 300: Loss = 0.0000
2025-01-24 10:09:13,407 - Batch 400: Loss = 0.0000
2025-01-24 10:09:13,959 - Batch 500: Loss = 0.0000
2025-01-24 10:09:14,559 - Batch 600: Loss = 0.0000
2025-01-24 10:09:15,147 - Batch 700: Loss = 0.0000
2025-01-24 10:09:15,735 - Batch 800: Loss = 0.0000
2025-01-24 10:09:16,372 - Batch 900: Loss = 0.0000
2025-01-24 10:09:17,107 - Batch 1000: Loss = 0.0000
2025-01-24 10:09:18,043 - Batch 1100: Loss = 0.0000
2025-01-24 10:09:19,024 - Batch 1200: Loss = 0.0000
2025-01-24 10:09:20,009 - Batch 1300: Loss = 0.0000
2025-01-24 10:09:21,004 - Batch 1400: Loss = 0.0000
2025-01-24 10:09:22,003 - Batch 1500: Loss = 0.0000
2025-01-24 10:09:24,653 - Train Loss: 0.0000
2025-01-24 10:09:24,653 - Test Loss: 68412.5874
2025-01-24 10:09:24,653 - FID Score: 0.1329
2025-01-24 10:09:24,653 - Commitment Loss: 0.0000
2025-01-24 10:09:24,653 - Codebook Loss: 0.0000
2025-01-24 10:09:24,653 - Entropy Reg: 0.0000
2025-01-24 10:09:24,653 - L2 Reg: 0.0001
2025-01-24 10:09:24,676 - 
Epoch 39/50
2025-01-24 10:09:24,874 - Batch 0: Loss = 0.0000
2025-01-24 10:09:25,686 - Batch 100: Loss = 0.0000
2025-01-24 10:09:26,507 - Batch 200: Loss = 0.0000
2025-01-24 10:09:27,323 - Batch 300: Loss = 0.0000
2025-01-24 10:09:28,135 - Batch 400: Loss = 0.0000
2025-01-24 10:09:28,944 - Batch 500: Loss = 0.0000
2025-01-24 10:09:29,768 - Batch 600: Loss = 0.0000
2025-01-24 10:09:30,587 - Batch 700: Loss = 0.0000
2025-01-24 10:09:31,396 - Batch 800: Loss = 0.0000
2025-01-24 10:09:32,200 - Batch 900: Loss = 0.0000
2025-01-24 10:09:32,988 - Batch 1000: Loss = 0.0000
2025-01-24 10:09:33,645 - Batch 1100: Loss = 0.0000
2025-01-24 10:09:34,282 - Batch 1200: Loss = 0.0000
2025-01-24 10:09:34,918 - Batch 1300: Loss = 0.0000
2025-01-24 10:09:35,552 - Batch 1400: Loss = 0.0000
2025-01-24 10:09:36,187 - Batch 1500: Loss = 0.0000
2025-01-24 10:09:38,582 - Train Loss: 0.0000
2025-01-24 10:09:38,582 - Test Loss: 68412.5874
2025-01-24 10:09:38,582 - FID Score: 0.1330
2025-01-24 10:09:38,582 - Commitment Loss: 0.0000
2025-01-24 10:09:38,582 - Codebook Loss: 0.0000
2025-01-24 10:09:38,582 - Entropy Reg: 0.0000
2025-01-24 10:09:38,582 - L2 Reg: 0.0001
2025-01-24 10:09:38,605 - 
Epoch 40/50
2025-01-24 10:09:38,799 - Batch 0: Loss = 0.0000
2025-01-24 10:09:39,616 - Batch 100: Loss = 0.0000
2025-01-24 10:09:40,425 - Batch 200: Loss = 0.0000
2025-01-24 10:09:41,221 - Batch 300: Loss = 0.0000
2025-01-24 10:09:42,029 - Batch 400: Loss = 0.0000
2025-01-24 10:09:42,843 - Batch 500: Loss = 0.0000
2025-01-24 10:09:43,646 - Batch 600: Loss = 0.0000
2025-01-24 10:09:44,460 - Batch 700: Loss = 0.0000
2025-01-24 10:09:45,297 - Batch 800: Loss = 0.0000
2025-01-24 10:09:45,967 - Batch 900: Loss = 0.0000
2025-01-24 10:09:46,628 - Batch 1000: Loss = 0.0000
2025-01-24 10:09:47,288 - Batch 1100: Loss = 0.0000
2025-01-24 10:09:47,953 - Batch 1200: Loss = 0.0000
2025-01-24 10:09:48,618 - Batch 1300: Loss = 0.0000
2025-01-24 10:09:49,282 - Batch 1400: Loss = 0.0000
2025-01-24 10:09:49,936 - Batch 1500: Loss = 0.0000
2025-01-24 10:09:52,498 - Train Loss: 0.0000
2025-01-24 10:09:52,498 - Test Loss: 68412.5874
2025-01-24 10:09:52,498 - FID Score: 0.1330
2025-01-24 10:09:52,498 - Commitment Loss: 0.0000
2025-01-24 10:09:52,498 - Codebook Loss: 0.0000
2025-01-24 10:09:52,498 - Entropy Reg: 0.0000
2025-01-24 10:09:52,498 - L2 Reg: 0.0001
2025-01-24 10:09:52,522 - 
Epoch 41/50
2025-01-24 10:09:52,718 - Batch 0: Loss = 0.0000
2025-01-24 10:09:53,674 - Batch 100: Loss = 0.0000
2025-01-24 10:09:54,665 - Batch 200: Loss = 0.0000
2025-01-24 10:09:55,598 - Batch 300: Loss = 0.0000
2025-01-24 10:09:56,424 - Batch 400: Loss = 0.0000
2025-01-24 10:09:57,261 - Batch 500: Loss = 0.0000
2025-01-24 10:09:58,098 - Batch 600: Loss = 0.0000
2025-01-24 10:09:58,933 - Batch 700: Loss = 0.0000
2025-01-24 10:09:59,771 - Batch 800: Loss = 0.0000
2025-01-24 10:10:00,608 - Batch 900: Loss = 0.0000
2025-01-24 10:10:01,447 - Batch 1000: Loss = 0.0000
2025-01-24 10:10:02,290 - Batch 1100: Loss = 0.0000
2025-01-24 10:10:03,120 - Batch 1200: Loss = 0.0000
2025-01-24 10:10:03,960 - Batch 1300: Loss = 0.0000
2025-01-24 10:10:04,807 - Batch 1400: Loss = 0.0000
2025-01-24 10:10:05,744 - Batch 1500: Loss = 0.0000
2025-01-24 10:10:08,414 - Train Loss: 0.0000
2025-01-24 10:10:08,415 - Test Loss: 68412.5874
2025-01-24 10:10:08,415 - FID Score: 0.1330
2025-01-24 10:10:08,415 - Commitment Loss: 0.0000
2025-01-24 10:10:08,415 - Codebook Loss: 0.0000
2025-01-24 10:10:08,415 - Entropy Reg: 0.0000
2025-01-24 10:10:08,415 - L2 Reg: 0.0001
2025-01-24 10:10:08,423 - Saved checkpoint at epoch 40
2025-01-24 10:10:08,446 - 
Epoch 42/50
2025-01-24 10:10:08,649 - Batch 0: Loss = 0.0000
2025-01-24 10:10:09,579 - Batch 100: Loss = 0.0000
2025-01-24 10:10:10,574 - Batch 200: Loss = 0.0000
2025-01-24 10:10:11,528 - Batch 300: Loss = 0.0000
2025-01-24 10:10:12,498 - Batch 400: Loss = 0.0000
2025-01-24 10:10:13,467 - Batch 500: Loss = 0.0000
2025-01-24 10:10:14,456 - Batch 600: Loss = 0.0000
2025-01-24 10:10:15,428 - Batch 700: Loss = 0.0000
2025-01-24 10:10:16,396 - Batch 800: Loss = 0.0000
2025-01-24 10:10:17,380 - Batch 900: Loss = 0.0000
2025-01-24 10:10:18,364 - Batch 1000: Loss = 0.0000
2025-01-24 10:10:19,336 - Batch 1100: Loss = 0.0000
2025-01-24 10:10:20,306 - Batch 1200: Loss = 0.0000
2025-01-24 10:10:21,278 - Batch 1300: Loss = 0.0000
2025-01-24 10:10:22,273 - Batch 1400: Loss = 0.0000
2025-01-24 10:10:23,268 - Batch 1500: Loss = 0.0000
2025-01-24 10:10:25,952 - Train Loss: 0.0000
2025-01-24 10:10:25,952 - Test Loss: 68412.5874
2025-01-24 10:10:25,952 - FID Score: 0.1330
2025-01-24 10:10:25,952 - Commitment Loss: 0.0000
2025-01-24 10:10:25,952 - Codebook Loss: 0.0000
2025-01-24 10:10:25,952 - Entropy Reg: 0.0000
2025-01-24 10:10:25,952 - L2 Reg: 0.0001
2025-01-24 10:10:25,975 - 
Epoch 43/50
2025-01-24 10:10:26,181 - Batch 0: Loss = 0.0000
2025-01-24 10:10:27,049 - Batch 100: Loss = 0.0000
2025-01-24 10:10:27,958 - Batch 200: Loss = 0.0000
2025-01-24 10:10:28,819 - Batch 300: Loss = 0.0000
2025-01-24 10:10:29,755 - Batch 400: Loss = 0.0000
2025-01-24 10:10:30,725 - Batch 500: Loss = 0.0000
2025-01-24 10:10:31,663 - Batch 600: Loss = 0.0000
2025-01-24 10:10:32,636 - Batch 700: Loss = 0.0000
2025-01-24 10:10:33,506 - Batch 800: Loss = 0.0000
2025-01-24 10:10:34,361 - Batch 900: Loss = 0.0000
2025-01-24 10:10:35,205 - Batch 1000: Loss = 0.0000
2025-01-24 10:10:36,050 - Batch 1100: Loss = 0.0000
2025-01-24 10:10:36,896 - Batch 1200: Loss = 0.0000
2025-01-24 10:10:37,700 - Batch 1300: Loss = 0.0000
2025-01-24 10:10:38,522 - Batch 1400: Loss = 0.0000
2025-01-24 10:10:39,348 - Batch 1500: Loss = 0.0000
2025-01-24 10:10:41,904 - Train Loss: 0.0000
2025-01-24 10:10:41,905 - Test Loss: 68412.5874
2025-01-24 10:10:41,905 - FID Score: 0.1331
2025-01-24 10:10:41,905 - Commitment Loss: 0.0000
2025-01-24 10:10:41,905 - Codebook Loss: 0.0000
2025-01-24 10:10:41,905 - Entropy Reg: 0.0000
2025-01-24 10:10:41,905 - L2 Reg: 0.0001
2025-01-24 10:10:41,928 - 
Epoch 44/50
2025-01-24 10:10:42,113 - Batch 0: Loss = 0.0000
2025-01-24 10:10:42,931 - Batch 100: Loss = 0.0000
2025-01-24 10:10:43,764 - Batch 200: Loss = 0.0000
2025-01-24 10:10:44,600 - Batch 300: Loss = 0.0000
2025-01-24 10:10:45,434 - Batch 400: Loss = 0.0000
2025-01-24 10:10:46,269 - Batch 500: Loss = 0.0000
2025-01-24 10:10:46,989 - Batch 600: Loss = 0.0000
2025-01-24 10:10:47,660 - Batch 700: Loss = 0.0000
2025-01-24 10:10:48,369 - Batch 800: Loss = 0.0000
2025-01-24 10:10:49,339 - Batch 900: Loss = 0.0000
2025-01-24 10:10:50,329 - Batch 1000: Loss = 0.0000
2025-01-24 10:10:51,322 - Batch 1100: Loss = 0.0000
2025-01-24 10:10:52,316 - Batch 1200: Loss = 0.0000
2025-01-24 10:10:53,307 - Batch 1300: Loss = 0.0000
2025-01-24 10:10:54,299 - Batch 1400: Loss = 0.0000
2025-01-24 10:10:55,288 - Batch 1500: Loss = 0.0000
2025-01-24 10:10:57,928 - Train Loss: 0.0000
2025-01-24 10:10:57,928 - Test Loss: 68412.5874
2025-01-24 10:10:57,928 - FID Score: 0.1329
2025-01-24 10:10:57,929 - Commitment Loss: 0.0000
2025-01-24 10:10:57,929 - Codebook Loss: 0.0000
2025-01-24 10:10:57,929 - Entropy Reg: 0.0000
2025-01-24 10:10:57,929 - L2 Reg: 0.0001
2025-01-24 10:10:57,952 - 
Epoch 45/50
2025-01-24 10:10:58,162 - Batch 0: Loss = 0.0000
2025-01-24 10:10:59,016 - Batch 100: Loss = 0.0000
2025-01-24 10:10:59,881 - Batch 200: Loss = 0.0000
2025-01-24 10:11:00,745 - Batch 300: Loss = 0.0000
2025-01-24 10:11:01,608 - Batch 400: Loss = 0.0000
2025-01-24 10:11:02,475 - Batch 500: Loss = 0.0000
2025-01-24 10:11:03,334 - Batch 600: Loss = 0.0000
2025-01-24 10:11:04,193 - Batch 700: Loss = 0.0000
2025-01-24 10:11:05,056 - Batch 800: Loss = 0.0000
2025-01-24 10:11:05,911 - Batch 900: Loss = 0.0000
2025-01-24 10:11:06,771 - Batch 1000: Loss = 0.0000
2025-01-24 10:11:07,592 - Batch 1100: Loss = 0.0000
2025-01-24 10:11:08,472 - Batch 1200: Loss = 0.0000
2025-01-24 10:11:09,351 - Batch 1300: Loss = 0.0000
2025-01-24 10:11:10,225 - Batch 1400: Loss = 0.0000
2025-01-24 10:11:11,087 - Batch 1500: Loss = 0.0000
2025-01-24 10:11:13,672 - Train Loss: 0.0000
2025-01-24 10:11:13,672 - Test Loss: 68412.5874
2025-01-24 10:11:13,672 - FID Score: 0.1331
2025-01-24 10:11:13,672 - Commitment Loss: 0.0000
2025-01-24 10:11:13,672 - Codebook Loss: 0.0000
2025-01-24 10:11:13,672 - Entropy Reg: 0.0000
2025-01-24 10:11:13,672 - L2 Reg: 0.0001
2025-01-24 10:11:13,695 - 
Epoch 46/50
2025-01-24 10:11:13,897 - Batch 0: Loss = 0.0000
2025-01-24 10:11:14,729 - Batch 100: Loss = 0.0000
2025-01-24 10:11:15,567 - Batch 200: Loss = 0.0000
2025-01-24 10:11:16,420 - Batch 300: Loss = 0.0000
2025-01-24 10:11:17,281 - Batch 400: Loss = 0.0000
2025-01-24 10:11:18,140 - Batch 500: Loss = 0.0000
2025-01-24 10:11:18,990 - Batch 600: Loss = 0.0000
2025-01-24 10:11:19,844 - Batch 700: Loss = 0.0000
2025-01-24 10:11:20,703 - Batch 800: Loss = 0.0000
2025-01-24 10:11:21,562 - Batch 900: Loss = 0.0000
2025-01-24 10:11:22,419 - Batch 1000: Loss = 0.0000
2025-01-24 10:11:23,245 - Batch 1100: Loss = 0.0000
2025-01-24 10:11:24,061 - Batch 1200: Loss = 0.0000
2025-01-24 10:11:24,896 - Batch 1300: Loss = 0.0000
2025-01-24 10:11:25,739 - Batch 1400: Loss = 0.0000
2025-01-24 10:11:26,580 - Batch 1500: Loss = 0.0000
2025-01-24 10:11:29,164 - Train Loss: 0.0000
2025-01-24 10:11:29,165 - Test Loss: 68412.5874
2025-01-24 10:11:29,165 - FID Score: 0.1330
2025-01-24 10:11:29,165 - Commitment Loss: 0.0000
2025-01-24 10:11:29,165 - Codebook Loss: 0.0000
2025-01-24 10:11:29,165 - Entropy Reg: 0.0000
2025-01-24 10:11:29,165 - L2 Reg: 0.0001
2025-01-24 10:11:29,188 - 
Epoch 47/50
2025-01-24 10:11:29,390 - Batch 0: Loss = 0.0000
2025-01-24 10:11:30,084 - Batch 100: Loss = 0.0000
2025-01-24 10:11:30,716 - Batch 200: Loss = 0.0000
2025-01-24 10:11:31,338 - Batch 300: Loss = 0.0000
2025-01-24 10:11:31,963 - Batch 400: Loss = 0.0000
2025-01-24 10:11:32,592 - Batch 500: Loss = 0.0000
2025-01-24 10:11:33,203 - Batch 600: Loss = 0.0000
2025-01-24 10:11:33,953 - Batch 700: Loss = 0.0000
2025-01-24 10:11:34,773 - Batch 800: Loss = 0.0000
2025-01-24 10:11:35,601 - Batch 900: Loss = 0.0000
2025-01-24 10:11:36,449 - Batch 1000: Loss = 0.0000
2025-01-24 10:11:37,329 - Batch 1100: Loss = 0.0000
2025-01-24 10:11:38,209 - Batch 1200: Loss = 0.0000
2025-01-24 10:11:39,074 - Batch 1300: Loss = 0.0000
2025-01-24 10:11:39,952 - Batch 1400: Loss = 0.0000
2025-01-24 10:11:40,833 - Batch 1500: Loss = 0.0000
2025-01-24 10:11:43,420 - Train Loss: 0.0000
2025-01-24 10:11:43,420 - Test Loss: 68412.5874
2025-01-24 10:11:43,420 - FID Score: 0.1331
2025-01-24 10:11:43,420 - Commitment Loss: 0.0000
2025-01-24 10:11:43,420 - Codebook Loss: 0.0000
2025-01-24 10:11:43,420 - Entropy Reg: 0.0000
2025-01-24 10:11:43,420 - L2 Reg: 0.0001
2025-01-24 10:11:43,443 - 
Epoch 48/50
2025-01-24 10:11:43,650 - Batch 0: Loss = 0.0000
2025-01-24 10:11:44,598 - Batch 100: Loss = 0.0000
2025-01-24 10:11:45,598 - Batch 200: Loss = 0.0000
2025-01-24 10:11:46,591 - Batch 300: Loss = 0.0000
2025-01-24 10:11:47,578 - Batch 400: Loss = 0.0000
2025-01-24 10:11:48,574 - Batch 500: Loss = 0.0000
2025-01-24 10:11:49,563 - Batch 600: Loss = 0.0000
2025-01-24 10:11:50,558 - Batch 700: Loss = 0.0000
2025-01-24 10:11:51,555 - Batch 800: Loss = 0.0000
2025-01-24 10:11:52,544 - Batch 900: Loss = 0.0000
2025-01-24 10:11:53,534 - Batch 1000: Loss = 0.0000
2025-01-24 10:11:54,393 - Batch 1100: Loss = 0.0000
2025-01-24 10:11:55,236 - Batch 1200: Loss = 0.0000
2025-01-24 10:11:56,080 - Batch 1300: Loss = 0.0000
2025-01-24 10:11:56,924 - Batch 1400: Loss = 0.0000
2025-01-24 10:11:57,766 - Batch 1500: Loss = 0.0000
2025-01-24 10:12:00,357 - Train Loss: 0.0000
2025-01-24 10:12:00,357 - Test Loss: 68412.5874
2025-01-24 10:12:00,357 - FID Score: 0.1329
2025-01-24 10:12:00,357 - Commitment Loss: 0.0000
2025-01-24 10:12:00,357 - Codebook Loss: 0.0000
2025-01-24 10:12:00,357 - Entropy Reg: 0.0000
2025-01-24 10:12:00,357 - L2 Reg: 0.0001
2025-01-24 10:12:00,381 - 
Epoch 49/50
2025-01-24 10:12:00,586 - Batch 0: Loss = 0.0000
2025-01-24 10:12:01,433 - Batch 100: Loss = 0.0000
2025-01-24 10:12:02,298 - Batch 200: Loss = 0.0000
2025-01-24 10:12:03,166 - Batch 300: Loss = 0.0000
2025-01-24 10:12:04,033 - Batch 400: Loss = 0.0000
2025-01-24 10:12:04,902 - Batch 500: Loss = 0.0000
2025-01-24 10:12:05,771 - Batch 600: Loss = 0.0000
2025-01-24 10:12:06,636 - Batch 700: Loss = 0.0000
2025-01-24 10:12:07,502 - Batch 800: Loss = 0.0000
2025-01-24 10:12:08,368 - Batch 900: Loss = 0.0000
2025-01-24 10:12:09,295 - Batch 1000: Loss = 0.0000
2025-01-24 10:12:10,140 - Batch 1100: Loss = 0.0000
2025-01-24 10:12:11,002 - Batch 1200: Loss = 0.0000
2025-01-24 10:12:11,866 - Batch 1300: Loss = 0.0000
2025-01-24 10:12:12,731 - Batch 1400: Loss = 0.0000
2025-01-24 10:12:13,594 - Batch 1500: Loss = 0.0000
2025-01-24 10:12:16,212 - Train Loss: 0.0000
2025-01-24 10:12:16,212 - Test Loss: 68412.5874
2025-01-24 10:12:16,212 - FID Score: 0.1331
2025-01-24 10:12:16,212 - Commitment Loss: 0.0000
2025-01-24 10:12:16,212 - Codebook Loss: 0.0000
2025-01-24 10:12:16,212 - Entropy Reg: 0.0000
2025-01-24 10:12:16,212 - L2 Reg: 0.0001
2025-01-24 10:12:16,236 - 
Epoch 50/50
2025-01-24 10:12:16,444 - Batch 0: Loss = 0.0000
2025-01-24 10:12:17,295 - Batch 100: Loss = 0.0000
2025-01-24 10:12:18,132 - Batch 200: Loss = 0.0000
2025-01-24 10:12:18,970 - Batch 300: Loss = 0.0000
2025-01-24 10:12:19,848 - Batch 400: Loss = 0.0000
2025-01-24 10:12:20,727 - Batch 500: Loss = 0.0000
2025-01-24 10:12:21,606 - Batch 600: Loss = 0.0000
2025-01-24 10:12:22,485 - Batch 700: Loss = 0.0000
2025-01-24 10:12:23,362 - Batch 800: Loss = 0.0000
2025-01-24 10:12:24,240 - Batch 900: Loss = 0.0000
2025-01-24 10:12:25,089 - Batch 1000: Loss = 0.0000
2025-01-24 10:12:25,932 - Batch 1100: Loss = 0.0000
2025-01-24 10:12:26,774 - Batch 1200: Loss = 0.0000
2025-01-24 10:12:27,613 - Batch 1300: Loss = 0.0000
2025-01-24 10:12:28,455 - Batch 1400: Loss = 0.0000
2025-01-24 10:12:29,297 - Batch 1500: Loss = 0.0000
2025-01-24 10:12:31,891 - Train Loss: 0.0000
2025-01-24 10:12:31,891 - Test Loss: 68412.5874
2025-01-24 10:12:31,891 - FID Score: 0.1329
2025-01-24 10:12:31,891 - Commitment Loss: 0.0000
2025-01-24 10:12:31,891 - Codebook Loss: 0.0000
2025-01-24 10:12:31,891 - Entropy Reg: 0.0000
2025-01-24 10:12:31,891 - L2 Reg: 0.0001
2025-01-24 10:12:31,916 - Training history saved to regularized_training_history.json
2025-01-24 10:12:33,930 - 
Final Evaluation:
2025-01-24 10:12:33,930 - Test Loss: 68412.5874
2025-01-24 10:12:33,930 - FID Score: 0.1329
