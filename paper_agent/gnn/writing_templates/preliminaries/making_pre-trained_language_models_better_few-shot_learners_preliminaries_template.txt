\section{Problem Setup}

\subsection{Task Formulation}
Inspired by [previous work/technique], we propose [module name] to address [challenge]. In this work, we assume access to [context or existing model] that we wish to [goal related to the model] on a task [task name] with a label space [label space]. For the task, we only assume [number of training examples] training examples per class for the task's training set [training set notation], such that the total number of examples is [total examples notation]. Our goal is then to develop [learning strategies or approach] that generalize well to an unseen test set [test set notation]. For model selection and hyper-parameter tuning, we assume a development set [dev set notation], of the same size as the training set, i.e., [size relationship]. This distinction is important: [explanation of the importance]. For all of the following experiments (unless specified otherwise), we take [model name and parameter values].

\subsection{Evaluation Datasets}
We conduct a systematic study across [number of tasks] [task types], including [list of datasets or benchmarks]. All of the dataset details are provided in [appendix reference]. For [task type 1], the goal is to [prediction goal for task type 1]. For [task type 2], the goal is to [prediction goal for task type 2]. We also interchangeably refer to the inputs as [different input notations]. Note that we mainly use [specific datasets] for [purpose of using these datasets], making it close to a true few-shot setting, at least for all the other datasets we evaluate on.

\subsection{Evaluation Protocol}
Systematically evaluating [evaluation type] can be tricky. It is well-known that [common challenges in the evaluation]. To account for this, we measure [method of measure] across [number of splits] different randomly sampled [training set notation] and [development set notation] splits. This issue has also been discussed in [citation] - they suggest [alternative approach]. We argue that [justification of approach], thus we [method for determining optimal settings]. We also observe that [observation about hyperparameters], therefore we [hyperparameter tuning approach].